{"cells":[{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":["<a id=\"setup\"></a>\n","## LangChain memory with watsonx.ai models\n","\n","This notebook contains sample code for using memory buffers with models included in watsonx.ai.\n"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":["### Install dependecies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"26c0d80d-0f79-4bdb-a21a-1f9b16c4c6b8","pycharm":{"is_executing":true,"name":"#%%\n"}},"outputs":[],"source":["import sys\n","\n","!{sys.executable} -m pip install ibm-watsonx-ai | tail -n 1\n","!{sys.executable} -m pip install langchain | tail -n 1\n","!{sys.executable} -m pip install langchain-community | tail -n 1\n","!{sys.executable} -m pip install -U langchain-ibm | tail -n 1"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":["### Define the WML credentials\n","\n","This cell defines the WML credentials required to work with watsonx Foundation Model inferencing.\n","\n","**Action:** Provide the IBM Cloud user API key. For details, see\n","[documentation](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui)."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"4b1083f1-84cd-4242-8c80-459e3e4ac964","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["import getpass\n","from os import environ\n","\n","try:\n","    REGION = environ[\"RUNTIME_ENV_REGION\"]\n","except KeyError:\n","    # Set your region here if you are not running this notebook in the watsonx.ai Jupyter environment\n","    # us-south, eu-de, etc.\n","    REGION = \"us-south\"\n","\n","credentials = {\n","    \"url\": \"https://\" + REGION + \".ml.cloud.ibm.com\",\n","    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \"),\n","}"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":["### Define the project id\n","The Foundation Model requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"2cd480e6-6920-422f-b772-323dd4f5f142","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["from os import getenv\n","\n","try:\n","    project_id = environ[\"PROJECT_ID\"]\n","except KeyError:\n","    # Enter project ID here if not running this notebook in the watsonx.ai Jupyter environment\n","    project_id = \"MY_PROJECT_ID\""]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":["### Import required packages"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"75aca90a-9b44-436c-ab22-40c72617af17","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["from ibm_watsonx_ai.foundation_models import Model\n","from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n","from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n","from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f661ee4d-9002-412a-b760-b67ffa606db3"},"outputs":[],"source":["print(\"Supported Models:\")\n","\n","for model in ModelTypes:\n","    print(\"-\", model.name)"]},{"cell_type":"markdown","metadata":{},"source":["### Create the LangChain model object"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3430c57c-ecd0-4d45-956a-8653d9e9d25b"},"outputs":[],"source":["# We will use the flan model\n","\n","model_parameters = {\n","    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n","    GenParams.MAX_NEW_TOKENS: 300,\n","    GenParams.MIN_NEW_TOKENS: 1,\n","    GenParams.TOP_K: 50,\n","    GenParams.TOP_P: 1,\n","}\n","\n","# Experiment with different models. We noticed that Flan produces a more concise ouput, but llama - more descriptive and better quality\n","current_model = Model(\n","    model_id=ModelTypes.LLAMA_3_70B_INSTRUCT,\n","    params=model_parameters,\n","    credentials=credentials,\n","    project_id=project_id,\n",")\n","\n","# Create the model objects that will be used by LangChain\n","current_llm = current_model.to_langchain()"]},{"cell_type":"markdown","metadata":{"id":"5d44030b-beca-4336-896a-07718a64d9e0"},"source":["### Create conversation"]},{"cell_type":"markdown","metadata":{"id":"1bbe7348-1186-4fa2-956c-8f998ff5e450"},"source":["Use [`RunnableWithMessageHistory`](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) (replacement of `ConversationChain`) to create a runnable that manages chat message history for another Runnable. See [migrating from ConversationalChain](https://python.langchain.com/docs/versions/migrating_chains/conversation_chain/) for more information."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"b5f7cff9-8eef-47a3-b5e3-132cab6e6be7"},"outputs":[],"source":["from langchain_core.chat_history import BaseChatMessageHistory\n","from langchain_community.chat_message_histories import ChatMessageHistory\n","\n","# Here we use a global variable to store the chat message history.\n","# This will make it easier to inspect it to see the underlying results.\n","local_memory = {}\n","\n","\n","def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n","    if session_id not in local_memory:\n","        local_memory[session_id] = ChatMessageHistory()\n","    return local_memory[session_id]"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"f6362578-2998-4b7f-8d5d-a2c2aa255514"},"outputs":[],"source":["from langchain import PromptTemplate\n","\n","base_prompt = PromptTemplate.from_template(\n","    template=\"You are virtual assistant working in customer service. Perform the following task to the best of your ability: {task}\"\n",")\n","\n","base_chain = base_prompt | current_llm"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"8043a65f-56b7-4b8a-ba91-e6c16adacd41"},"outputs":[],"source":["from langchain_core.runnables.history import RunnableWithMessageHistory\n","\n","conversation = RunnableWithMessageHistory(\n","    base_chain,\n","    get_session_history=get_by_session_id,  # Function that returns a new BaseChatMessageHistory.\n","    input_messages_key=\"task\",  # key in the input dict that contains the messages.\n","    history_messages_key=\"history\",  # Use separate key for historical messages.\n",")"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"655fdbc3-b850-4fcf-8e8d-2d15dabccba8"},"outputs":[],"source":["# Example of a first quesiton from the user\n","user_input = \"From the following customer complaint, extract 3 factors that caused the customer to be unhappy. \\\n","                            Put each factor on a new line. Customer complaint: I am writing you this statement to delete the \\\n","                            following information on my credit report. The items I need deleted are listed in the report. \\\n","                            I am a victim of identity thief, I demand that you remove these errors to correct my report immediately! \\\n","                            I have reported this to the federal trade commission and have attached the federal trade commission affidavit. \\\n","                            Now that I have given you the following information, please correct my credit report or I shall proceed with involving my attorney! \\\n","                            Numbered list of complaints:\"\n","\n","# user_input=\"Right now I am bothered! I have attempted to be patient however it is hard to be patient when you feel that you are continually being overlooked by somebody. I think you fail to remember that \\Consumer detailing organizations have expected an essential part in amassing and assessing customer credit and other data on shoppers. The XXXX XXXX  is reliant on the reasonable and precision.\""]},{"cell_type":"markdown","metadata":{"id":"c249f20d-5be6-46e4-a8d5-55a00f9ef319"},"source":["Invoke the LLM chain using `user_input` as input.\n","\n","> *Pro tip:* see [`RunnableConfig`](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig) for more information on how to invoke these flows."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4e3d68f6-6728-474d-b307-972d19c77982"},"outputs":[],"source":["conversation.invoke(\n","    input={\"task\": user_input}, config={\"configurable\": {\"session_id\": \"foo\"}}\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Now, invoke it for a different input, as a follow-up question:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eef20ad3-0cf7-4e76-81b2-b0d061cf5cc4"},"outputs":[],"source":["# Example of a second quesiton from the user\n","user_input = \"Does the numbered list of complaints contain a statement about identity fraud? Provide a short answer: yes or no.\"\n","\n","conversation.invoke(\n","    input={\"task\": user_input}, config={\"configurable\": {\"session_id\": \"foo\"}}\n",")"]},{"cell_type":"markdown","metadata":{},"source":["You can print the memory object to see the conversation history:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e19e7806-4432-426e-9aca-acd581abaf89"},"outputs":[],"source":["# For debugging, print the history of the conversation\n","print(local_memory)"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":["### Authors: \n","- **Elena Lowery**, Data and AI Architect\n","- **Josefina Casanova**, Engagement Lead, Build Lab Americas. Edited for L4 watsonx course. 2024"]}],"metadata":{"kernelspec":{"display_name":"Python 3.11","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
