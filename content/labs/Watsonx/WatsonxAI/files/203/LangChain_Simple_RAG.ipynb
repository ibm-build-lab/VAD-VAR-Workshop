{"cells":[{"cell_type":"markdown","metadata":{},"source":["![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n","\n","# Implement a simple RAG use case with LangChain\n","\n","_Retrieval Augmented Generation (RAG)_ allows us to use LLMs to interact with \"external data\" i.e. data that was not used for model training. Many use cases require working with proprietary company data, and it's one of the reasons why RAG is frequently used in generative AI applications.\n","\n","There is more than one way to implement the RAG pattern, which we will cover in a later lab. In this notebook, we will use _LangChain's RetrievalQA_ API to demonstrate one implementation of a RAG pattern. In general, RAG can be used for more than just question-and-answer use cases, but as you can tell from the name of the API, _RetrievalQA_ was implemented specifically for question-and-answer. \n","\n","To get started we'll first verify that you have the necessary dependencies installed to run this notebook.\n","\n","Go ahead and run the following code cell. **This may take a few seconds to complete.**\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"5d307b20-2e5a-4046-9acb-0ad342cdf145"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: SQLAlchemy==2.0.29 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (2.0.29)\n","Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from SQLAlchemy==2.0.29) (4.11.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from SQLAlchemy==2.0.29) (3.0.1)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from lomond->ibm-watsonx-ai) (1.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n","Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from lomond->ibm-watsonx-ai<2.0.0,>=1.1.14->langchain-ibm) (1.16.0)\n","Collecting langchain-huggingface\n","  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from langchain-huggingface) (0.26.1)\n","Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from langchain-huggingface) (0.3.12)\n","Requirement already satisfied: sentence-transformers>=2.6.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from langchain-huggingface) (3.2.1)\n","Requirement already satisfied: tokenizers>=0.19.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from langchain-huggingface) (0.20.1)\n","Requirement already satisfied: transformers>=4.39.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from langchain-huggingface) (4.45.2)\n","Requirement already satisfied: filelock in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2023.10.0)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.1)\n","Requirement already satisfied: requests in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.2)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.11.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.1.126)\n","Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (2.9.2)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (8.2.2)\n","Requirement already satisfied: torch>=1.11.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.1.2)\n","Requirement already satisfied: scikit-learn in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n","Requirement already satisfied: scipy in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.11.4)\n","Requirement already satisfied: Pillow in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.3.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from transformers>=4.39.0->langchain-huggingface) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from transformers>=4.39.0->langchain-huggingface) (2024.9.11)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.5)\n","Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.0.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.26.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.10.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-huggingface) (2.23.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (1.26.19)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.8.30)\n","Requirement already satisfied: sympy in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.8.4)\n","Requirement already satisfied: jinja2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (2.2.0)\n","Requirement already satisfied: anyio in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.5.0)\n","Requirement already satisfied: httpcore==1.* in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.0.2)\n","Requirement already satisfied: sniffio in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.3.0)\n","Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.14.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n","Downloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n","Installing collected packages: langchain-huggingface\n","Successfully installed langchain-huggingface-0.1.0\n"]}],"source":["# Install dependencies\n","\n","import sys\n","%pip install SQLAlchemy==2.0.29\n","!{sys.executable} -m pip install -q chromadb\n","!{sys.executable} -m pip install ibm-watsonx-ai | tail -n 1\n","!{sys.executable} -m pip install langchain | tail -n 1\n","!{sys.executable} -m pip install -U langchain-ibm | tail -n 1\n","!{sys.executable} -m pip install -q pypdf\n","!{sys.executable} -m pip install -q sentence-transformers\n","!{sys.executable} -m pip install -U langchain-huggingface\n","\n","# !{sys.executable} -m pip install -q chardet"]},{"cell_type":"markdown","metadata":{},"source":["## Bring in dependencies\n","\n","In this next code cell we'll bring in all the dependencies we'll need for later use.\n","\n","Go ahead and run the following code cell. **There should be no ouput.**"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"f443e502-548b-43ab-87b5-2e28b9f61300"},"outputs":[],"source":["# Bring in dependencies\n","# SQLite fix: https://docs.trychroma.com/troubleshooting#sqlite\n","# __import__('pysqlite3')\n","# import sys\n","# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n","\n","from langchain_community.document_loaders import PyPDFLoader\n","from langchain.chains import RetrievalQA\n","from langchain.indexes import VectorstoreIndexCreator\n","from langchain.text_splitter import CharacterTextSplitter\n","\n","from langchain_huggingface import HuggingFaceEmbeddings\n","\n","# WML python SDK\n","from ibm_watsonx_ai.foundation_models import Model\n","from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n","from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes, DecodingMethods\n","\n","from langchain_ibm import WatsonxLLM"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Environment Setup\n","\n","In this next code cell you'll define some variables that will be used in order to interact with your instance of watsonx.ai."]},{"cell_type":"markdown","metadata":{},"source":["### Defining the WML credentials\n","This cell defines the WML credentials required to work with watsonx Foundation Model inferencing.\n","\n","**Action:** Provide the IBM Cloud user API key. For details, see\n","[documentation](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui)."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"f8c32693-b76e-4ec8-b6e7-6fab996facff"},"outputs":[],"source":["import getpass\n","from os import environ\n","\n","try:\n","    REGION = environ[\"RUNTIME_ENV_REGION\"]\n","except KeyError:\n","    # Set your region here if you are not running this notebook in the watsonx.ai Jupyter environment\n","    # us-south, eu-de, etc.\n","    REGION = \"us-south\"\n","\n","credentials = {\n","    \"url\": \"https://\" + REGION + \".ml.cloud.ibm.com\",\n","    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \")\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Defining the project id\n","The Foundation Model requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"bc2566ea-faff-44f0-8d2f-e64ded00512e"},"outputs":[],"source":["try:\n","    project_id = environ[\"PROJECT_ID\"]\n","except KeyError:\n","    # Enter project ID here if not running this notebook in the watsonx.ai Jupyter environment\n","    project_id = \"MY_PROJECT_ID\"\n"]},{"cell_type":"markdown","metadata":{},"source":["## Understanding the code\n","\n","In this next code cell we'll create the `get_model()` function that we can use later to interact easier with watsonx.ai. \n","\n","The function creates a model object that will be used to invoke the LLM. Since the ***get_model()*** function is parametrized, it's the same in all examples. If the `to_langchain` parameter is set to True, a model wrapper to will be used with the _LangChain_ API will be returned.\n","\n","Go ahead and run the following code cell. **There should be no ouput**."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"b0f2aa5a-921e-41f3-a32e-5d1ed81379cb"},"outputs":[],"source":["def get_model(\n","    model_id : str,\n","    model_params : dict = {\n","        GenParams.MAX_NEW_TOKENS: 300,\n","        GenParams.MIN_NEW_TOKENS: 10,\n","        GenParams.DECODING_METHOD: 'greedy',\n","    },\n","    to_langchain:bool = False\n","):\n","\n","    model = Model(\n","        model_id=model_id,\n","        params=model_params,\n","        credentials=credentials,\n","        project_id=project_id\n","    )\n","\n","    if to_langchain:\n","        return model.to_langchain()\n","\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["The next function, `answer_questions_from_doc`, that we create is created to help combine the previous three that we defined. This is the wrapper that we will call when we want to interact with watsonx.ai.\n","\n","The function specifies model parameters, loads the PDF file, creates an index from the loaded document, the instantiates and invokes the chain.\n","\n","Go ahead and run the following code cell. **There should be no ouput**."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"9911180d-3c5d-45ea-a158-04bc15e93dcd"},"outputs":[],"source":["def answer_questions_from_doc(file_path : str, question : str, print_res : bool = True) -> str:\n","    \n","    # Get the watsonx model that can be used with LangChain\n","    model = get_model(\n","        model_id = \"meta-llama/llama-3-8b-instruct\",\n","        model_params = {\n","            GenParams.MAX_NEW_TOKENS: 300,\n","            GenParams.MIN_NEW_TOKENS: 100,\n","            GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n","        },\n","        to_langchain = True\n","    )\n","    \n","    loaders = [PyPDFLoader(file_path)]\n","    \n","    index = VectorstoreIndexCreator(\n","      embedding=HuggingFaceEmbeddings(),\n","      text_splitter=CharacterTextSplitter(\n","          chunk_size=1000, \n","          chunk_overlap=100\n","      )\n","    ).from_loaders(loaders)\n","    \n","    chain = RetrievalQA.from_chain_type(\n","        llm=model,\n","        chain_type=\"stuff\",\n","        retriever=index.vectorstore.as_retriever(),\n","        input_key=\"question\"\n","    )\n","    \n","    # Invoke the chain\n","    response_text = chain.invoke(question)\n","\n","    if print_res:\n","        # print model response\n","        print(\"--------------------------------- Generated response -----------------------------------\")\n","        print(response_text)\n","        print(\"*********************************************************************************************\")\n","\n","    return response_text\n"]},{"cell_type":"markdown","metadata":{},"source":["## Answering some questions\n","\n","The next code cell will use all the previous code we've created so far to source information from the input documents and ask a question about them using watsonx.ai (Notice the use of the `answer_questions_from_doc`).\n","\n","To do so we'll pass in a question we want to ask, the PDF file we want to reference for said question, and finally the name of the collection where the embeddings of the file exist.\n","\n","Notice the commented questions as well? Feel free to uncomment these or create some or your own to ask\n","\n","Go ahead and run the next code cell. **You _will_ see output from this cell**"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3b4e13a2-2846-468e-abdd-6c5a8436ae5a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:415: LifecycleWarning: Model 'meta-llama/llama-3-8b-instruct' is in deprecated state from 2024-12-02 until 2025-02-03. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"939a953da6de4746b761485669f68e77","version_major":2,"version_minor":0},"text/plain":["model.safetensors:  72%|#######1  | 315M/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c45dbffc5bd4bf5b3834bd763915bca","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56e4478a453a4a7798a9b9e9262367cf","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6cc1a9ce53804d37aaff7ca627869533","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5726ac35ff9542bfb3703664e8eec01e","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1313b79d2c084e3c861056f09fb563a5","version_major":2,"version_minor":0},"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langchain/indexes/vectorstore.py:128: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["--------------------------------- Generated response -----------------------------------\n","{'question': 'What is Generative AI?', 'result': \" Generative AI is a type of artificial intelligence that can create new content, such as text, images, code, and videos, using algorithms and machine learning. It has the potential to revolutionize content creation and has many practical uses, such as generating marketing copy, creating art, and optimizing business processes. However, it also has limitations and risks, such as the potential for biased or inaccurate output, and the need for careful selection and training of the initial data used to train the models. \\n\\nNote: The answer is based on the provided text and does not attempt to provide a comprehensive definition of generative AI. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"}\n","*********************************************************************************************\n"]},{"data":{"text/plain":["{'question': 'What is Generative AI?',\n"," 'result': \" Generative AI is a type of artificial intelligence that can create new content, such as text, images, code, and videos, using algorithms and machine learning. It has the potential to revolutionize content creation and has many practical uses, such as generating marketing copy, creating art, and optimizing business processes. However, it also has limitations and risks, such as the potential for biased or inaccurate output, and the need for careful selection and training of the initial data used to train the models. \\n\\nNote: The answer is based on the provided text and does not attempt to provide a comprehensive definition of generative AI. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Test answering questions based on the provided .pdf file\n","question = \"What is Generative AI?\"\n","# question = \"What does it take to build a generative AI model?\"\n","# question = \"What are the limitations of generative AI models?\"\n","file_path = \"https://raw.githubusercontent.com/CloudPak-Outcomes/Outcomes-Projects/main/L4assets/watsonx.ai-Assets/Documents/Generative_AI_Overview.pdf\"\n","\n","answer_questions_from_doc(\n","    file_path = file_path,\n","    question = question,\n","    print_res = True\n",")"]},{"cell_type":"markdown","metadata":{"id":"7868d367-ecc9-4320-98ea-c8aa33a5ff96"},"source":["\n","### Authors:\n","- **Josefina Casanova**, Engagement Lead, Build Lab Americas. Edited for L4 watsonx course. 2024"]}],"metadata":{"kernelspec":{"display_name":"Python 3.11","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
