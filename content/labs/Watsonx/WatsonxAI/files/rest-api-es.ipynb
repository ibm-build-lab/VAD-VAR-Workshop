{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Accediendo a Watsonx.ai vía REST API\n",
    "\n",
    "En este laboratorio, vamos a echar un vistazo completo a la realización de peticiones HTTP para acceder a la API REST de Watsonx.ai y aprender a utilizar la funcionalidad. Este laboratorio explora sólo algunos de los muchos puntos finales REST disponibles, así que explora la documentación de la API REST para ver la lista completa de capacidades.\n",
    "\n",
    "Antes de empezar, debe disponer de los elementos necesarios para acceder a watsonx.ai mediante programación, a saber\n",
    "\n",
    "- tu clave de API de IBM Cloud\n",
    "- la URL de servicio de IBM Cloud vinculada a su instancia\n",
    "- el ID de proyecto asociado a su instancia de watsonx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero empezaremos instalando algunas dependencias\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q requests\n",
    "!{sys.executable} -m pip install -q ibm-cloud-sdk-core\n",
    "!{sys.executable} -m pip install -q ibm-watson-machine-learning==1.0.311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y verificando que se han instalado correctamente\n",
    "import json\n",
    "import requests\n",
    "from ibm_cloud_sdk_core import IAMTokenManager\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cabeceras de solicitud HTTP\n",
    "\n",
    "Las cabeceras contienen valores de parámetros que representan los metadatos asociados a las solicitudes y respuestas de una API. En el siguiente ejemplo, la cabecera Authorization proporciona al servidor las credenciales para validar tu acceso. Watsonx.ai utiliza un token de acceso \"Bearer\" que se utiliza para pasar nuestra clave de autenticación Watsonx.ai. La cabecera `Content-type` de la petición se añade para indicar al servidor o al navegador que está sirviendo el recurso al usuario final el tipo de medio de la petición. En este caso, el tipo de datos esperados es `application/json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTOS SON LOS VALORES QUE TENDRÁ QUE RELLENAR USTED MISMO\n",
    "# clave API que ha creado\n",
    "api_key = \"INSERTE AQUÍ SU CLAVE\"\n",
    "\n",
    "# ID del proyecto de su instancia watsonx\n",
    "project_id = \"INSERTE AQUÍ EL IDENTIFICADOR DE SU PROYECTO\"\n",
    "\n",
    "# Punto final del servicio URL\n",
    "ibm_cloud_url = \"INSERTE AQUÍ LA URL DE SU SERVICIO\"\n",
    "\n",
    "access_token = ''\n",
    "try:\n",
    "  access_token = IAMTokenManager(\n",
    "    apikey = api_key,\n",
    "    url = \"https://iam.cloud.ibm.com/identity/token\"\n",
    "  ).get_token()\n",
    "except:\n",
    "  print('Problema para obtener el token de acceso. ¿Comprobar variables?')\n",
    "\n",
    "# Las cabeceras que enviaremos para nuestras llamadas POST y GET\n",
    "headers = {\n",
    "  \"Authorization\": \"Bearer \" + access_token,\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Accept\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST vs GET\n",
    "\n",
    "Las peticiones HTTP son de dos tipos: GET y POST. Cuando se utiliza GET, los parámetros de datos se incluyen en la URL y son visibles para todos. Sin embargo, cuando se utiliza POST, los datos no se muestran en la URL, sino que se pasan en el cuerpo del mensaje HTTP.\n",
    "\n",
    "Las peticiones GET están pensadas para recuperar datos de un servidor y no modifican su estado. Por otro lado, las peticiones POST se utilizan para enviar datos al servidor para su procesamiento y pueden modificar el estado del servidor.\n",
    "\n",
    "## Peticiones POST con el endpoint \"Generar\"\n",
    "El punto final de generación \"https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text\" proporciona una interfaz para el envío de mensajes a cualquier modelo soportado por Watsonx.ai. Dado un texto como entrada, y los parámetros requeridos, el modelo seleccionado intentará completar la entrada proporcionada y devolverá \"texto_generado\".\n",
    "\n",
    "El cuerpo de la solicitud debe incluir\n",
    "- Model id (cadena): id del modelo\n",
    "- Entrada (cadena): solicitud para generar la compleción\n",
    "- Parámetros del modelo (pares clave-valor)\n",
    "- ID de su proyecto watsonx\n",
    "\n",
    "> Es importante tener en cuenta que la URL del punto final de generación depende de la ubicación de su instancia watsonx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El contenido que enviaremos como parte de nuestra petición POST\n",
    "body = {\n",
    "  \"model_id\": \"google/flan-ul2\",\n",
    "  \"input\": \"Write a short blog post for an advanced cloud service for large language models: This service is\",\n",
    "  \"parameters\": {\n",
    "    \"temperature\": 0,\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"min_new_tokens\": 25\n",
    "  },\n",
    "  \"project_id\": project_id  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"2023-07-07\"\n",
    "generation_endpoint = f\"{ibm_cloud_url}/ml/v1-beta/generation/text?version={version}\"\n",
    "\n",
    "response = requests.post(url=generation_endpoint, headers=headers, json=body)\n",
    "print(\"Respuesta JSON sin formato:\\n\", json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando llamadas GET para recuperar datos\n",
    "\n",
    "El método GET se utiliza para recuperar información de Watsonx.ai utilizando una URL dada.\n",
    "\n",
    "### GET /modelos\n",
    "\n",
    "Este enpoint obtendrá la lista de todos los modelos actualmente soportados por Watsonx.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_endpoint = f\"{ibm_cloud_url}/ml/v1-beta/foundation_model_specs?version={version}\"\n",
    "response = requests.get(url = model_endpoint, headers=headers)\n",
    "models = response.json()['resources']\n",
    "\n",
    "print(f\"{len(models)} modelos apoyados por Watsonx.ai\")\n",
    "\n",
    "# Descomentar a continuación para ver el JSON formato\n",
    "# print(json.dumps(models, indent=2))\n",
    "\n",
    "# Embellecer la información sólo para ver los nombres de los modelos\n",
    "pretty_output = lambda m: m['label']\n",
    "print(json.dumps(list(map(pretty_output, models)), indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando una librería Python\n",
    "\n",
    "Ahora que hemos visto cómo interactuar con watsonx.ai a través del uso de una API REST vamos a echar un vistazo a cómo podemos interactuar con él a través de una biblioteca de Python también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# las credenciales para autenticarse con ibm cloud\n",
    "# similar a las cabeceras creadas anteriormente\n",
    "creds = {\n",
    "  \"url\": ibm_cloud_url,\n",
    "  \"apikey\": api_key \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# una función de ayuda que permite enviar varios avisos a la vez y parámetros para ajustarlos\n",
    "def send_to_watsonxai(prompts,\n",
    "                    model_name=\"google/flan-ul2\",\n",
    "                    decoding_method=\"greedy\",\n",
    "                    max_new_tokens=100,\n",
    "                    min_new_tokens=30,\n",
    "                    temperature=1.0,\n",
    "                    repetition_penalty=2.0\n",
    "                    ):\n",
    "    '''\n",
    "   helper function for sending prompts and params to Watsonx.ai\n",
    "    \n",
    "    Args:  \n",
    "        prompts:list list of text prompts\n",
    "        decoding:str Watsonx.ai parameter \"sample\" or \"greedy\"\n",
    "        max_new_tok:int Watsonx.ai parameter for max new tokens/response returned\n",
    "        temperature:float Watsonx.ai parameter for temperature (range 0>2)\n",
    "        repetition_penalty:float Watsonx.ai parameter for repetition penalty (range 1.0 to 2.0)\n",
    "\n",
    "    Returns: None\n",
    "        prints response\n",
    "    '''\n",
    "\n",
    "    assert not any(map(lambda prompt: len(prompt) < 1, prompts)), \"asegúrese de que ninguno de los inputs prompts está vacío\"\n",
    "\n",
    "# Instanciar parámetros para la generación de texto\n",
    "    model_params = {\n",
    "        GenParams.DECODING_METHOD: decoding_method,\n",
    "        GenParams.MIN_NEW_TOKENS: min_new_tokens,\n",
    "        GenParams.MAX_NEW_TOKENS: max_new_tokens,\n",
    "        GenParams.RANDOM_SEED: 42,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "        GenParams.REPETITION_PENALTY: repetition_penalty,\n",
    "    }\n",
    "\n",
    "\n",
    "   # Instanciar un objeto proxy modelo para enviar sus llamadas\n",
    "    model = Model(\n",
    "        model_id=model_name,\n",
    "        params=model_params,\n",
    "        credentials=creds,\n",
    "        project_id=project_id)\n",
    "\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(model.generate_text(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a short blog post for an advanced cloud service for large language models: This service is\"\n",
    "\n",
    "# siéntete libre de probar cambiando el modelo de uno de los valores listados antes\n",
    "# y prueba a cambiar también otros valores\n",
    "response = send_to_watsonxai(\n",
    "  prompts=[prompt],\n",
    "  model_name=\"google/flan-ul2\",\n",
    "  decoding_method=\"greedy\",\n",
    "  max_new_tokens=100,\n",
    "  min_new_tokens=30,\n",
    "  temperature=1.0,\n",
    "  repetition_penalty=2.0\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Aunque hemos utilizado Python como nuestro lenguaje de elección en este laboratorio. Es importante señalar que a través del uso de una API REST esencialmente cualquier lenguaje puede utilizar watsonx.ai mediante programación. Lo que significa que no hay límite a la forma en que los clientes eligen integrar watsonx.ai en su pila tecnológica existente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
