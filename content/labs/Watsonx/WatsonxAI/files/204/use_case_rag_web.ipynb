{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement RAG using a website and watsonx.ai \n",
    "\n",
    "In the lab **\"Implement RAG Use Cases in watsonx.ai\"** we looked at how to implement a RAG use with our source being from some `.pdf` and `.txt` files. In this example we instead source of content by scraping a given website URL. \n",
    "\n",
    "The main difference from the previous example is how data is sourced for our embedding. We'll use open source APIs [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) and [spacy](https://spacy.io/) to get data from a Web page.\n",
    "\n",
    "To get started we'll first verify that you have the necessary dependencies installed to run this notebook.\n",
    "\n",
    "Go ahead and run the following code cell. **This may take a few seconds to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q chromadb==0.4.22\n",
    "!{sys.executable} -m pip install -q ibm_watson_machine_learning==1.0.342\n",
    "!{sys.executable} -m pip install -q langchain==0.1.3\n",
    "!{sys.executable} -m pip install -q langchain_community==0.0.15\n",
    "!{sys.executable} -m pip install -q beautifulsoup4==4.12.3\n",
    "!{sys.executable} -m pip install -q spacy==3.7.2\n",
    "\n",
    "!{sys.executable} -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in dependencies\n",
    "\n",
    "In this next code cell we'll bring in all the dependencies we'll need for later use.\n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in dependencies\n",
    "# SQLite fix: https://docs.trychroma.com/troubleshooting#sqlite\n",
    "# __import__('pysqlite3')\n",
    "# import sys\n",
    "# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import chromadb\n",
    "import en_core_web_md\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important variables\n",
    "\n",
    "In this next code cell you'll define some variables that will be used in order to interact with your instance of watsonx.ai.\n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the global variables that will be used for authentication in another function\n",
    "watsonx_project_id = \"PASTE_PROJECT_ID_HERE\"\n",
    "api_key = \"PASTE_API_KEY_HERE\"\n",
    "instance_url = \"https://us-south.ml.cloud.ibm.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the code\n",
    "\n",
    "In this next code cell we'll create some functions that we can use later to interact easier with watsonx.ai. These functions are `get_model`, `create_embedding`, and `create_prompt`: \n",
    "\n",
    "- `get_model`: Creates a model object that will be used to invoke the LLM\n",
    "- `extract_text`: Will pull text from a given website to create embedding from\n",
    "- `split_text_into_sentences`: Split the text we extracted into individual sentences and clean them of any unnecessary characters\n",
    "- `create_embedding`: Loads text data from a given URL into the in-memory `chromadb` instance\n",
    "- `create_prompt`: Generates the prompt that is sent to watsonx.ai API\n",
    "   - Notice that in the beginning of the function we query the vector database to retrieve information thatâ€™s related to our question (semantic search).\n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_type, max_tokens, min_tokens, decoding, temperature, top_k, top_p):\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "        GenParams.TOP_K: top_k,\n",
    "        GenParams.TOP_P: top_p,\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials={\n",
    "            \"apikey\": api_key,\n",
    "            \"url\": instance_url\n",
    "        },\n",
    "        project_id=watsonx_project_id\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def extract_text(url):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract contents of <p> elements\n",
    "            p_contents = [p.get_text() for p in soup.find_all('p')]\n",
    "\n",
    "            # Print the contents of <p> elements\n",
    "            print(\"\\nContents of <p> elements: \\n\")\n",
    "            for content in p_contents:\n",
    "                print(content)\n",
    "            raw_web_text = \" \".join(p_contents)\n",
    "            # remove \\xa0 which is used in html to avoid words break acorss lines.\n",
    "            cleaned_text = raw_web_text.replace(\"\\xa0\", \" \")\n",
    "            return cleaned_text\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def split_text_into_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    cleaned_sentences = [s.strip() for s in sentences]\n",
    "    return cleaned_sentences\n",
    "\n",
    "def create_embedding(url, collection_name):\n",
    "    cleaned_text = extract_text(url)\n",
    "    cleaned_sentences = split_text_into_sentences(cleaned_text)\n",
    "\n",
    "    client = chromadb.Client()\n",
    "\n",
    "    collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "    # Upload text to chroma\n",
    "    collection.upsert(\n",
    "        documents=cleaned_sentences,\n",
    "        metadatas=[{\"source\": str(i)} for i in range(len(cleaned_sentences))],\n",
    "        ids=[str(i) for i in range(len(cleaned_sentences))],\n",
    "    )\n",
    "\n",
    "    return collection\n",
    "\n",
    "def create_prompt(url, question, collection_name):\n",
    "    # Create embeddings for the text file\n",
    "    collection = create_embedding(url, collection_name)\n",
    "\n",
    "    # query relevant information\n",
    "    relevant_chunks = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=5,\n",
    "    )\n",
    "    context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\n",
    "    # Please note that this is a generic format. You can change this format to be specific to llama\n",
    "    prompt = (f\"{context}\\n\\nPlease answer the following question in one sentence using this \"\n",
    "              + f\"text. \"\n",
    "              + f\"If the question is unanswerable, say \\\"unanswerable\\\". Do not include information that's not relevant to the question.\"\n",
    "              + f\"Question: {question}\")\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gluing it together\n",
    "\n",
    "The next function, `answer_questions_from_web`, that we create is created to help combine the previous five that we defined. This is the wrapper that we will call when we want to interact with watsonx.ai. \n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions_from_web(url, question, collection_name):\n",
    "    # Specify model parameters\n",
    "    model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "    max_tokens = 100\n",
    "    min_tokens = 50\n",
    "    top_k = 50\n",
    "    top_p = 1\n",
    "    decoding = DecodingMethods.GREEDY\n",
    "    temperature = 0.7\n",
    "\n",
    "    # Get the watsonx model = try both options\n",
    "    model = get_model(model_type, max_tokens, min_tokens, decoding, temperature, top_k, top_p)\n",
    "\n",
    "    # Get the prompt\n",
    "    complete_prompt = create_prompt(url, question, collection_name)\n",
    "\n",
    "    generated_response = model.generate(prompt=complete_prompt)\n",
    "    response_text = generated_response['results'][0]['generated_text']\n",
    "\n",
    "    # Remove trailing white spaces\n",
    "    response_text = response_text.strip()\n",
    "\n",
    "    # print model response\n",
    "    print(\"--------------------------------- Generated response -----------------------------------\")\n",
    "    print(response_text.strip())\n",
    "    print(\"*********************************************************************************************\")\n",
    "\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering some questions\n",
    "\n",
    "The next code cell will use all the previous code we've created so far to source information from the input documents and ask a question about them using watsonx.ai (Notice the return of the `answer_questions_from_web` function). \n",
    "\n",
    "To do so we'll pass in a question we want to ask, the web URL we want to reference for said question, and finally the name of the collection where the embeddings exist.\n",
    "\n",
    "Go ahead and run the next code cell. **You will see output from this cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try diffrent URLs and questions\n",
    "web_url = \"https://www.ibm.com/products/watsonx-ai\"\n",
    "question = \"What is Prompt Lab?\"\n",
    "collection_name = \"test_web_RAG\"\n",
    "\n",
    "answer_questions_from_web(web_url, question, collection_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
