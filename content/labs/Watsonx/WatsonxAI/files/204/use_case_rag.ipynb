{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement RAG Use Cases in watsonx.ai\n",
    "\n",
    "In this lab you will review and run examples of LLM applications that implement the Retrieval Augmented Generation (RAG) pattern of working with LLMs. We will expand on the concepts that you learned in the previous labs.\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) is one of the most common use cases in generative AI because it allows us to work with data \"external to the model\", for example, data that was not used for model training. Many use cases require working with proprietary company data, and it's one of the reasons why RAG is frequently used in generative AI applications. RAG also allows us to add some guardrails to generated output and reduce hallucination. RAG can be used with several generative AI use cases, including:\n",
    "\n",
    "- Question and answer\n",
    "- Summarization\n",
    "- Content generation\n",
    "\n",
    "> A \"human interaction\" analogy of RAG is providing a document to a person and asking \n",
    "them to answer question based on the information in the document.\n",
    "\n",
    "To get started we'll first verify that you have the necessary dependencies installed to run this notebook.\n",
    "\n",
    "Go ahead and run the following code cell. **This may take a few seconds to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q chromadb==0.4.22\n",
    "!{sys.executable} -m pip install -q ibm_watson_machine_learning==1.0.342\n",
    "!{sys.executable} -m pip install -q langchain==0.1.4\n",
    "!{sys.executable} -m pip install -q langchain_community==0.0.15\n",
    "!{sys.executable} -m pip install -q pypdf==4.0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in dependencies\n",
    "\n",
    "In this next code cell we'll bring in all the dependencies we'll need for later use.\n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in dependencies\n",
    "# SQLite fix: https://docs.trychroma.com/troubleshooting#sqlite\n",
    "# __import__('pysqlite3')\n",
    "# import sys\n",
    "# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "import requests\n",
    "import chromadb\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Document loaders\n",
    "from langchain.document_loaders.pdf import PyPDFLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "print(\"Successfully loaded dependencies!\")\n",
    "\n",
    "FILE_TYPE_TXT = \"txt\"\n",
    "FILE_TYPE_PDF = \"pdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important variables\n",
    "\n",
    "In this next code cell you'll define some variables that will be used in order to interact with your instance of watsonx.ai.\n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the global variables that will be used for authentication in another function\n",
    "watsonx_project_id = \"PASTE_PROJECT_ID_HERE\"\n",
    "api_key = \"PASTE_API_KEY_HERE\"\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the code\n",
    "\n",
    "In this next code cell we'll create some functions that we can use later to interact easier with watsonx.ai. These functions are `get_model`, `create_embedding`, and `create_prompt`: \n",
    "\n",
    "- `get_model`: Creates a model object that will be used to invoke the LLM\n",
    "- `create_embedding`: Loads text data from given file path into the in-memory `chromadb` instance\n",
    "- `create_prompt`: Generates the prompt that is sent to watsonx.ai API\n",
    "   - Notice that in the beginning of the function we query the vector database to retrieve information thatâ€™s related to our question (semantic search). Search results are appended to the prompt, and the prompt instruction is \"to give an answer using the provided text\".\n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Answer the following question using the context provided. \n",
    "If there is no good answer, say \"unanswerable\".\n",
    "\n",
    "Context: %s\n",
    "\n",
    "Question: %s\n",
    "\"\"\"\n",
    "\n",
    "# Creates a model object that will be used to invoke the LLM\n",
    "def get_model(model_type,max_tokens,min_tokens,decoding,temperature):\n",
    "\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials={\n",
    "            \"apikey\": api_key,\n",
    "            \"url\": url\n",
    "        },\n",
    "        project_id=watsonx_project_id\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Loads text data from given file path into the chromadb instance\n",
    "def create_embedding(file_path,file_type,collection_name):\n",
    "    documents = []\n",
    "\n",
    "    if file_type == FILE_TYPE_TXT:\n",
    "        if file_path.startswith('http'):\n",
    "            r = requests.get(file_path)\n",
    "            metadata = {\"source\": file_path}\n",
    "            raw_text = r.text.encode('utf-8').strip()\n",
    "            documents = [Document(page_content=raw_text, metadata=metadata)]\n",
    "        else:\n",
    "            loader = TextLoader(file_path,encoding=\"1252\")\n",
    "            documents = loader.load()        \n",
    "    elif file_type == FILE_TYPE_PDF:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "  \n",
    "    print(type(texts))\n",
    "    print(len(texts))\n",
    "\n",
    "    # Load chunks into chromadb\n",
    "    client = chromadb.Client()\n",
    "    collection = client.get_or_create_collection(name=collection_name,embedding_function=embedding_functions.DefaultEmbeddingFunction())\n",
    "    collection.upsert(\n",
    "        documents=[doc.page_content for doc in texts],\n",
    "        ids=[str(i) for i in range(len(texts))],  # unique for each doc\n",
    "    )\n",
    "\n",
    "    return collection\n",
    "\n",
    "# Generates the prompt that is sent to watsonx.ai API\n",
    "def create_prompt(file_path, file_type, question, collection_name):\n",
    "    # Create embeddings for the text file\n",
    "    collection = create_embedding(file_path,file_type,collection_name)\n",
    "\n",
    "    # Query relevant information\n",
    "    relevant_chunks = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=3,\n",
    "    )\n",
    "\n",
    "    context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\n",
    "    prompt = prompt_template % ( context, question )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gluing it together\n",
    "\n",
    "The next function, `answer_questions_from_doc`, that we create is created to help combine the previous three that we define. This is the wrapper that we will call when we want to interact with watsonx.ai. \n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions_from_doc(file_path,file_type,question,collection_name):\n",
    "\n",
    "    # Specify model parameters\n",
    "    model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "    max_tokens = 300\n",
    "    min_tokens = 100\n",
    "    decoding = DecodingMethods.GREEDY\n",
    "    temperature = 0.7\n",
    "\n",
    "    # Get the watsonx model\n",
    "    model = get_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "\n",
    "    # Get the prompt\n",
    "    complete_prompt = create_prompt(file_path, file_type, question, collection_name)\n",
    "\n",
    "    generated_response = model.generate(prompt=complete_prompt)\n",
    "    response_text = generated_response['results'][0]['generated_text']\n",
    "\n",
    "    # print model response\n",
    "    print(\"--------------------------------- Generated response -----------------------------------\")\n",
    "    print(response_text.strip(\"\\n\"))\n",
    "    print(\"*********************************************************************************************\")\n",
    "\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering some questions\n",
    "\n",
    "The next code cell will use all the previous code we've created so far to source information from the input documents and ask a question about them using watsonx.ai (Notice the return of the `answer_questions_from_doc`). \n",
    "\n",
    "To do so we'll pass in a question we want to ask, the file we want to reference for said question, and finally the name of the collection where the embeddings of the file exist.\n",
    "\n",
    "Go ahead and run the next code cell. **You will see output from this cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test answering questions based on the provided .txt file\n",
    "question = \"What did the president say about corporate tax?\"\n",
    "file_path = \"https://raw.githubusercontent.com/CloudPak-Outcomes/Outcomes-Projects/main/L4assets/watsonx.ai-Assets/Documents/state_of_the_union.txt\"\n",
    "collection_name = \"state_of_the_union_remote\"\n",
    "answer_questions_from_doc(file_path,FILE_TYPE_TXT, question, collection_name)\n",
    "\n",
    "# Test answering questions based on the provided .pdf file\n",
    "question = \"How can you build a Generative AI model?\"\n",
    "file_path = \"https://raw.githubusercontent.com/CloudPak-Outcomes/Outcomes-Projects/main/L4assets/watsonx.ai-Assets/Documents/Generative_AI_Overview.pdf\"\n",
    "collection_name = \"generative_ai_doc\"\n",
    "answer_questions_from_doc(file_path, FILE_TYPE_PDF, question, collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
