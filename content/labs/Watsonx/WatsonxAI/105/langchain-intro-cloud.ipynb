{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "032112c4",
      "metadata": {},
      "source": [
        "## 1. Programmatically using WatsonX.ai models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "515172b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell to install dependencies if running on the watsonx UI.\n",
        "\n",
        "import sys\n",
        "\n",
        "!{sys.executable} -m pip install \"ibm-watson-machine-learning>=1.0.320\" | tail -n 1\n",
        "!{sys.executable} -m pip install \"pydantic>=1.10.0\" | tail -n 1\n",
        "!{sys.executable} -m pip install langchain-ibm | tail -n 1\n",
        "!{sys.executable} -m pip install langchain-community | tail -n 1\n",
        "!{sys.executable} -m pip install -q sentence-transformers | tail -n 1\n",
        "!{sys.executable} -m pip install pypdf | tail -n 1\n",
        "!{sys.executable} -m pip install SQLAlchemy==2.0.29 | tail -n 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4adcdb35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the dependencies we need:\n",
        "import os\n",
        "from time import sleep\n",
        "try:\n",
        "    from langchain import PromptTemplate\n",
        "    from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "    from langchain.document_loaders import PyPDFLoader\n",
        "    from langchain.indexes import VectorstoreIndexCreator # Vectorize db index with chromadb\n",
        "    from langchain.embeddings import HuggingFaceEmbeddings # For using HuggingFace embedding models\n",
        "    from langchain.text_splitter import CharacterTextSplitter # Text splitter\n",
        "\n",
        "    from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
        "    from ibm_watson_machine_learning.foundation_models import Model\n",
        "    from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(e)\n",
        "\n",
        "print(\"Done importing dependencies.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30fbe90a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill in project_id and api_key below\n",
        "\n",
        "project_id = \"\"\n",
        "api_key = \"\"\n",
        "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\n",
        "\n",
        "\n",
        "\n",
        "if not api_key or not project_id:\n",
        "    raise Exception(\"One or more environment variables are missing!\")\n",
        "else:\n",
        "    creds = {\n",
        "        \"url\": ibm_cloud_url,\n",
        "        \"apikey\": api_key \n",
        "    }\n",
        "print(\"Done getting env variables.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a51cbd27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the WatsonX model\n",
        "params = {\n",
        "    GenParams.DECODING_METHOD: \"sample\",\n",
        "    GenParams.TEMPERATURE: 0.2,\n",
        "    GenParams.TOP_P: 1,\n",
        "    GenParams.TOP_K: 25,\n",
        "    GenParams.REPETITION_PENALTY: 1.0,\n",
        "    GenParams.MIN_NEW_TOKENS: 1,\n",
        "    GenParams.MAX_NEW_TOKENS: 20\n",
        "}\n",
        "\n",
        "llm_model = Model(\n",
        "    model_id=\"google/flan-ul2\",\n",
        "    params=params,\n",
        "    credentials=creds,\n",
        "    project_id=project_id\n",
        ")\n",
        "print(\"Done initializing LLM.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b25c008",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict with the model\n",
        "countries = [\"France\", \"Japan\", \"Australia\"]\n",
        "\n",
        "try:\n",
        "  for country in countries:\n",
        "    question = f\"What is the capital of {country}\"\n",
        "    res = llm_model.generate_text(question)\n",
        "    print(f\"The capital of {country} is {res.capitalize()}\")\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a52e2e8f",
      "metadata": {},
      "source": [
        "## 2. Prompt Templates & Chains\n",
        "\n",
        "In the previous example, the user input is sent directly to the Watsonx LLM, without using Langchain. This is a basic use case, but real applications are rarely so simple. When using an LLM in an application, you will usually need to reuse the same prompt across multiple scenarios. We will now replicate the previous example, but use an LLM chain. This allows us to:\n",
        "\n",
        "- Accept user input and contruct a prompt\n",
        "- Generate multiple prompts from a collection of data points in a dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "310c2bbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the prompt template\n",
        "prompt = PromptTemplate(\n",
        "  input_variables=[\"country\"],\n",
        "  template= \"What is the capital of {country}?\",\n",
        ")\n",
        "\n",
        "try:\n",
        "  # In order to use Langchain, we need to instantiate Langchain extension\n",
        "  lc_llm_model = WatsonxLLM(model=llm_model)\n",
        "  \n",
        "  # Define a chain based on model and prompt\n",
        "  chain = LLMChain(llm=lc_llm_model, prompt=prompt)\n",
        "\n",
        "  # Getting predictions\n",
        "  countries = [\"Sweden\", \"Mexico\", \"Vietnam\"]\n",
        "  for country in countries:\n",
        "    response = chain.invoke(country)\n",
        "    print(prompt.format(country=country) + \" = \" + response.capitalize())\n",
        "    sleep(0.5)\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "918a9df3",
      "metadata": {},
      "source": [
        "## 3. Simple sequential chains\n",
        "The utility of LangChain becomes apparent as we chain outputs of one model as input to another model. Here's a simple example where one generates a question which the other model answers.\n",
        "\n",
        "LangChain determines a model's output based on its response.  In our examples, the first model creates a response to the end prompt of \"Question:\" which LangChain maps as an input variable called \"question\" which it passes to the 2nd model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffda7c24",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create two sequential prompts \n",
        "pt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\n",
        "pt2 = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Answer the following question: {question}\",\n",
        ")\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23e4e1ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate 2 models (Note, these could be different models depending on use case)\n",
        "# Note the .to_langchain() method which returns a WatsonxLLM wrapper, like above.\n",
        "model_1 = Model(\n",
        "    model_id=\"google/flan-ul2\",\n",
        "    params=params,\n",
        "    credentials=creds,\n",
        "    project_id=project_id\n",
        ").to_langchain()\n",
        "model_2 = Model(\n",
        "    model_id=\"google/flan-ul2\",\n",
        "    credentials=creds,\n",
        "    project_id=project_id\n",
        ").to_langchain()\n",
        "\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35de1e4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construct the sequential chain\n",
        "prompt_to_model_1 = LLMChain(llm=model_1, prompt=pt1)\n",
        "prompt_to_model_2 = LLMChain(llm=model_2, prompt=pt2)\n",
        "qa = SimpleSequentialChain(chains=[prompt_to_model_1, prompt_to_model_2], verbose=True)\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34586549",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run our chain with the topic: \"an animal\"\n",
        "# Play around with providing different topics to see the output. eg. cars, the Roman empire\n",
        "try:\n",
        "  qa.invoke(\"an animal\")\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed7c152d",
      "metadata": {},
      "source": [
        "## 4. Easy Loading of Documents Using Lang Chain\n",
        "LangChain makes it easy to extract passages from documents so that you can answer questions based on your document's content. First download the example PDF file to your working folder: [what-is-generative-ai.pdf](https://github.com/ibm-build-lab/VAD-VAR-Workshop/blob/main/content/Watsonx/WatsonxAI/105/what-is-generative-ai.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1743ccd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load PDF document\n",
        "# pdf='what-is-generative-ai.pdf'\n",
        "file_path = \"https://raw.githubusercontent.com/CloudPak-Outcomes/Outcomes-Projects/main/L4assets/watsonx.ai-Assets/Documents/Generative_AI_Overview.pdf\"\n",
        "loaders = [PyPDFLoader(file_path)]\n",
        "# loaders = [PyPDFLoader(pdf)]\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9348f8c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Index loaded PDF\n",
        "index = VectorstoreIndexCreator(\n",
        "    embedding=HuggingFaceEmbeddings(),\n",
        "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a890f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize watsonx google/flan-ul2 model\n",
        "params = {\n",
        "    GenParams.DECODING_METHOD: \"sample\",\n",
        "    GenParams.TEMPERATURE: 0.2,\n",
        "    GenParams.TOP_P: 1,\n",
        "    GenParams.TOP_K: 100,\n",
        "    GenParams.MIN_NEW_TOKENS: 50,\n",
        "    GenParams.MAX_NEW_TOKENS: 300\n",
        "}\n",
        "model = Model(\n",
        "    model_id=\"google/flan-ul2\",\n",
        "    params=params,\n",
        "    credentials=creds,\n",
        "    project_id=project_id\n",
        ").to_langchain()\n",
        "\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d1a50d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Init RAG chain\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "chain = RetrievalQA.from_chain_type(llm=model, \n",
        "                                    chain_type=\"stuff\", \n",
        "                                    retriever=index.vectorstore.as_retriever(), \n",
        "                                    input_key=\"question\")\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b6efed8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Answer based on the document\n",
        "res = chain.invoke(\"what is Machine Learning?\")\n",
        "print(res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc2679f2",
      "metadata": {},
      "source": [
        "Retrieval Augmented Generation (RAG) is a common AI use case. Many companies have vast amounts of data about which they want an AI system to answer questions, do searches or perform summarization tasks. We will learn more about RAG in lab 106."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
