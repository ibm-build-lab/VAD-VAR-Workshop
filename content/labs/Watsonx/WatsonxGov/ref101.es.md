---
title: 'Ref 101: Atlas de riesgos de IA'
updated: 2024-04-15
---
# Atlas de riesgos de IA

Con la llegada y el crecimiento exponencial de la IA, en concreto de la IA generativa mediante el uso de LLM (Large Language Models), los riesgos asociados también han crecido a un ritmo similar. Las organizaciones que basan sus decisiones en resultados injustos o sesgados pueden enfrentarse a auditorías, multas, costosos litigios y daños a su reputación.

Ahora es más importante que nunca que las organizaciones adopten un enfoque más proactivo a la hora de detectar y mitigar los riesgos, controlar la imparcialidad, la parcialidad y la desviación, y realizar un seguimiento de las nuevas métricas generativas del LLM a medida que el campo sigue expandiéndose.

A continuación se muestra un ejemplo de algunos de los riesgos de la IA divididos en 3 áreas comunes y clasificados en tres categorías: riesgos ***tradicionales de la IA** (aplicables tanto a los modelos tradicionales como a la IA generativa)*, ***riesgos amplificados por la IA generativa** (también aplicables a los modelos tradicionales)* y ***nuevos riesgos asociados específicamente a la IA generativa***. [En la página del Atlas de Riesgos de la IA](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=ai-risk-atlas) encontrará una lista completa y detallada de los distintos riesgos, junto con explicaciones y ejemplos.

## Riesgos asociados a las entradas

### [Sesgo de los datos](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=atlas-data-bias)

Los sesgos presentes en los datos históricos pueden dar lugar a un tratamiento injusto de determinados grupos en las predicciones del modelo. Estos sesgos pueden ser el resultado de prejuicios sociales e injusticias históricas, que a menudo se reflejan en los datos utilizados para entrenar modelos de IA. Abordar estos sesgos requiere una cuidadosa consideración de las fuentes de datos y el desarrollo de modelos que sean robustos frente a las posibles fuentes de sesgo.

### [Riesgo de inyección precoz para la IA](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=atlas-prompt-injection)

Los ataques de inyección de prompts son un tipo de exploit de seguridad en el que un atacante manipula el prompt de entrada proporcionado a un modelo para provocar que genere salidas incorrectas o inesperadas. Estos ataques pueden aprovecharse de la estructura, los sesgos o los patrones aprendidos del modelo, y pueden ser especialmente peligrosos en modelos utilizados para aplicaciones críticas de seguridad. Para mitigar el riesgo de ataques de inyección de instrucciones, es esencial aplicar técnicas adecuadas de validación y saneamiento de entradas, así como garantizar que los datos de entrenamiento y las instrucciones del modelo se inspeccionan y protegen minuciosamente.

### [Riesgo de jailbreaking para la IA](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=atlas-jailbreaking)

Jailbreaking en IA y LLM se refiere a un asalto que va más allá de los límites establecidos para el modelo, intentando manipular o explotar sus capacidades más allá de su diseño previsto. Esta acción no autorizada puede tener consecuencias imprevistas y comprometer potencialmente la integridad y seguridad del modelo.

## Riesgos asociados a la producción

### [Riesgo de infracción de los derechos de autor para la IA](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=atlas-copyright-infringement)

Para evitar posibles infracciones de los derechos de autor, es fundamental que los modelos de generación de contenidos produzcan contenidos únicos y originales. Esto garantiza que el material generado no se parezca demasiado a obras existentes, respetando así los derechos de propiedad intelectual de los creadores y evitando repercusiones legales. Al adherirse a este principio, los creadores de modelos pueden ayudar a mantener la integridad del proceso de creación de contenidos y promover la innovación en este campo.

### [Riesgo de alucinación para la IA](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=atlas-hallucination)

Los LLM pueden generar información falsa o engañosa, lo que puede tener graves consecuencias. Por ejemplo, pueden perjudicar a personas, dañar su reputación e incluso acarrear repercusiones legales. Es crucial que los desarrolladores y usuarios de LLM se aseguren de que el contenido generado es preciso y fiable.

### [Riesgo de desinformación para la IA](https://www.ibm.com/docs/en/SSYOK8/wsj/ai-risk-atlas/spreading-disinformation.html)

Los actores maliciosos pueden explotar modelos de generación de texto basados en IA para crear y difundir información engañosa o falsa, conocida como deepfakes. Estas sofisticadas falsificaciones pueden diseñarse para suplantar la identidad de personas o entidades, lo que dificulta la distinción entre contenidos reales y falsos. Los deepfakes pueden utilizarse para manipular la opinión pública, difundir desinformación y socavar la confianza en diversos ámbitos, como la política, el entretenimiento y el periodismo. Para mitigar el riesgo de deepfakes, es crucial desarrollar técnicas sólidas de detección y verificación y promover la alfabetización digital para ayudar a la gente a identificar y evitar caer en estos medios engañosos.

## Riesgos no técnicos

### [Falta de transparencia de los datos](https://www.ibm.com/docs/en/SSYOK8/wsj/ai-risk-atlas/lack-of-data-transparency.html)

Documentar los datos utilizados para la formación de modelos es crucial para garantizar el cumplimiento de los requisitos legales, como demostrar la representatividad de los datos y esbozar el comportamiento esperado del modelo. Una documentación insuficiente puede dificultar la explicación de los fundamentos de las decisiones del modelo y acarrear responsabilidades legales.

### [Responsabilidad jurídica](https://www.ibm.com/docs/en/SSYOK8/wsj/ai-risk-atlas/legal-accountability.html)

El riesgo de responsabilidad jurídica de la IA se refiere al reto de identificar y responsabilizar a la parte adecuada de las acciones y decisiones tomadas por los sistemas de inteligencia artificial, en particular cuando estos sistemas causan daños o cometen errores. Los usuarios de modelos sin una titularidad clara podrían tener dificultades para cumplir la futura normativa sobre IA.

### [Plagio](https://www.ibm.com/docs/en/SSYOK8/wsj/ai-risk-atlas/plagiarism.html)

El plagio, ya sea intencionado o no, implica el uso de modelos de IA para replicar o imitar el estilo, la estructura y el contenido de obras existentes sin atribuirlas o citarlas debidamente. Esto puede dar lugar a problemas relacionados con la originalidad, la credibilidad y los derechos de propiedad intelectual. Es esencial desarrollar y utilizar los modelos de IA de forma responsable, asegurándose de que están diseñados para promover la creatividad y la originalidad, respetando al mismo tiempo los derechos y las obras originales de otros.
