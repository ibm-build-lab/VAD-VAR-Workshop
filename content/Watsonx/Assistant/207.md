---
title: '207: LLM-powered Conversational Search'
timeToComplete: 45
updated: 2024-02-01
---

# 207: LLM-powered Conversational Search

## In this section

<QuizAlert text='Heads up! Parts of this section will be on the quiz.'/>

Learn about IBM watsonx Assistant’s large language model (LLM)-powered Conversational Search: what it is, how it works, how to set it up, and how to use it.

## Prerequisites

This section does not have to be done sequentially with the other lab sections. You may either use the watsonx Assistant instance from the previous sections of this lab or create a new one. 
This lesson will provision Assistant and Discovery services from the IBM Technology Zone (TechZone), so you don’t have to start a Watson Discovery trial.


## What is Conversational Search?

This section will focus on Conversational Search, a large language model (LLM)-powered feature that allows your virtual assistant to answer questions conversationally on a wide range of topics.

### Business Context

Virtual assistants are built to answer a wide range of frequently asked questions (FAQs). In watsonx Assistant, builders have historically built anywhere from dozens to hundreds of _Actions_ to answer FAQs.

Over time, answers change as an organization’s processes, products, or services evolve. Builders must periodically manually review the _Actions_ they have built to ensure their answers are accurate and up to date.

_Actions_ that answer FAQs can also be challenging to build because end users can ask questions in different ways. It can be challenging for builders  to write an answer that addresses all these potential questions without sounding robotic or unintelligent.

Conversational Search helps builders answer FAQs more easily by using generative AI (specifically, an LLM) to provide answers. To ensure that generative AI creates accurate, relevant, and up to date answers to FAQs, Conversational Search first searches for relevant information in the company’s knowledge base and then feeds that information into the LLM so that the LLM can generate an answer grounded in the company-specific information.

### General benefits

- **Faster build time:** Conversational Search essentially automates the process of answering FAQs. Builders no longer need to manually write or build _Actions_ to respond to FAQs, saving them dozens of hours of work for the inital build time.

- **Less maintenance required:** Conversational Search generates answers grounded in the company’s knowledge base which contains the company's most recently updated information. As a result, builders no longer need to manually maintain or update _Actions_ to respond to FAQs, saving them hundreds of hours of maintenance.

- **Answers are truly conversational:** Answers generated by Conversational Search are unlikely to sound robotic. They are generated in real-time as a response to the end user’s unique style of questioning. 

### Framework: Retrieval-augmented generation (RAG)

Conversational Search uses an AI framework called retrieval-augmented generation (RAG). RAG is a very popular starting point for enterprises that are just beginning to deploy generative AI. 

RAG has two main benefits: 
 - It ensures that the LLM generating an answer to a question has 
access to the most current, reliable facts relevant to that question.
 - Users (both end users and builders) have access to the LLM’s sources ensuring that its answers can be traced, checked for accuracy, and ultimately trusted.

Before continuing, and if you are not already familiar with RAG, either watch this well-made informational [video](https://www.youtube.com/watch?v=T-D1OfcDW1M) or read this well-written informational [article](https://research.ibm.com/blog/retrieval-augmented-generation-RAG) that were created by the IBM Research team.

### Process

There are two general ways to implement RAG in watsonx Assistant. 
First, you can use Conversational Search, the native, out-of-the-box, no-code feature built in to watsonx Assistant. 
Second, you can set up custom extensions to set up a custom implementation of RAG with low code.
This lab will primarily focus on Conversational Search, the native, no-code pattern built into watsonx Assistant.

If the assistant does not recognize a question and it can't answer it using one of its actions for which it has been trained on, the question will go to Conversational Search.

The Conversational Search process is shown and explained in detail below:

  ![](./images/207/convo-search-process.png)

  1. The virtual assistant sends the end user’s question to a search tool - in this exercise, Watson Discovery. Watson Discovery has read and processed all relevant corporate documents.

  2. The search tool (Watson Discovery) will then search its content and produce search results in response to the question.

  3. The search tool passes these search results back to the virtual assistant in a list.

  4. At this point, watsonx Assistant could display the results back to the user. However, they would not resemble natural speech; they would look more like a set of search results. Possibly helpful but not summarized and presented concisely and conversationally.

  5. Therefore, watsonx Assistant sends the question and the list of search results to watsonx, which invokes an LLM.
  
  6. In some implementations of RAG, an LLM re-ranks the search results. It may reorder or disregard some of the search results according to how relevant and useful it thinks the search results are to the question. For example, an LLM might decrease the ranking of a search result if it is from a document not recently updated, indicating the information may be outdated. This capability is often called a **neural re-ranker** and is coming soon to watsonx Assistant’s Conversational Search.

  7. The LLM generates an answer to the question using the information in the search results, and it passes this answer back to watsonx Assistant.

  8. The virtual assistant presents this conversational answer to the end user.


### LLMs in Conversational Search

IBM watsonx Assistant commercially and technically **embeds** watsonx.ai large language models, so customers need only purchase watsonx Assistant to embed generative AI functionality into their virtual assistants.

Other watsonx LLMs or non-watsonx LLMs can also be fine-tuned or prompt-engineered to perform RAG or other generative AI use cases. These LLMs can be integrated with watsonx Assistant via custom extensions. This pattern may be preferable to clients who train their own LLMs in-house or want customized functionality beyond what is available natively in watsonx 
Assistant through Conversational Search.

### Demonstrating conversational search

This section of the lab gives you two options to implement Conversational Search with watsonx Assistant.

**Option 1** implements watsonx Assistant, by custom extension, Watson Discovery, and Llama 2.

**Option 2** uses Beta functionality, specifically the watsonx Assistant native Conversational Search feature, which does not require custom extensions or watsonx.ai. The native Conversational Search capability is faster and easier to deploy, however it is still in Beta.

Pick the option most appropriate for your use case or learning needs, and continue.


## Option 1: Setting up conversational search in your virtual assistant using Llama2

> **Important:** This section of this lab assumes you have your TechZone enrionment running.

### Create a watsonx project

1. Navigate to [watsonx](https://dataplatform.cloud.ibm.com/)

2. Once on the main watsonx page, first, ensure you are under the same TechZone account that your watsonx Assistant is in.

    > Note: your TechZone account may be different from the screenshot.

    ![](./images/207/verify-env.png)

3. Create a new project by clicking the **+** button under **Projects**:

    ![](./images/207/new-project.png)

    a. On the **New project** screen:
      - Enter a **Name** that is meaningful to you.
      - Optionally, enter a **Description**.
      - The **Object storage** service will be automatically populated (your service name will be different from the screenshot)
      - Click **Create**.

      ![](./images/207/create-new-project.png)
  
4. You will be redirected to the **Overview** tab of your new watsonx project

      ![](./images/207/overview-page.png)

5. You will need the watsonx project ID to set up the action that calls the watsonx custom extension in Assistant later on. To get the watsonx project ID, select the **Manage (1)** tab, then click **copy (2)**.

    > Note: Save this project ID in a notepad for now, as you will need it shortly. Label it properly, as you will be copying a few more IDs and URLs.

    ![](./images/207/save-project-id.png)

6. Next, you will link the automatically provisioned Watson Machine Learning service instance to this new watsonx project. To do this, select **Services & integrations (1)** and click **Associate service + (2)**:

    ![](./images/207/associate-service.png)

7. On the next screen, select the **Watson Machine Learning instance (1)** and click **Associate (2)**. 
    > Note: Your service instance will have a different name than shown in the screenshot.

    ![](./images/207/existing-wml.png)

8. Setting up your watsonx extension in Assistant will also require an API key from your IBM Cloud account. 
  To get the API key, in a new browser tab:
   - Navigate to [Manage access and users - API Keys in IBM Cloud](https://cloud.ibm.com/iam/apikeys), which will take you to the following screen. Then, click **Create +**.

       ![](./images/207/api-key.png)
  
    - On the popup Create IBM Cloud API key screen, enter a **Name** and **Description** then click **Create**.

       ![](./images/207/create-api-key.png)

    - When you see the notification API key successfully created, click **Copy**:

        ![](./images/207/copy-api-key.png)


      Save this API key somewhere safe and accessible. You will need this API key later to set up the watsonx custom extension in Assistant. You can close this IBM Cloud browser tab.

### Load a document into Watson Discovery

1. Go to [My Reservations](https://techzone.ibm.com/my/reservations) on TechZone. Click on your instance tile:

    ![](./images/207/instance-tile.png)

2. Click the **Watson Discovery URL**

    ![](./images/207/watson-discovery-url.png)

3. On the Watson Discovery launch page, copy the **API Key (1)** and **service URL (2)** and save them. You will need these to configure the Watson Discovery custom extension in watsonx Assistant. Then, click **Launch Watson Discovery (3)**.


    ![](./images/207/discovery-creds.png)

4. On the welcome page, click **New Project +**

    ![](./images/207/new-project-btn.png)

5. Enter a **project name (1)** such as `Lendyr documents` and select **Document Retrieval (2)**, then click **Next (3)**.

    ![](./images/207/project-type.png)

6. Download the [Lendyr FAQ document]((https://ibm.seismic.com/app?ContentId=39aeac7d-8d54-4a71-842c-d7ef019f3ee2)). This is the file you will upload to Watson Discovery. Once downloaded, Enter a **collection name (1)** such as `Lendyr documents`, click **Drag and drop files here or upload (2)**, and hit **Finish (3)**

    ![](./images/207/create-collection.png)

  > Note: The file uploads relatively quickly, however it may take 15 minutes for Watson Discovery to process the document.
