---
title: '109: Large language model application building blocks'
timeToComplete: 45
updated: 2024-01-30
---

<QuizAlert text='Heads Up! Quiz material will be flagged like this!' />

# Large language model application building blocks

This hands-on exercise will show you how to integrate large language models (LLMs) with client applications. We will review several Python code samples which can be used as “building blocks” for an LLM application.
We will then use a simple UI prototype implemented with [Streamlit](https://streamlit.io/) to show how these building blocks can be invoked from client applications.

Prompt engineering is just one of the steps in the process of integrating LLMs into business applications. It is outside the scope of this guide to provide an introduction to prompt engineering in watsonx.ai. For an introduction to prompt engineering in watsonx.ai, check out the [VEST watsonx.ai L3 Labs](/watsonx/watsonxai)

## Prerequisites

- Access to watsonx.ai.
- Python IDE with Python 3.10 environment
  - We will be using the Python IDE, [Visual Studio Code (VSCode)](https://code.visualstudio.com/)
- You will also need to download the lab files from [this GitHub folder](https://github.com/ibm-build-lab/VAD-VAR-Workshop/tree/main/content/Watsonx/WatsonxAI/109)
  - We will refer to this folder as the _repo_ folder.


## Review scripts for various LLM tasks

In this section we will review 4 Python scripts and 2 notebooks that can be used as building blocks for LLM applications.

> Note: Prompts and model configuration in the scripts may not always return perfect results. If you wish, you can modify prompts and model parameters.

1. Load the following scripts from the repo/scripts folder into your Python IDE:

    - use_case_infererence.py
    - use_case_summary.py
    - use_case_generate.py
    - use_case_transform.py

    As you can tell from the script names, the scripts contain prompts for most common LLM use cases. 
    Each script follows a similar code structure:

     - The _ibm-watson-machine-learning_ library is used to invoke LLMs deployed in watsonx.ai.
     - An IBM Cloud API key and watsonx project id are retrieved from the _.env_ file (environmental file) which we will create in a later step in this lab.
    
        ![credentials](./images/109/credentials.png)


     - The **get_model()** parameterized function creates a model object that will be used to invoke the LLM.

        ![get_model](./images/109/get_model.png)

     - The **get_prompt()** function will create the final prompt that will be passed to the model. The function will use text from a passed in parameter from either the **get_review()** or **get_sample_text()** function which will be analyzed by the LLM.

        > It's possible to organize the code without hardcoding the instruction part of the prompt, but we will not cover it in this lab.

        ![get_prompt](./images/109/get_prompt.png)

     - The **main()** function specifies the model parameters and invokes the other functions. The function prints LLM output to the terminal. 
    
        ![main](./images/109/main.png)

     - Lastly, each script contains at least one function that can be called by an external Python module. For example, in _use_case_generate.py_, the function name is **generate**.

2. Prior to running the scripts, create a _.env_ file in the root directory of your project and add your IBM Cloud API key and watsonx project id. 



 
