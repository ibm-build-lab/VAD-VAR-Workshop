# 107: Implement RAG Use Cases (New)

In this lab you will review and run examples of LLM applications that implement the _Retrieval Augmented Generation (RAG)_ pattern of working with LLMs. We will expand on the concepts that you learned in the previous labs.

## Required software, access, and files

Requi

- To complete this lab you will need access to **watsonx.ai**
- Maybe a little knowledge of Python

## RAG Overview

_RAG (Retrieval-Augmented Generation)_ is one of the most common use cases in generative AI because it allows us to work with data "external to the model", for example, data that was not used for model training. Many use cases require working with proprietary company data, and it's one of the reasons why RAG is frequently used in generative AI applications. RAG also allows us to add some guardrails to generated output and reduce hallucination.

RAG can be implemented for various AI use cases, including:

- Question and answer
- Summarization
- Content generation

> A "human interaction" analogy of RAG is providing a document to a person and asking them to answer a question based on the information contained within.

RAG is "an application pattern" than can be implemented with multiple technologies. However, there are two key steps:

- During the **retrieval** step we searched through a knowledge base (documents, websites, databases, etc.) to find information relevant to the instructions sent to the model. Retrieval can be based on keyword search or more advanced algorithms, like semantic similarity.
- Then **generation** step is similar to generation in non-RAG use cases. The main difference is that information retrieved in the first step is provided as "context" (included in the prompt). Prompting the model to rely on "retrieved context" only, in contrast to the general knowledge which was used to train the LLM, is the key step for preventing hallucinations.

![rag_diagram](./images/107/2.png)

### Vector databases

The "knowledge base" for the retrieval step is typically implemented as a **vector database**. Vector databases are not new, they have been used for several years for semantic search.

Semantic search is a search technique that aims to understand the intent and context of a user's query. It's more advanced than traditional keyword-based search, which relies on matching specific keywords.

Some popular vector databases are:

- Chroma (open-source)
- Milvus
- Elasticsearch
- SingleStore
- Pinecone
- Redis
- Postgres

Some vector databases, like _Chroma_, are an in-memory database, which makes it a good choice for experimenting and prototyping RAG use cases. In-memory database do not need to be installed or configured. This is because all of their data is stored in volatile memory for the duration of the program's runtime. The downside of this though is that we will need to load the data each time we run the application.

Vector databases store unstructured data in a numeric format. For AI use cases, we use a specific term to describe the converted unstructured data - **_embeddings_**. Embeddings are created by using an embedding model, for example, a popular open source model _word2vec_.

One of the key features of embeddings is the ability to preserve relationships. With embeddings, words can be "added and subtracted" like vectors in math. One of the most famous examples that demonstrate this is the following:

```txt
king - man + woman = queen
```

In other words, adding the vectors associated with the words _king_ and _woman_ while subracting _man_ is equal to the vector associated with the word _queen_. This example describes a gender relationship.

Another example might be:

```txt
Paris - France + Poland = Warsaw
```

The vector difference between _Paris_ and _France_ here captures the concept of a capital city.

Here's an example of semantic search for the word "education". Notice the "nearest point" words in the table. The measurement is in numbers because the search was performed using vector (numeric) data.

You can experiment further with this [here](https://projector.tensorflow.org/).

![semantic_search_example](./images/107/3.png)

Based on what we have discussed so far, we can make the follow conclusions:

- We have a choice of _vector databases_ for implementations of RAG use cases
- We have a choice of several _embedding models_ for coverting unstructured data to vectors
- Retrieval of relevant information is the first step in the RAG pattern, and the quality may be affected by the choice of a vector database and an embedding model.

<QuizAlert />

In addition to the mentioned factors, loading data into the vector database requires **_"chunking_"**, which is the process of dividing text into sections that are loaded into the database. There is no single guidline for defining chunks because the best size depends on the content we're working with.

For generative AI use cases we typically start with _fixed-size chunks_, which sould be "semantically meaninguful". For example, if in your document each paragraph has approximately 200 words, convert the number of words to number of tokens (may very per LLM), and use this number as the starting chunk size. In addition to the size of the chunk, we also need to specify overlapping (same information in more than one chunk).

Model providers should publish information on recommended chunk size and overlap percentage for specific use cases. However, even if this information is published, developers will still need to experiment to find the optimal chunk size. If you can't find information on the recommend overlap, you can start with 10 percent (specified in the format of "number of tokens", for example, _100_ if your chunk size _1000_).

To summarize, we should follow these steps when implementing the RAG use case:

1. Select a vector database. When working with watsonx.ai, we should confirm that the selected vector database is supported. [Check IBM documentation for the latest updates](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-rag.html?context=wx&audience=wdp).
2. Select the embeddings model. Confirm that the embeddings model works with the selected vector database.
3. Experiment with chunk size.

As we discussed earlier, the second part of the RAG use case, sending instructions to LLM, is similar to non-RAG patterns. Developers should pay special attention to token limit because retrieved content will be added to the prompt. If the token limit is exceeded on input, then the model will not be able to generate ouput. This check will need to be implemented as "custom code" in your LLM applications.

WML API does return errors if token size is exceeded.

![wml_error](./images/107/4.png)

## Rag on documents with LangChain and ChromaDB
