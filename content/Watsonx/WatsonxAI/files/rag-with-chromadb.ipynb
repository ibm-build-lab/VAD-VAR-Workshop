{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation using watsonx.ai and Vector Database\n",
    "\n",
    "In this notebook, we'll demonstrate how to utilize a Vector Database to retrieve relevant passages based on a user query. We'll then append these passages as context to the prompt that will be passed to the LLM in watsonx.ai for generation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a powerful technique that combines the strengths of pre-trained large language models (LLM) and information retrieval systems to generate responses based on a given context. In this notebook, we will be using a Vector Database and watsonx.ai foundation models to implement a RAG use-case.\n",
    "\n",
    "A vector database (or store), when applied to text data, is a specialized database that efficiently stores embeddings, representing pieces of text, for efficient  queries. It enables quick similarity searches, allowing you to pinpoint texts that are _'similar'_ based on their vectorized representations. For our purposes, we will use Chroma, an open-source embedding database.\n",
    "\n",
    "Instead of using Watson Discovery to pass back the relevant passages, we are using a vector database called Chroma. Chroma is mainly used to parse through the PDFs, store the content, and then query from that collection. The code in the notebook below demonstrates the implementation of this approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "This lab should take about 45 minutes.\n",
    "\n",
    "Before we begin lets start off by ensuring we have completed some pre-requisites; ensure you gave the following\n",
    "\n",
    "- IBM Cloud API key \n",
    "- Project ID associated with your watsonx instance\n",
    "\n",
    "You can use the following support links if you need any help with the pre-requisites above\n",
    "\n",
    "- [Creating IBM Cloud API Key](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui#create_user_key)\n",
    "- [Finding watsonx Project ID](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=library-project-id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Required Libraries\n",
    "\n",
    "Before we get started looking at some code, we will need to install some dependencies for our notebook; the following notebook cell will do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dependencies\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q langchain\n",
    "!{sys.executable} -m pip install -q chromadb\n",
    "!{sys.executable} -m pip install -q pypdf\n",
    "\n",
    "!{sys.executable} -m pip install -q ibm_cloud_sdk_core\n",
    "!{sys.executable} -m pip install -q ibm_watson_machine_learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and packages\n",
    "\n",
    "from ibm_cloud_sdk_core import IAMTokenManager\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "\n",
    "import langchain.embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.pdf import PyPDFLoader\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Optional, Iterable, List\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding & Vector Database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Embeddings Class\n",
    "\n",
    "To start off we will create a custom class, **MiniLML6V2EmbeddingFunctionLangchain**, and define some functions which are designed to generate embeddings using the `MiniLM-L6-v2` model from the `sentence_transformers` library. This class will serve as our embedding function where text we want to store in vector format will be processed before being stored within a vector database. As a quick reminder, embeddings are used to create a vector representation of the text data and capture the semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniLML6V2EmbeddingFunctionLangchain(langchain.embeddings.openai.Embeddings):\n",
    "    MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    def embed_documents(self, texts):\n",
    "        return MiniLML6V2EmbeddingFunctionLangchain.MODEL.encode(texts).tolist()\n",
    "    \n",
    "    def embed_query():\n",
    "        super().embed_query()\n",
    " \n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating A VectorDB Class\n",
    "\n",
    "We will also create a custom class, **ChromaWithUpsert**, which is an abstraction using `Chroma` class from the `Chroma` vectorstore class in the langchain module. Using this class we introduce the ability to _upsert_ texts within the vector database _(either adding or updating)_. The _upsert_texts_ method from our class takes in the text content, their metadata _(i.e. source document)_, and their ids _(if provided)_, and generates the embeddings using the class defined earlier before adding the newly created vector in to the `Chroma` vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaWithUpsert(Chroma):\n",
    "    def upsert_texts(\n",
    "        self,\n",
    "        texts: Iterable[str],\n",
    "        metadatas: Optional[List[dict]] = None,\n",
    "        ids: Optional[List[str]] = None,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n",
    "        Args:\n",
    "            texts (Iterable[str]): Texts to add to the vectorstore.\n",
    "            metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n",
    "            ids (Optional[List[str]], optional): Optional list of IDs.\n",
    "        Returns:\n",
    "            List[str]: List of IDs of the added texts.\n",
    "        \"\"\"\n",
    "        \n",
    "        if ids is None:\n",
    "            import uuid\n",
    "            ids = [str(uuid.uuid1()) for _ in texts]\n",
    "        embeddings = None\n",
    "\n",
    "        if self._embedding_function is not None:\n",
    "            embeddings = self._embedding_function.embed_documents(texts = list(texts))\n",
    "\n",
    "        self._collection.upsert(\n",
    "            metadatas=metadatas, embeddings=embeddings, documents=texts, ids=ids\n",
    "        )\n",
    "        return ids\n",
    "    \n",
    "    def query(self, query_texts:str, n_results:int=5, include: Optional[List[str]]=None):\n",
    "        return self._collection.query(\n",
    "            query_texts=query_texts,\n",
    "            n_results=n_results,\n",
    "            include=include\n",
    "        )\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and Splitting PDF Text Content\n",
    "\n",
    "In the following cell we are loading PDF documents using the **PyPDFLoader** class and storing it in the data variable. Our PDF is being loaded from a URL and will be used to represent our existing knowledge base.\n",
    "\n",
    "The loaded data is then split into smaller chunks using the **RecursiveCharacterTextSplitter** class, which allows us to split long text on predefined characters that are considered potential division points . The size of the chunks and the overlap between them is defined by `CHUNK_SIZE` and `CHUNK_OVERLAP` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"https://www.captiveaire.com/manuals/exhaustfans/exhaust-oim.pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 10\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP\n",
    "        )\n",
    "texts = text_splitter.split_documents(data)\n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving texts to a VectorDB\n",
    "\n",
    "\n",
    "Once our loaded text is split we can now can create an instance of our vector database using the `ChromaWithUpsert` class with our custom embedding function and a collection name. Once defined, using the `upsert_texts` method, we add the split texts and their metadata to the vector database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaWithUpsert(\n",
    "    collection_name=f\"store_minilm6v2\",\n",
    "    embedding_function=MiniLML6V2EmbeddingFunctionLangchain(),\n",
    ")\n",
    "\n",
    "vector_store.upsert_texts(\n",
    "        texts=[doc.page_content for doc in texts],\n",
    "        metadatas=[doc.metadata for doc in texts]\n",
    ")\n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Language Learning Model (LLM)\n",
    "\n",
    "In this cell, we are setting up the parameters for the Language Learning Model (LLM). This includes our IBM Cloud API Key and watsonx Project ID in order to make use of `watsonx.ai` foundation models. Default tuning parameters (gen) are provided, but can be adjusted as needed; aAfter setting up these parameters, we will use them to initialize our LLM (watsonx.ai) in the next cell.\n",
    "\n",
    "If you want to learn more about watsonx.ai foundation models tuning paremeter, you can visit the watsonx.ai foundation [documentation link here](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-parameters.html?context=wx&audience=wdp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your IBM Cloud API key\n",
    "api_key = \"INSERT YOUR API KEY HERE\"\n",
    "\n",
    "# Project ID of your watsonx project\n",
    "watsonx_project_id = \"INSERT YOUR watsonx PROJECT ID HERE\"\n",
    "\n",
    "# LLM that we want to use with watsonx.ai\n",
    "model_id= \"google/flan-ul2\"\n",
    "\n",
    "endpoint= \"https://us-south.ml.cloud.ibm.com\"\n",
    "\n",
    "access_token = ''\n",
    "\n",
    "try:\n",
    "  access_token = IAMTokenManager(\n",
    "    apikey = api_key,\n",
    "    url = \"https://iam.cloud.ibm.com/identity/token\"\n",
    "  ).get_token()\n",
    "except:\n",
    "  print('Issue obtaining access token. Check variables?') \n",
    "\n",
    "credentials = { \n",
    "    \"url\"    : endpoint, \n",
    "    \"token\" : access_token\n",
    "}\n",
    "\n",
    "# watsonx.ai tuning parameters\n",
    "gen_params = {\n",
    "    \"DECODING_METHOD\" : \"greedy\",\n",
    "    \"MAX_NEW_TOKENS\" : 300,\n",
    "    \"MIN_NEW_TOKENS\" : 1,\n",
    "    \"TEMPERATURE\" : 0.7,\n",
    "    \"TOP_K\" : 50,\n",
    "    \"TOP_P\" : 0.15,\n",
    "    \"REPETITION_PENALTY\" : 2.0\n",
    "}\n",
    "\n",
    "model = Model( model_id, credentials, gen_params, watsonx_project_id )\n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining watsonx.ai LLM and VectorDB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constrcut Query & Identify the relevant texts in the documents\n",
    "\n",
    "In this cell, we are constructing the query prompt for the Language Learning Model (LLM). The query is the question that we want to ask our foundation model. This question will be used to retrieve the relevant texts from the documents in our vector database.\n",
    "\n",
    "We specify the number of text passages we want returned from our vector database using the `search_k` variable _(in this case, we use 3)_. If you find that you are not getting very good answers, you can increase the `search_k` variable, in order to increase the amount of context (number of matching passages) provided to \n",
    "\n",
    "We will store the best relevant text passage along with its metadata and distances, which identify the source and page number and join them all into our `context` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'For power roof ventilators should dampers be installed when an exhauster is used?'\n",
    "\n",
    "search_k = 13\n",
    "docs = []\n",
    "docs = vector_store.query(\n",
    "            query_texts=[question],\n",
    "            n_results=search_k,\n",
    "            include=[\"documents\",\"metadatas\", \"distances\"]\n",
    "        )\n",
    "\n",
    "context = \" \".join(docs[\"documents\"][0])\n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the Prompt & Query watsonx.ai\n",
    "\n",
    "Now, we combine the query and the context we received from the vector database into a prompt. We created a custom function to take in both the query and context.\n",
    "\n",
    "We will then query our foundation model from watsonx.ai that we created earlier; given that we _upserted_ the documents with the metadata of the source and documents, we can identify which document and where in that document that we are using text context from in order to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "prompt_template = \"\"\"\n",
    "Answer the following question using the context provided. \n",
    "If there is no good answer, say \"I don't know\".\n",
    "\n",
    "Context: %s\n",
    "\n",
    "Question: %s\n",
    "\"\"\"\n",
    "\n",
    "#######################################################################################\n",
    "def augment( template_in, context_in, query_in ):\n",
    "    return template_in % ( context_in,  query_in )\n",
    "\n",
    "#######################################################################################\n",
    "import json\n",
    "\n",
    "def generate( model_in, augmented_prompt_in ):\n",
    "    \n",
    "    generated_response = model_in.generate( augmented_prompt_in )\n",
    " \n",
    "    if ( \"results\" in generated_response ) \\\n",
    "       and ( len( generated_response[\"results\"] ) > 0 ) \\\n",
    "       and ( \"generated_text\" in generated_response[\"results\"][0] ):\n",
    "        return generated_response[\"results\"][0][\"generated_text\"]\n",
    "    else:\n",
    "        print( \"The model failed to generate an answer\" )\n",
    "        print( \"\\nDebug info:\\n\" + json.dumps( generated_response, indent=3 ) )\n",
    "        return \"\"\n",
    "\n",
    "########################################################################################\n",
    "import re\n",
    "\n",
    "augmented_prompt = augment( prompt_template, context, question)\n",
    "output = generate( model, augmented_prompt )\n",
    "if not re.match( r\"\\S+\", output ):\n",
    "    print( \"The model failed to generate an answer\")\n",
    "print( \"\\nAnswer:\\t\" + output )\n",
    "\n",
    "source_file = docs['metadatas'][0][0]['source']\n",
    "page = docs['metadatas'][0][0]['page']\n",
    "\n",
    "print('\\nSource\\t', source_file)\n",
    "print('Page\\t',page)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations you just completed a RAG implementaion using VectorDB. Feel free to re-run the prompt by asking other questions or change the PDF used to provide watsonx.ai with a different context. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
