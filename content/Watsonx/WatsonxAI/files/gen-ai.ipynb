{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accesing Watsonx.ai via Python Library\n",
    "\n",
    "In this notebook, we will take a look at using a Python libray to instead access the functionality of Watsonx.ai. This notebook will cover all the same use cases of the previous lab but outline the differences in approach.\n",
    "\n",
    "Before you start you should once again have the necessary items to access Watsonx.ai programmatically, them being:\n",
    "\n",
    "- your IBM Cloud API key\n",
    "- the IBM Cloud regional URL (eg)\n",
    "- the Project ID associated with your Watsonx instance\n",
    "\n",
    "This lab should take about 25-30 min to walk through at your own pace. \n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we'll start by installing some dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q ibm-generative-ai==0.2.6\n",
    "!{sys.executable} -m pip install -q ibm-watson-machine-learning==1.0.311\n",
    "# !{sys.executable} -m pip install -q pandas\n",
    "# !{sys.executable} -m pip install -q pyspark\n",
    "# !{sys.executable} -m pip install -q scikit-learn==1.2.2\n",
    "# !{sys.executable} -m pip install -q safetensors==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and then importing them for later use in our notebook\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE ARE THE VALUE YOU'LL NEED TO FILL IN YOURSELF\n",
    "# API key you created\n",
    "api_key = \"INSERT YOUR KEY HERE\"\n",
    "\n",
    "# Project ID of your watsonx instance\n",
    "project_id = \"INSERT YOUR PROJECT ID HERE\"\n",
    "\n",
    "# URL service endpoint\n",
    "ibm_cloud_url = \"INSERT SERVICE ENDPOINT HERE\"\n",
    "\n",
    "creds = {\n",
    "  \"url\": ibm_cloud_url,\n",
    "  \"apikey\": api_key \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def send_to_watsonxai(prompts,\n",
    "                    model_name=\"google/flan-ul2\",\n",
    "                    decoding_method=\"greedy\",\n",
    "                    max_new_tokens=100,\n",
    "                    min_new_tokens=30,\n",
    "                    temperature=1.0,\n",
    "                    repetition_penalty=2.0\n",
    "                    ):\n",
    "    '''\n",
    "   helper function for sending prompts and params to Watsonx.ai\n",
    "    \n",
    "    Args:  \n",
    "        prompts:list list of text prompts\n",
    "        decoding:str Watsonx.ai parameter \"sample\" or \"greedy\"\n",
    "        max_new_tok:int Watsonx.ai parameter for max new tokens/response returned\n",
    "        temperature:float Watsonx.ai parameter for temperature (range 0>2)\n",
    "        repetition_penalty:float Watsonx.ai parameter for repetition penalty (range 1.0 to 2.0)\n",
    "\n",
    "    Returns: None\n",
    "        prints response\n",
    "    '''\n",
    "\n",
    "    assert not any(map(lambda prompt: len(prompt) < 1, prompts)), \"make sure none of the prompts in the inputs prompts are empty\"\n",
    "\n",
    "    # Instantiate parameters for text generation\n",
    "    model_params = {\n",
    "        GenParams.DECODING_METHOD: decoding_method,\n",
    "        GenParams.MIN_NEW_TOKENS: min_new_tokens,\n",
    "        GenParams.MAX_NEW_TOKENS: max_new_tokens,\n",
    "        GenParams.RANDOM_SEED: 42,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "        GenParams.REPETITION_PENALTY: repetition_penalty,\n",
    "    }\n",
    "\n",
    "\n",
    "    # Instantiate a model proxy object to send your requests\n",
    "    model = Model(\n",
    "        model_id=model_name,\n",
    "        params=model_params,\n",
    "        credentials=creds,\n",
    "        project_id=project_id)\n",
    "\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(model.generate_text(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a short blog post for an advanced cloud service for large language models: This service is\"\n",
    "response = send_to_watsonxai(prompts=[prompt])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
