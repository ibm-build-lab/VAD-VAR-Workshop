{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Accesing Watsonx.ai via REST API\n",
    "\n",
    "In this lab, we will take a comprehensive look at making HTTP requests to access Watsonx.ai's REST API and learn how to use the functionality. This lab explore only a few of the many REST endpoints available so explore the REST API documentation to view the full list of capabilities.\n",
    "\n",
    "Before you start you should have the necessary items to access watsonx.ai programmatically, them being:\n",
    "\n",
    "- your IBM Cloud API key\n",
    "- the IBM Cloud service URL tied to your instance\n",
    "- the Project ID associated with your watsonx instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we'll start by installing some dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q requests\n",
    "!{sys.executable} -m pip install -q ibm-cloud-sdk-core\n",
    "!{sys.executable} -m pip install -q ibm-watson-machine-learning==1.0.311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and verifying they've install correctly\n",
    "import json\n",
    "import requests\n",
    "from ibm_cloud_sdk_core import IAMTokenManager\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP request headers\n",
    "\n",
    "Headers contain parameter values that represent the metadata associated with an API requests and response. In the following example, the Authorization header provides the server with credentials to validate your access. Watsonx.ai uses a \"Bearer\" access token which is used to pass our Watsonx.ai authentication key. The `Content-type` header in the request is added to tell the server or the browser which is serving the resource to the end user about the media type of the request. In this case, type of expected data as `application/json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE ARE THE VALUE YOU'LL NEED TO FILL IN YOURSELF\n",
    "# API key you created\n",
    "api_key = \"INSERT YOUR KEY HERE\"\n",
    "\n",
    "# Project ID of your watsonx instance\n",
    "project_id = \"INSERT YOUR PROJECT ID HERE\"\n",
    "\n",
    "# URL service endpoint\n",
    "ibm_cloud_url = \"INSERT YOUR SERVICE URL HERE\"\n",
    "\n",
    "access_token = ''\n",
    "try:\n",
    "  access_token = IAMTokenManager(\n",
    "    apikey = api_key,\n",
    "    url = \"https://iam.cloud.ibm.com/identity/token\"\n",
    "  ).get_token()\n",
    "except:\n",
    "  print('Issue obtaining access token. Check variables?')\n",
    "\n",
    "# The headers we'll send for both our POST and GET requests\n",
    "headers = {\n",
    "  \"Authorization\": \"Bearer \" + access_token,\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Accept\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST vs GET\n",
    "\n",
    "HTTP requests come in two flavors: GET and POST. When using GET, data parameters are included in the URL and visible to everyone. However, when using POST, data is not displayed in the URL but is instead passed in the HTTP message body.\n",
    "\n",
    "GET requests are intended to retrieve data from a server and do not modify the server’s state. On the other hand, POST requests are used to send data to the server for processing and may modify the server’s state.\n",
    "\n",
    "## POST requests with 'Generate' endpoint\n",
    "The generate endpoint \"https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text\" provides an interface for sending prompts to any model supported by Watsonx.ai. Given a text prompt as inputs, and required parameters, the selected model will attempt to complete the provided input and return \"generated_text\".\n",
    "\n",
    "Request body needs to include:\n",
    "- Model id (string): the id of the model\n",
    "- Input (string): prompt to generate completion\n",
    "- Parameters for the model (key-value pairs)\n",
    "- your watsonx project ID\n",
    "\n",
    "> It's important to note the generate endpoint URL is dependent on the location of where your watsonx instance is provisioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The body we'll send as part of our POST request\n",
    "body = {\n",
    "  \"model_id\": \"google/flan-ul2\",\n",
    "  \"input\": \"Write a short blog post for an advanced cloud service for large language models: This service is\",\n",
    "  \"parameters\": {\n",
    "    \"temperature\": 0,\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"min_new_tokens\": 25\n",
    "  },\n",
    "  \"project_id\": project_id  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"2023-07-07\"\n",
    "generation_endpoint = f\"{ibm_cloud_url}/ml/v1-beta/generation/text?version={version}\"\n",
    "\n",
    "response = requests.post(url=generation_endpoint, headers=headers, json=body)\n",
    "print(\"Raw JSON response:\\n\", json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GET requests to retrieve data\n",
    "\n",
    "The GET method is used to retrieve information from Watsonx.ai using a given URL.\n",
    "\n",
    "### GET /models\n",
    "\n",
    "This enpoint will get the list of all models currently supported by Watsonx.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_endpoint = f\"{ibm_cloud_url}/ml/v1-beta/foundation_model_specs?version={version}\"\n",
    "response = requests.get(url = model_endpoint, headers=headers)\n",
    "models = response.json()['resources']\n",
    "\n",
    "print(f\"{len(models)} models supported the Watsonx.ai\")\n",
    "\n",
    "# Uncomment below to see the raw JSON\n",
    "# print(json.dumps(models, indent=2))\n",
    "\n",
    "# prettify the output just to see the model names\n",
    "pretty_output = lambda m: m['label']\n",
    "print(json.dumps(list(map(pretty_output, models)), indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Python library\n",
    "\n",
    "Now that we've seen how to interact with watsonx.ai through the use of a REST API we'll now take a look at how we can interact with it via a Python library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the credentials to authenticate with ibm cloud\n",
    "# similar to the headers created before\n",
    "creds = {\n",
    "  \"url\": ibm_cloud_url,\n",
    "  \"apikey\": api_key \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function that allows multiple prompts to be sent in at once and parameters for tweaking\n",
    "def send_to_watsonxai(prompts,\n",
    "                    model_name=\"google/flan-ul2\",\n",
    "                    decoding_method=\"greedy\",\n",
    "                    max_new_tokens=100,\n",
    "                    min_new_tokens=30,\n",
    "                    temperature=1.0,\n",
    "                    repetition_penalty=2.0\n",
    "                    ):\n",
    "    '''\n",
    "   helper function for sending prompts and params to Watsonx.ai\n",
    "    \n",
    "    Args:  \n",
    "        prompts:list list of text prompts\n",
    "        decoding:str Watsonx.ai parameter \"sample\" or \"greedy\"\n",
    "        max_new_tok:int Watsonx.ai parameter for max new tokens/response returned\n",
    "        temperature:float Watsonx.ai parameter for temperature (range 0>2)\n",
    "        repetition_penalty:float Watsonx.ai parameter for repetition penalty (range 1.0 to 2.0)\n",
    "\n",
    "    Returns: None\n",
    "        prints response\n",
    "    '''\n",
    "\n",
    "    assert not any(map(lambda prompt: len(prompt) < 1, prompts)), \"make sure none of the prompts in the inputs prompts are empty\"\n",
    "\n",
    "    # Instantiate parameters for text generation\n",
    "    model_params = {\n",
    "        GenParams.DECODING_METHOD: decoding_method,\n",
    "        GenParams.MIN_NEW_TOKENS: min_new_tokens,\n",
    "        GenParams.MAX_NEW_TOKENS: max_new_tokens,\n",
    "        GenParams.RANDOM_SEED: 42,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "        GenParams.REPETITION_PENALTY: repetition_penalty,\n",
    "    }\n",
    "\n",
    "\n",
    "    # Instantiate a model proxy object to send your requests\n",
    "    model = Model(\n",
    "        model_id=model_name,\n",
    "        params=model_params,\n",
    "        credentials=creds,\n",
    "        project_id=project_id)\n",
    "\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(model.generate_text(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a short blog post for an advanced cloud service for large language models: This service is\"\n",
    "\n",
    "# feel free to test changing the model from one of the values listed before\n",
    "# and try changing other values as well\n",
    "response = send_to_watsonxai(\n",
    "  prompts=[prompt],\n",
    "  model_name=\"google/flan-ul2\",\n",
    "  decoding_method=\"greedy\",\n",
    "  max_new_tokens=100,\n",
    "  min_new_tokens=30,\n",
    "  temperature=1.0,\n",
    "  repetition_penalty=2.0\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Although we've used Python as our language of choice in this lab. It's important to note that through the use of an REST API essentially any language can use watsonx.ai programmatically. Meaning that there's no limit to how clients choose to integrate watsonx.ai into their currently existing tech stack. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
