{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Accesing Watsonx.ai via REST API\n",
    "\n",
    "In this lab, we will take a comprehensive look at making HTTP requests to access [Watsonx.ai's REST API](https://workbench.res.ibm.com/docs/api-reference) and learn how to use the functionality. This lab explore only a few of the many REST endpoints available so explore the REST API documentation to view the full list of capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we'll start by installing some dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q requests\n",
    "!{sys.executable} -m pip install -q ibm-cloud-sdk-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and verifying they've install correctly\n",
    "import json\n",
    "import requests\n",
    "from ibm_cloud_sdk_core import IAMTokenManager"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP request headers\n",
    "\n",
    "Headers contain parameter values that represent the metadata associated with an API requests and response. In the following example, the Authorization header provides the server with credentials to validate your access. Watsonx.ai uses a \"Bearer\" access token wich is used to pass our Watsonx.ai authentication key. The `Content-type` header in the request is added to tell the server or the browser which is serving the resource to the end user about the media type of the request. In this case, type of expected data as `application/json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE ARE THE VALUE YOU'LL NEED TO FILL IN YOURSELF\n",
    "# API key you created\n",
    "api_key = \"INSERT YOUR KEY HERE\"\n",
    "\n",
    "# Project ID of your watsonx instance\n",
    "project_id = \"INSERT YOUR PROJECT ID HERE\"\n",
    "\n",
    "# URL service endpoint\n",
    "ibm_cloud_url = \"INSERT SERVICE ENDPOINT HERE\"\n",
    "\n",
    "access_token = ''\n",
    "try:\n",
    "  access_token = IAMTokenManager(\n",
    "    apikey = api_key,\n",
    "    url = \"https://iam.cloud.ibm.com/identity/token\"\n",
    "  ).get_token()\n",
    "except:\n",
    "  print('Issue obtaining access token. Check variables?')\n",
    "\n",
    "# The headers we'll send for both our POST and GET requests\n",
    "headers = {\n",
    "  \"Authorization\": \"Bearer \" + access_token,\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Accept\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST vs GET\n",
    "\n",
    "HTTP requests come in two flavors: GET and POST. When using GET, data parameters are included in the URL and visible to everyone. However, when using POST, data is not displayed in the URL but is instead passed in the HTTP message body.\n",
    "\n",
    "GET requests are intended to retrieve data from a server and do not modify the server’s state. On the other hand, POST requests are used to send data to the server for processing and may modify the server’s state.\n",
    "\n",
    "## POST requests with 'Generate' endpoint\n",
    "The generate endpoint \"https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text\" provides an interface for sending prompts to any model supported by Watsonx.ai. Given a text prompt as inputs, and required parameters, the selected model will attempt to complete the provide input and return \"generated_text\".\n",
    "\n",
    "Request body needs to include:\n",
    "- Model id (string): the id of the model\n",
    "- Input (string): prompt to generate completion\n",
    "- Parameters for the model (key-value pairs)\n",
    "- your watsonx project ID\n",
    "\n",
    "> It's important to note the generate endpoint URL is dependent on the location of where your watsonx instance is provisioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The body we'll send as part of our POST request\n",
    "body = {\n",
    "  \"model_id\": \"google/flan-ul2\",\n",
    "  \"input\": \"Write a short blog post for an advanced cloud service for large language models: This service is\",\n",
    "  \"parameters\": {\n",
    "    \"temperature\": 0,\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"min_new_tokens\": 25\n",
    "  },\n",
    "  \"project_id\": project_id  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw JSON response:\n",
      " {\n",
      "  \"model_id\": \"google/flan-ul2\",\n",
      "  \"created_at\": \"2023-09-12T18:13:54.031Z\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"generated_text\": \"a new cloud service for large language models. It is a scalable, high-performance, and secure service for training and deploying language models.\",\n",
      "      \"generated_token_count\": 33,\n",
      "      \"input_token_count\": 20,\n",
      "      \"stop_reason\": \"EOS_TOKEN\"\n",
      "    }\n",
      "  ],\n",
      "  \"system\": {\n",
      "    \"warnings\": [\n",
      "      {\n",
      "        \"message\": \"This model is a Non-IBM Product governed by a third-party license that may impose use restrictions and other obligations. By using this model you agree to its terms as identified in the following URL. URL: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx\",\n",
      "        \"id\": \"DisclaimerWarning\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "version = \"2023-07-07\"\n",
    "generation_endpoint = f\"{ibm_cloud_url}/ml/v1-beta/generation/text?version={version}\"\n",
    "\n",
    "response = requests.post(url=generation_endpoint, headers=headers, json=body)\n",
    "print(\"Raw JSON response:\\n\", json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GET requests to retrieve data\n",
    "\n",
    "The GET method is used to retrieve information from Watsonx.ai using a given URL.\n",
    "\n",
    "### GET /models\n",
    "\n",
    "This enpoint will get the list of all models currently supported by Watsonx.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 models supported the Watsonx.ai\n",
      "[\n",
      "  \"starcoder-15.5b\",\n",
      "  \"mt0-xxl-13b\",\n",
      "  \"gpt-neox-20b\",\n",
      "  \"flan-t5-xxl-11b\",\n",
      "  \"flan-ul2-20b\",\n",
      "  \"mpt-7b-instruct2\",\n",
      "  \"llama-2-70b-chat\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model_endpoint = f\"{ibm_cloud_url}/ml/v1-beta/foundation_model_specs?version={version}\"\n",
    "response = requests.get(url = model_endpoint, headers=headers)\n",
    "models = response.json()['resources']\n",
    "\n",
    "print(f\"{len(models)} models supported the Watsonx.ai\")\n",
    "\n",
    "# Uncomment below to see the raw JSON\n",
    "# print(json.dumps(models, indent=2))\n",
    "\n",
    "# prettify the output just to see the model names\n",
    "pretty_output = lambda m: m['label']\n",
    "print(json.dumps(list(map(pretty_output, models)), indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Although we've used Python as our language of choice in this lab. It's important to note that through the use of an REST API essentially any language can use watsonx.ai programmatically. Meaning that there's no limit to how clients choose to integrate watsonx.ai into their currently existing tech stack. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
