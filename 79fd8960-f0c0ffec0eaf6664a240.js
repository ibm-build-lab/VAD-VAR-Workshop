"use strict";(self.webpackChunkvad_var_site=self.webpackChunkvad_var_site||[]).push([[2090],{2622:function(e,s,t){t.d(s,{$Sz:function(){return ns},DcG:function(){return ws},ENH:function(){return gs},En$:function(){return hs},Hqk:function(){return ls},K2m:function(){return ps},Kf0:function(){return is},OjJ:function(){return os},U$$:function(){return ms},Zn:function(){return fs},hZO:function(){return rs},lbf:function(){return cs},o$X:function(){return as},t78:function(){return ds},tLj:function(){return us},wiU:function(){return _s}});var n=t(3675),a=t(9346),o=t(5261),i=t(7082),r=t(4e3),c=t(1328),l=t(3308);const{InferenceSession:d,Tensor:_}=c.ONNX,u=0,h=1,m=2,f=3,g=4,p=new Map,w=new Map,y=new Map;async function x(e,s,t){let n=`onnx/${s}${t.quantized?"_quantized":""}.onnx`,a=await(0,o.st)(e,n,!0,t);try{return await d.create(a,{executionProviders:c.p})}catch(i){if(1===c.p.length&&"wasm"===c.p[0])throw i;return console.warn(i),console.warn("Something went wrong during model construction (most likely a missing operation). Using `wasm` as a fallback. "),await d.create(a,{executionProviders:["wasm"]})}}async function k(e,s){const t=await async function(e,s){const t={},n=[];for(let i of e.inputNames)void 0===s[i]?n.push(i):t[i]=s[i];if(n.length>0)throw new Error(`An error occurred during model execution: "Missing the following inputs: ${n.join(", ")}.`);const a=Object.keys(s).length,o=e.inputNames.length;if(a>o){let t=Object.keys(s).filter((s=>!e.inputNames.includes(s)));console.warn(`WARNING: Too many inputs were provided (${a} > ${o}). The following inputs will be ignored: "${t.join(", ")}".`)}return t}(e,s);try{let s=await e.run(t);return s=M(s),s}catch(n){throw console.error(`An error occurred during model execution: "${n}".`),console.error("Inputs given to model:",t),n}}function M(e){for(let s in e)e[s]instanceof _?e[s]=new r.es(e[s]):"object"==typeof e[s]&&M(e[s]);return e}function b(e){if(e instanceof r.es)return e;if(0===e.length)throw Error("items must be non-empty");if(Array.isArray(e[0])){if(e.some((s=>s.length!==e[0].length)))throw Error("Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' and/or 'truncation=True' to have batched tensors with the same length.");return new r.es("int64",BigInt64Array.from(e.flat().map((e=>BigInt(e)))),[e.length,e[0].length])}return new r.es("int64",BigInt64Array.from(e.map((e=>BigInt(e)))),[1,e.length])}function v(e,s){let t=e.config.pad_token_id??null,n=e.config.eos_token_id??null;(0,a.Wy)(n)&&(n=[n]);let o=-1!==s.indexOf(t),i=null===n||!n.includes(t);if(o&&i){let e=BigInt64Array.from(s.data.map((e=>e!=t)));return new r.es("int64",e,s.dims)}return(0,r.r6)(s)}function S(e){return new r.es("bool",[e],[1])}async function C(e,s){let{encoder_outputs:t,past_key_values:n}=s;t||(t=(await P(e,s)).last_hidden_state);let a={input_ids:s.decoder_input_ids,encoder_hidden_states:t,use_cache_branch:S(!!n)};e.decoder_merged_session.inputNames.includes("encoder_attention_mask")&&(a.encoder_attention_mask=s.attention_mask),e.addPastKeyValues(a,n);const o=await k(e.decoder_merged_session,a);let i=o.logits;n=e.getPastKeyValues(o,n);const r=e.getAttentions(o);return new ys({logits:i,past_key_values:n,encoder_outputs:t,...r})}function A(e,s,t,n){let a=[],o=0;const i=e.requires_attention_mask??!0;let c=t.decoder_input_ids??t.decoder_start_token_id??t.bos_token_id??t.eos_token_id;c instanceof r.es?c=c.tolist().flat():Array.isArray(c)||(c=[c]);for(let r of s){r.dims=[1,...r.dims];let s={inputs:r,encoder_outputs:null,prev_model_outputs:null,output_token_ids:c,done:!1,score:0,id:o++};i&&(s.attention_mask=v(e,r)),a.push(s)}return a}async function F(e,s){const t=e.main_input_name;let n=s.output_token_ids;s.prev_model_outputs&&(n=n.slice(-1));let a={[t]:s.inputs,decoder_input_ids:b(n),encoder_outputs:s.encoder_outputs,past_key_values:s.prev_model_outputs?.past_key_values};s.attention_mask&&(a.attention_mask=s.attention_mask);let o=await e.forward(a);return s.prev_model_outputs=o,s.encoder_outputs=o.encoder_outputs,o}function L(e,s){e.output_token_ids=[...e.output_token_ids,s]}async function P(e,s){let t={};for(let n of e.session.inputNames)t[n]=s[n];return await k(e.session,t)}async function B(e,s){let{input_ids:t,past_key_values:n,attention_mask:a}=s,o={input_ids:t,attention_mask:a??v(e,t),use_cache_branch:S(!!n)};e.addPastKeyValues(o,n);let i=await k(e.session,o),r=i.logits;return n=e.getPastKeyValues(i,n),{logits:r,past_key_values:n}}function T(e,s,t,n,a){let o=[],i=0;for(let r of s){let s,t=r.tolist().map(Number);r.dims=[1,...r.dims],a?(s=a[i],s.dims=[1,...s.dims]):s=v(e,r);let c={input:r,model_input_ids:r,attention_mask:s,prev_model_outputs:null,output_token_ids:t,num_output_tokens:n,done:!1,score:0,id:i++};o.push(c)}return o}async function D(e,s){let t=new BigInt64Array(s.output_token_ids.length).fill(1n),n={input_ids:s.model_input_ids,attention_mask:new r.es("int64",t,[1,t.length]),past_key_values:s.prev_model_outputs?.past_key_values},a=await e.forward(n);return s.prev_model_outputs=a,a}function G(e,s){e.output_token_ids=[...e.output_token_ids,s],e.model_input_ids=new r.es("int64",[BigInt(s)],[1,1])}class I extends a.Ag{main_input_name="input_ids";constructor(e,s){super(),this.config=e,this.session=s;const t=y.get(this.constructor),n=p.get(t);this.can_generate=!1,this._runBeam=null,this._getStartBeams=null,this._updateBeam=null,this._forward=null,n===g?(this.can_generate=!0,this._runBeam=D,this._getStartBeams=T,this._updateBeam=G,this._forward=B):n===m||n===f?(this.can_generate=!0,this._runBeam=F,this._getStartBeams=A,this._updateBeam=L,this._forward=C):this._forward=P}async dispose(){let e=[];for(let s of Object.keys(this)){let t=this[s];t instanceof d&&e.push(t.handler.dispose())}return await Promise.all(e)}static async from_pretrained(e,{quantized:s=!0,progress_callback:t=null,config:a=null,cache_dir:i=null,local_files_only:r=!1,revision:c="main",model_file_name:l=null}={}){let d={quantized:s,progress_callback:t,config:a,cache_dir:i,local_files_only:r,revision:c,model_file_name:l};const _=y.get(this),w=p.get(_);let k;return w===g?k=await Promise.all([n.z.from_pretrained(e,d),x(e,d.model_file_name??"decoder_model_merged",d),(0,o.yM)(e,"generation_config.json",!1,d)]):w===m||w===f?k=await Promise.all([n.z.from_pretrained(e,d),x(e,"encoder_model",d),x(e,"decoder_model_merged",d),(0,o.yM)(e,"generation_config.json",!1,d)]):w===h?k=await Promise.all([n.z.from_pretrained(e,d),x(e,"encoder_model",d),x(e,"decoder_model_merged",d)]):(w!==u&&console.warn(`Model type for '${_}' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose.`),k=await Promise.all([n.z.from_pretrained(e,d),x(e,d.model_file_name??"model",d)])),new this(...k)}async _call(e){return await this.forward(e)}async forward(e){return await this._forward(this,e)}_get_logits_processor(e,s,t=null){const n=new i.Jm;if(null!==e.repetition_penalty&&1!==e.repetition_penalty&&n.push(new i.Jj(e.repetition_penalty)),null!==e.no_repeat_ngram_size&&e.no_repeat_ngram_size>0&&n.push(new i.jF(e.no_repeat_ngram_size)),null!==e.min_length&&null!==e.eos_token_id&&e.min_length>0&&n.push(new i.ez(e.min_length,e.eos_token_id)),null!==e.min_new_tokens&&null!==e.eos_token_id&&e.min_new_tokens>0&&n.push(new i.CJ(s,e.min_new_tokens,e.eos_token_id)),null!==e.forced_bos_token_id&&n.push(new i.C9(e.forced_bos_token_id)),null!==e.forced_eos_token_id&&n.push(new i.dZ(e.max_length,e.forced_eos_token_id)),null!==e.begin_suppress_tokens){let t=s>1||null===e.forced_bos_token_id?s:s+1;null!==e.forced_decoder_ids&&(t+=e.forced_decoder_ids[e.forced_decoder_ids.length-1][0]),n.push(new i.GU(e.begin_suppress_tokens,t))}return null!==e.forced_decoder_ids&&n.push(new i.E(e.forced_decoder_ids)),null!==t&&n.extend(t),n}_get_generation_config(e){let s=new i.aP(this.config);return"generation_config"in this&&Object.assign(s,this.generation_config),null!==e&&Object.assign(s,e),s}async generate(e,s=null,t=null,{inputs_attention_mask:n=null}={}){if(!this.can_generate){let e=`The current model class (${y.get(this.constructor)}) is not compatible with \`.generate()\`, as it doesn't have a language model head.`;const s=this.config.model_type,t=We.get(s)??Ve.get(s)??Oe.get(s)??Qe.get(s);throw t&&(e+=` Please use the following class instead: '${t[0]}'`),Error(e)}if(!(e instanceof r.es||(0,a.fU)(e)||Array.isArray(e)))throw Error(`\`inputs\` must be a Tensor, TypedArray, or Array, but is "${e.constructor.name}".`);let o;if(this.config.is_encoder_decoder)o=0;else if(o=e instanceof r.es?e.dims.at(-1):e.length,0===o)throw Error("Must supply a non-empty array of input token ids.");s=this._get_generation_config(s),t=t??new i.Jm,t=this._get_logits_processor(s,o,t);let c=s.eos_token_id;null===c||Array.isArray(c)||(c=[c]);let l=1;const d=l+(s.max_new_tokens??1/0),_=Number.isInteger(s.max_length)&&null===(s.max_new_tokens??null);let u=i.Z4.getSampler(s),h=this.getStartBeams(e,s,l,n);for(;h.some((e=>!e.done))&&l<d;){let e=[];for(let n of h){if(n.done){e.push(n);continue}if(_&&n.output_token_ids.length>=s.max_length){n.done=!0,e.push(n);continue}let a=await this.runBeam(n);s.output_attentions&&this.addAttentionsToBeam(n,a),s.output_scores;let o=a.logits.slice(null,-1,null);t(n.output_token_ids,o);let i=u(o);for(let[s,t]of i){let a={...n};this.updateBeam(a,s),a.score+=t,c&&c.includes(s)&&(a.done=!0),e.push(a)}}++l,e=this.groupBeams(e).map((e=>e.sort(((e,s)=>s.score-e.score)).slice(0,s.num_beams))),h=e.flat(),s.callback_function&&s.callback_function(h)}const m=this.groupBeams(h),f=e=>m.map((t=>s.num_return_sequences>1?t.slice(0,s.num_return_sequences).map((s=>s[e])):[t[0][e]])).flat(),g=f("output_token_ids");if(s.return_dict_in_generate){return{sequences:g,decoder_attentions:f("decoder_attentions"),cross_attentions:f("cross_attentions")}}return g}addAttentionsToBeam(e,s){if(this.config.is_encoder_decoder){if(!s.cross_attentions||0===s.cross_attentions.length)throw Error("`output_attentions` is true, but the model did not produce cross-attentions. This is most likely because the model was not exported with `output_attentions=True`.");e.cross_attentions||(e.cross_attentions=[]),e.cross_attentions.push(s.cross_attentions)}if(!s.decoder_attentions||0===s.decoder_attentions.length)throw Error("`output_attentions` is true, but the model did not produce decoder-attentions. This is most likely because the model was not exported with `output_attentions=True`.");e.decoder_attentions||(e.decoder_attentions=[]),e.decoder_attentions.push(s.decoder_attentions)}groupBeams(e){const s=Object.create(null);for(const t of e)void 0===s[t.id]?s[t.id]=[t]:s[t.id].push(t);return Object.values(s)}getPastKeyValues(e,s){const t=Object.create(null);for(const n in e)if(n.startsWith("present")){let a=n.replace("present","past_key_values");s&&n.includes("encoder")?t[a]=s[a]:t[a]=e[n]}return t}getAttentions(e){const s=Object.create(null);for(const t of["cross_attentions","decoder_attentions"]){const n=[];for(const s in e)if(s.startsWith(t)){n[s.split(".").pop()]=e[s]}s[t]=n}return s}addPastKeyValues(e,s){if(s)Object.assign(e,s);else if(this.config.is_encoder_decoder&&(this.add_encoder_pkv??1)){let s=[1,this.num_encoder_heads,0,this.encoder_dim_kv],t=[1,this.num_decoder_heads,0,this.decoder_dim_kv];for(let n=0;n<this.num_decoder_layers;++n)e[`past_key_values.${n}.encoder.key`]=new r.es("float32",[],s),e[`past_key_values.${n}.encoder.value`]=new r.es("float32",[],s),e[`past_key_values.${n}.decoder.key`]=new r.es("float32",[],t),e[`past_key_values.${n}.decoder.value`]=new r.es("float32",[],t)}else if(this.config.multi_query){let s=[1,0,2*this.dim_kv];for(let t=0;t<this.num_layers;++t)e[`past_key_values.${t}.key_value`]=new r.es("float32",[],s)}else if("bloom"===this.config.model_type){let s=[1*this.num_heads,this.dim_kv,0],t=[1*this.num_heads,0,this.dim_kv];for(let n=0;n<this.num_layers;++n)e[`past_key_values.${n}.key`]=new r.es("float32",[],s),e[`past_key_values.${n}.value`]=new r.es("float32",[],t)}else{let s=[1,this.num_heads,0,this.dim_kv];for(let t=0;t<this.num_layers;++t)e[`past_key_values.${t}.key`]=new r.es("float32",[],s),e[`past_key_values.${t}.value`]=new r.es("float32",[],s)}}getStartBeams(e,s,t,n){return this._getStartBeams(this,e,s,t,n)}async runBeam(e){return await this._runBeam(this,e)}updateBeam(e,s){return this._updateBeam(e,s)}}class q{}class N extends I{}class E extends I{}class O extends I{}class z extends I{}class $ extends I{}class j extends I{}class V extends I{}class W extends I{}class X extends I{}class R extends I{}class Q extends I{}class K extends I{}class U extends I{}class J extends I{}class H extends I{}class Z extends I{}class Y extends I{}class ee extends I{}class se extends I{}class te extends I{}class ne extends I{main_input_name="pixel_values";constructor(e,s,t,n){super(e,s),this.decoder_merged_session=t,this.generation_config=n;const a=this.config.encoder,o=this.config.decoder,i=a.model_type;(qe.get(i)??Ne.get(i))||console.warn(`Model type for encoder '${i}' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose.`);const r=We.get(o.model_type);if(!r)throw new Error(`Unable to construct \`VisionEncoderDecoder\` due to unsupported decoder: "${this.config.decoder.model_type}"`);const c=new(0,r[1])(o,t,n);this.add_encoder_pkv="num_decoder_layers"in c,this.add_encoder_pkv?(this.num_decoder_layers=c.num_decoder_layers,this.num_decoder_heads=c.num_decoder_heads,this.decoder_dim_kv=c.decoder_dim_kv,this.num_encoder_layers=c.num_encoder_layers,this.num_encoder_heads=c.num_encoder_heads,this.encoder_dim_kv=c.encoder_dim_kv):(this.num_layers=c.num_layers,this.num_heads=c.num_heads,this.dim_kv=c.dim_kv)}}class ae extends I{}class oe extends I{constructor(e,s,t){super(e,s),this.generation_config=t,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.n_embd/this.num_heads}}class ie extends I{constructor(e,s,t){super(e,s),this.generation_config=t,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_heads,this.num_layers=this.config.num_layers,this.dim_kv=this.config.hidden_size/this.num_heads}}class re extends I{constructor(e,s,t){super(e,s),this.generation_config=t,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_attention_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.num_heads}}class ce extends I{constructor(e,s,t){super(e,s),this.generation_config=t,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.n_embd/this.num_heads}}class le extends I{constructor(e,s,t){super(e,s),this.generation_config=t,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.n_embd/this.num_heads}}class de extends I{constructor(e,s,t){super(e,s),this.generation_config=t,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.n_embd/this.num_heads}}class _e extends I{constructor(e,s,t){super(e,s),this.generation_config=t,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_attention_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.num_heads}}class ue extends I{constructor(e,s,t){super(e,s),this.generation_config=t,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.hidden_size/this.num_heads}}class he extends I{constructor(e,s,t){super(e,s),this.generation_config=t,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_heads,this.num_layers=this.config.n_layers,this.dim_kv=this.config.d_model/this.num_heads}}class me extends I{constructor(e,s,t){super(e,s),this.generation_config=t,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_attention_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.num_heads}}class fe extends I{}class ge extends I{}class pe extends I{}class we extends I{}class ye extends q{constructor({logits:e,pred_boxes:s}){super(),this.logits=e,this.pred_boxes=s}}class xe extends q{constructor({logits:e,pred_boxes:s,pred_masks:t}){super(),this.logits=e,this.pred_boxes=s,this.pred_masks=t}}class ke extends I{}class Me extends I{}class be extends I{}class ve extends I{}class Se extends I{}class Ce extends q{constructor({logits:e,pred_boxes:s}){super(),this.logits=e,this.pred_boxes=s}}class Ae extends I{}class Fe extends Ae{async _call(e){return new Le(await super._call(e))}}class Le extends q{constructor({iou_scores:e,pred_masks:s}){super(),this.iou_scores=e,this.pred_masks=s}}class Pe extends I{}class Be extends I{}class Te extends I{}class De extends I{}class Ge extends I{}class Ie{static MODEL_CLASS_MAPPINGS=null;static BASE_IF_FAIL=!1;static async from_pretrained(e,{quantized:s=!0,progress_callback:t=null,config:a=null,cache_dir:o=null,local_files_only:i=!1,revision:r="main",model_file_name:c=null}={}){let l={quantized:s,progress_callback:t,config:a,cache_dir:o,local_files_only:i,revision:r,model_file_name:c};if(a=await n.z.from_pretrained(e,l),l.config||(l.config=a),!this.MODEL_CLASS_MAPPINGS)throw new Error("`MODEL_CLASS_MAPPINGS` not implemented for this type of `AutoClass`: "+this.name);for(let n of this.MODEL_CLASS_MAPPINGS){const s=n.get(a.model_type);if(s)return await s[1].from_pretrained(e,l)}if(this.BASE_IF_FAIL)return console.warn(`Unknown model class "${a.model_type}", attempting to construct from base class.`),await I.from_pretrained(e,l);throw Error(`Unsupported model type: ${a.model_type}`)}}const qe=new Map([["bert",["BertModel",class extends N{}]],["camembert",["CamembertModel",class extends E{}]],["deberta",["DebertaModel",class extends O{}]],["deberta-v2",["DebertaV2Model",class extends z{}]],["mpnet",["MPNetModel",class extends V{}]],["albert",["AlbertModel",class extends X{}]],["distilbert",["DistilBertModel",class extends ${}]],["roberta",["RobertaModel",class extends Y{}]],["xlm",["XLMModel",class extends ee{}]],["xlm-roberta",["XLMRobertaModel",class extends se{}]],["clip",["CLIPModel",class extends ae{}]],["mobilebert",["MobileBertModel",class extends j{}]],["squeezebert",["SqueezeBertModel",class extends W{}]],["wav2vec2",["Wav2Vec2Model",class extends Te{}]],["wavlm",["WavLMModel",class extends De{}]],["detr",["DetrModel",class extends we{}]],["vit",["ViTModel",class extends fe{}]],["mobilevit",["MobileViTModel",class extends ge{}]],["beit",["BeitModel",class extends pe{}]],["deit",["DeiTModel",class extends ke{}]],["resnet",["ResNetModel",class extends Me{}]],["swin",["SwinModel",class extends be{}]],["donut-swin",["DonutSwinModel",class extends ve{}]],["yolos",["YolosModel",class extends Se{}]],["hifigan",["SpeechT5HifiGan",class extends I{main_input_name="spectrogram"}]],["sam",["SamModel",Fe]]]),Ne=new Map([["t5",["T5Model",class extends R{}]],["longt5",["LongT5Model",class extends Q{}]],["mt5",["MT5Model",class extends K{}]],["bart",["BartModel",class extends U{}]],["mbart",["MBartModel",class extends J{}]],["marian",["MarianModel",class extends Pe{}]],["whisper",["WhisperModel",class extends te{}]],["m2m_100",["M2M100Model",class extends Be{}]],["blenderbot",["BlenderbotModel",class extends H{}]],["blenderbot-small",["BlenderbotSmallModel",class extends Z{}]]]),Ee=new Map([["bloom",["BloomModel",class extends ue{}]],["gpt2",["GPT2Model",class extends oe{}]],["gptj",["GPTJModel",class extends ce{}]],["gpt_bigcode",["GPTBigCodeModel",class extends le{}]],["gpt_neo",["GPTNeoModel",class extends ie{}]],["gpt_neox",["GPTNeoXModel",class extends re{}]],["codegen",["CodeGenModel",class extends de{}]],["llama",["LlamaModel",class extends _e{}]],["mpt",["MptModel",class extends he{}]],["opt",["OPTModel",class extends me{}]]]),Oe=new Map([["speecht5",["SpeechT5ForSpeechToText",class extends Ge{}]],["whisper",["WhisperForConditionalGeneration",class extends te{requires_attention_mask=!1;main_input_name="input_features";constructor(e,s,t,n){super(e,s),this.decoder_merged_session=t,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}async generate(e,s=null,t=null){if(s=this._get_generation_config(s),s.return_timestamps??=!1,s.return_timestamps&&(t=[new i.Pg(s)]),s.return_token_timestamps&&(s.output_attentions=!0,s.return_dict_in_generate=!0,"translate"===s.task&&console.warn("Token-level timestamps may not be reliable for task 'translate'."),!s.alignment_heads))throw new Error("Model generation config has no `alignment_heads`, token-level timestamps not available. See https://gist.github.com/hollance/42e32852f24243b748ae6bc1f985b13a on how to add this property to the generation config.");const n=await super.generate(e,s,t);return s.return_token_timestamps&&s.alignment_heads&&(n.token_timestamps=this._extract_token_timestamps(n,s.alignment_heads,s.num_frames)),n}_extract_token_timestamps(e,s,t=null,n=.02){if(!e.cross_attentions)throw new Error("Model outputs must contain cross attentions to extract timestamps. This is most likely because the model was not exported with `output_attentions=True`.");let o=this.config.median_filter_width;void 0===o&&(console.warn("Model config has no `median_filter_width`, using default value of 7."),o=7);const i=e.cross_attentions.map((e=>{let n=Array.from({length:this.config.decoder_layers},((s,t)=>(0,r.d3)(e.map((e=>e[t])),2))),a=(0,r.kn)(s.map((([e,s])=>t?n[e].slice(null,s,null,[0,t]):n[e].slice(null,s))));a=a.transpose(1,0,2,3);let[i,c]=(0,r.f3)(a,-2,0,!0),d=a.clone();for(let s=0;s<d.dims[0];++s){let e=d[s];for(let t=0;t<e.dims[0];++t){let n=e[t];const a=i[s][t][0],r=c[s][t][0];for(let e=0;e<n.dims[0];++e){let s=n[e];for(let e=0;e<s.data.length;++e)s.data[e]=(s.data[e]-r.data[e])/a.data[e];s.data.set((0,l.qCb)(s.data,o))}}}return(0,r.J6)(d,1)})),c=[e.sequences.length,e.sequences[0].length],d=new r.es("float32",new Float32Array(c[0]*c[1]),c);for(let l=0;l<c[0];++l){const e=i[l].neg().squeeze_(0);let[s,t]=(0,r.Ks)(e),o=Array.from({length:s.length-1},((e,t)=>s[t+1]-s[t])),c=(0,a.eG)([1],o).map((e=>!!e)),_=[];for(let a=0;a<c.length;++a)c[a]&&_.push(t[a]*n);d[l].data.set(_,1)}return d}}]]]),ze=new Map([["speecht5",["SpeechT5ForTextToSpeech",class extends Ge{constructor(e,s,t,n){super(e,s),this.decoder_merged_session=t,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.hidden_size/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.hidden_size/this.num_encoder_heads}async generate_speech(e,s,{threshold:t=.5,minlenratio:n=0,maxlenratio:a=20,vocoder:o=null}={}){const i={input_ids:e},{encoder_outputs:c,encoder_attention_mask:l}=await P(this,i),d=c.dims[1]/this.config.reduction_factor,_=Math.floor(d*a),u=Math.floor(d*n),h=this.config.num_mel_bins;let m=[],f=null,g=null,p=0;for(;;){++p;const e=S(!!g);let n;n=g?g.output_sequence_out:new r.es("float32",new Float32Array(h),[1,1,h]);let a={use_cache_branch:e,output_sequence:n,encoder_attention_mask:l,speaker_embeddings:s,encoder_hidden_states:c};this.addPastKeyValues(a,f),g=await k(this.decoder_merged_session,a),f=this.getPastKeyValues(g,f);const{prob:o,spectrum:i}=g;if(m.push(i),p>=u&&(Array.from(o.data).filter((e=>e>=t)).length>0||p>=_))break}const w=(0,r.d3)(m),{waveform:y}=await k(o.session,{spectrogram:w});return{spectrogram:w,waveform:y}}}]]]),$e=new Map([["bert",["BertForSequenceClassification",class extends N{async _call(e){return new xs(await super._call(e))}}]],["camembert",["CamembertForSequenceClassification",class extends E{async _call(e){return new xs(await super._call(e))}}]],["deberta",["DebertaForSequenceClassification",class extends O{async _call(e){return new xs(await super._call(e))}}]],["deberta-v2",["DebertaV2ForSequenceClassification",class extends z{async _call(e){return new xs(await super._call(e))}}]],["mpnet",["MPNetForSequenceClassification",class extends V{async _call(e){return new xs(await super._call(e))}}]],["albert",["AlbertForSequenceClassification",class extends X{async _call(e){return new xs(await super._call(e))}}]],["distilbert",["DistilBertForSequenceClassification",class extends ${async _call(e){return new xs(await super._call(e))}}]],["roberta",["RobertaForSequenceClassification",class extends Y{async _call(e){return new xs(await super._call(e))}}]],["xlm",["XLMForSequenceClassification",class extends ee{async _call(e){return new xs(await super._call(e))}}]],["xlm-roberta",["XLMRobertaForSequenceClassification",class extends se{async _call(e){return new xs(await super._call(e))}}]],["bart",["BartForSequenceClassification",class extends U{async _call(e){return new xs(await super._call(e))}}]],["mbart",["MBartForSequenceClassification",class extends J{async _call(e){return new xs(await super._call(e))}}]],["mobilebert",["MobileBertForSequenceClassification",class extends j{async _call(e){return new xs(await super._call(e))}}]],["squeezebert",["SqueezeBertForSequenceClassification",class extends W{async _call(e){return new xs(await super._call(e))}}]]]),je=new Map([["bert",["BertForTokenClassification",class extends N{async _call(e){return new ks(await super._call(e))}}]],["camembert",["CamembertForTokenClassification",class extends E{async _call(e){return new ks(await super._call(e))}}]],["deberta",["DebertaForTokenClassification",class extends O{async _call(e){return new ks(await super._call(e))}}]],["deberta-v2",["DebertaV2ForTokenClassification",class extends z{async _call(e){return new ks(await super._call(e))}}]],["mpnet",["MPNetForTokenClassification",class extends V{async _call(e){return new ks(await super._call(e))}}]],["distilbert",["DistilBertForTokenClassification",class extends ${async _call(e){return new ks(await super._call(e))}}]],["roberta",["RobertaForTokenClassification",class extends Y{async _call(e){return new ks(await super._call(e))}}]],["xlm",["XLMForTokenClassification",class extends ee{async _call(e){return new ks(await super._call(e))}}]],["xlm-roberta",["XLMRobertaForTokenClassification",class extends se{async _call(e){return new ks(await super._call(e))}}]]]),Ve=new Map([["t5",["T5ForConditionalGeneration",class extends R{constructor(e,s,t,n){super(e,s),this.decoder_merged_session=t,this.generation_config=n,this.num_decoder_layers=this.config.num_decoder_layers,this.num_decoder_heads=this.config.num_heads,this.decoder_dim_kv=this.config.d_kv,this.num_encoder_layers=this.config.num_layers,this.num_encoder_heads=this.config.num_heads,this.encoder_dim_kv=this.config.d_kv}}]],["longt5",["LongT5ForConditionalGeneration",class extends Q{constructor(e,s,t,n){super(e,s),this.decoder_merged_session=t,this.generation_config=n,this.num_decoder_layers=this.config.num_decoder_layers,this.num_decoder_heads=this.config.num_heads,this.decoder_dim_kv=this.config.d_kv,this.num_encoder_layers=this.config.num_layers,this.num_encoder_heads=this.config.num_heads,this.encoder_dim_kv=this.config.d_kv}}]],["mt5",["MT5ForConditionalGeneration",class extends K{constructor(e,s,t,n){super(e,s),this.decoder_merged_session=t,this.generation_config=n,this.num_decoder_layers=this.config.num_decoder_layers,this.num_decoder_heads=this.config.num_heads,this.decoder_dim_kv=this.config.d_kv,this.num_encoder_layers=this.config.num_layers,this.num_encoder_heads=this.config.num_heads,this.encoder_dim_kv=this.config.d_kv}}]],["bart",["BartForConditionalGeneration",class extends U{constructor(e,s,t,n){super(e,s),this.decoder_merged_session=t,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["mbart",["MBartForConditionalGeneration",class extends J{constructor(e,s,t,n){super(e,s),this.decoder_merged_session=t,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["marian",["MarianMTModel",class extends Pe{constructor(e,s,t,n){super(e,s),this.decoder_merged_session=t,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["m2m_100",["M2M100ForConditionalGeneration",class extends Be{constructor(e,s,t,n){super(e,s),this.decoder_merged_session=t,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["blenderbot",["BlenderbotForConditionalGeneration",class extends H{constructor(e,s,t,n){super(e,s),this.decoder_merged_session=t,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["blenderbot-small",["BlenderbotSmallForConditionalGeneration",class extends Z{constructor(e,s,t,n){super(e,s),this.decoder_merged_session=t,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]]]),We=new Map([["bloom",["BloomForCausalLM",class extends ue{}]],["gpt2",["GPT2LMHeadModel",class extends oe{}]],["gptj",["GPTJForCausalLM",class extends ce{}]],["gpt_bigcode",["GPTBigCodeForCausalLM",class extends le{}]],["gpt_neo",["GPTNeoForCausalLM",class extends ie{}]],["gpt_neox",["GPTNeoXForCausalLM",class extends re{}]],["codegen",["CodeGenForCausalLM",class extends de{}]],["llama",["LlamaForCausalLM",class extends _e{}]],["mpt",["MptForCausalLM",class extends he{}]],["opt",["OPTForCausalLM",class extends me{}]],["mbart",["MBartForCausalLM",class extends J{constructor(e,s,t){super(e,s),this.generation_config=t,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]]]),Xe=new Map([["bert",["BertForMaskedLM",class extends N{async _call(e){return new Ms(await super._call(e))}}]],["camembert",["CamembertForMaskedLM",class extends E{async _call(e){return new Ms(await super._call(e))}}]],["deberta",["DebertaForMaskedLM",class extends O{async _call(e){return new Ms(await super._call(e))}}]],["deberta-v2",["DebertaV2ForMaskedLM",class extends z{async _call(e){return new Ms(await super._call(e))}}]],["mpnet",["MPNetForMaskedLM",class extends V{async _call(e){return new Ms(await super._call(e))}}]],["albert",["AlbertForMaskedLM",class extends X{async _call(e){return new Ms(await super._call(e))}}]],["distilbert",["DistilBertForMaskedLM",class extends ${async _call(e){return new Ms(await super._call(e))}}]],["roberta",["RobertaForMaskedLM",class extends Y{async _call(e){return new Ms(await super._call(e))}}]],["xlm",["XLMWithLMHeadModel",class extends ee{async _call(e){return new Ms(await super._call(e))}}]],["xlm-roberta",["XLMRobertaForMaskedLM",class extends se{async _call(e){return new Ms(await super._call(e))}}]],["mobilebert",["MobileBertForMaskedLM",class extends j{async _call(e){return new Ms(await super._call(e))}}]],["squeezebert",["SqueezeBertForMaskedLM",class extends W{async _call(e){return new Ms(await super._call(e))}}]]]),Re=new Map([["bert",["BertForQuestionAnswering",class extends N{async _call(e){return new bs(await super._call(e))}}]],["camembert",["CamembertForQuestionAnswering",class extends E{async _call(e){return new bs(await super._call(e))}}]],["deberta",["DebertaForQuestionAnswering",class extends O{async _call(e){return new bs(await super._call(e))}}]],["deberta-v2",["DebertaV2ForQuestionAnswering",class extends z{async _call(e){return new bs(await super._call(e))}}]],["mpnet",["MPNetForQuestionAnswering",class extends V{async _call(e){return new bs(await super._call(e))}}]],["albert",["AlbertForQuestionAnswering",class extends X{async _call(e){return new bs(await super._call(e))}}]],["distilbert",["DistilBertForQuestionAnswering",class extends ${async _call(e){return new bs(await super._call(e))}}]],["roberta",["RobertaForQuestionAnswering",class extends Y{async _call(e){return new bs(await super._call(e))}}]],["xlm",["XLMForQuestionAnswering",class extends ee{async _call(e){return new bs(await super._call(e))}}]],["xlm-roberta",["XLMRobertaForQuestionAnswering",class extends se{async _call(e){return new bs(await super._call(e))}}]],["mobilebert",["MobileBertForQuestionAnswering",class extends j{async _call(e){return new bs(await super._call(e))}}]],["squeezebert",["SqueezeBertForQuestionAnswering",class extends W{async _call(e){return new bs(await super._call(e))}}]]]),Qe=new Map([["vision-encoder-decoder",["VisionEncoderDecoderModel",ne]]]),Ke=new Map([["vision-encoder-decoder",["VisionEncoderDecoderModel",ne]]]),Ue=new Map([["vit",["ViTForImageClassification",class extends fe{async _call(e){return new xs(await super._call(e))}}]],["mobilevit",["MobileViTForImageClassification",class extends ge{async _call(e){return new xs(await super._call(e))}}]],["beit",["BeitForImageClassification",class extends pe{async _call(e){return new xs(await super._call(e))}}]],["deit",["DeiTForImageClassification",class extends ke{async _call(e){return new xs(await super._call(e))}}]],["resnet",["ResNetForImageClassification",class extends Me{async _call(e){return new xs(await super._call(e))}}]],["swin",["SwinForImageClassification",class extends be{async _call(e){return new xs(await super._call(e))}}]]]),Je=new Map([["detr",["DetrForObjectDetection",class extends we{async _call(e){return new ye(await super._call(e))}}]],["yolos",["YolosForObjectDetection",class extends Se{async _call(e){return new Ce(await super._call(e))}}]]]),He=new Map([["detr",["DetrForSegmentation",class extends we{async _call(e){return new xe(await super._call(e))}}]]]),Ze=new Map([["sam",["SamModel",Fe]]]),Ye=new Map([["wav2vec2",["Wav2Vec2ForCTC",class extends Te{async _call(e){return new vs(await super._call(e))}}]],["wavlm",["WavLMForCTC",class extends De{async _call(e){return new vs(await super._call(e))}}]]]),es=new Map([["wav2vec2",["Wav2Vec2ForSequenceClassification",class extends Te{async _call(e){return new xs(await super._call(e))}}]],["wavlm",["WavLMForSequenceClassification",class extends De{async _call(e){return new xs(await super._call(e))}}]]]),ss=[[qe,u],[Ne,h],[Ee,g],[$e,u],[je,u],[Ve,m],[Oe,m],[We,g],[Xe,u],[Re,u],[Qe,f],[Ue,u],[He,u],[Je,u],[Ze,u],[Ye,u],[es,u],[ze,m]];for(const[Ss,Cs]of ss)for(const[e,s]of Ss.values())p.set(e,Cs),y.set(s,e),w.set(e,s);const ts=[["CLIPTextModelWithProjection",class extends ae{static async from_pretrained(e,s={}){return s.model_file_name??="text_model",super.from_pretrained(e,s)}},u],["CLIPVisionModelWithProjection",class extends ae{static async from_pretrained(e,s={}){return s.model_file_name??="vision_model",super.from_pretrained(e,s)}},u]];for(const[Ss,Cs,As]of ts)p.set(Ss,As),y.set(Cs,Ss),w.set(Ss,Cs);class ns extends Ie{static MODEL_CLASS_MAPPINGS=[qe,Ne,Ee];static BASE_IF_FAIL=!0}class as extends Ie{static MODEL_CLASS_MAPPINGS=[$e]}class os extends Ie{static MODEL_CLASS_MAPPINGS=[je]}class is extends Ie{static MODEL_CLASS_MAPPINGS=[Ve]}class rs extends Ie{static MODEL_CLASS_MAPPINGS=[Oe]}class cs extends Ie{static MODEL_CLASS_MAPPINGS=[ze]}class ls extends Ie{static MODEL_CLASS_MAPPINGS=[We]}class ds extends Ie{static MODEL_CLASS_MAPPINGS=[Xe]}class _s extends Ie{static MODEL_CLASS_MAPPINGS=[Re]}class us extends Ie{static MODEL_CLASS_MAPPINGS=[Qe]}class hs extends Ie{static MODEL_CLASS_MAPPINGS=[Ue]}class ms extends Ie{static MODEL_CLASS_MAPPINGS=[He]}class fs extends Ie{static MODEL_CLASS_MAPPINGS=[Je]}class gs extends Ie{static MODEL_CLASS_MAPPINGS=[Ye]}class ps extends Ie{static MODEL_CLASS_MAPPINGS=[es]}class ws extends Ie{static MODEL_CLASS_MAPPINGS=[Ke]}class ys extends q{constructor({logits:e,past_key_values:s,encoder_outputs:t,decoder_attentions:n=null,cross_attentions:a=null}){super(),this.logits=e,this.past_key_values=s,this.encoder_outputs=t,this.decoder_attentions=n,this.cross_attentions=a}}class xs extends q{constructor({logits:e}){super(),this.logits=e}}class ks extends q{constructor({logits:e}){super(),this.logits=e}}class Ms extends q{constructor({logits:e}){super(),this.logits=e}}class bs extends q{constructor({start_logits:e,end_logits:s}){super(),this.start_logits=e,this.end_logits=s}}class vs extends q{constructor({logits:e}){super(),this.logits=e}}}}]);
//# sourceMappingURL=79fd8960-f0c0ffec0eaf6664a240.js.map