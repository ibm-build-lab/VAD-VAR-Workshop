{"version":3,"file":"79fd8960-f0c0ffec0eaf6664a240.js","mappings":"mmBAoFA,MAAM,iBAAEA,EAAkBC,OAAQC,GAAe,EAAAC,KAI3CC,EACW,EADXA,EAEc,EAFdA,EAGO,EAHPA,EAIU,EAJVA,EAKW,EASXC,EAAqB,IAAIC,IACzBC,EAA8B,IAAID,IAClCE,EAA8B,IAAIF,IAWxCG,eAAeC,EAAiBC,EAA+BC,EAAUC,GAErE,IAAIC,EAAgB,QAAQF,IAAWC,EAAQE,UAAY,aAAe,UACtEC,QAAe,QAAaL,EAA+BG,GAAe,EAAMD,GAEpF,IACI,aAAab,EAAiBiB,OAAOD,EAAQ,CACzCE,mBAAkB,KAE1B,CAAE,MAAOC,GAEL,GAAkC,IAA9B,YAA6D,SAA1B,OACnC,MAAMA,EAQV,OALAC,QAAQC,KAAKF,GACbC,QAAQC,KACJ,wHAGSrB,EAAiBiB,OAAOD,EAAQ,CACzCE,mBAAoB,CAAC,SAE7B,CACJ,CAiDAT,eAAea,EAAWC,EAASC,GAC/B,MAAMC,QAxCVhB,eAA8Bc,EAASC,GAEnC,MAAMC,EAAgB,CAAC,EACjBC,EAAgB,GACtB,IAAK,IAAIC,KAAaJ,EAAQK,gBACAC,IAAtBL,EAAOG,GACPD,EAAcI,KAAKH,GAEnBF,EAAcE,GAAaH,EAAOG,GAG1C,GAAID,EAAcK,OAAS,EACvB,MAAM,IAAIC,MACN,4EAA4EN,EAAcO,KAAK,UAGvG,MAAMC,EAAoBC,OAAOC,KAAKZ,GAAQO,OACxCM,EAAkBd,EAAQK,WAAWG,OAC3C,GAAIG,EAAoBG,EAAiB,CAGrC,IAAIC,EAAUH,OAAOC,KAAKZ,GAAQe,QAAOZ,IAAcJ,EAAQK,WAAWY,SAASb,KACnFP,QAAQC,KAAK,2CAA2Ca,OAAuBG,8CAA4DC,EAAQL,KAAK,UAC5J,CAEA,OAAOR,CACX,CAcgCgB,CAAelB,EAASC,GACpD,IACI,IAAIkB,QAAenB,EAAQoB,IAAIlB,GAE/B,OADAiB,EAASE,EAAeF,GACjBA,CACX,CAAE,MAAOG,GAIL,MAFAzB,QAAQ0B,MAAM,8CAA8CD,OAC5DzB,QAAQ0B,MAAM,yBAA0BrB,GAClCoB,CACV,CACJ,CAQA,SAASD,EAAeG,GACpB,IAAK,IAAIC,KAAQD,EACTA,EAAIC,aAAiB9C,EACrB6C,EAAIC,GAAQ,IAAI,KAAOD,EAAIC,IACC,iBAAdD,EAAIC,IAClBJ,EAAeG,EAAIC,IAG3B,OAAOD,CACX,CAUA,SAASE,EAAYC,GACjB,GAAIA,aAAiB,KACjB,OAAOA,EAGX,GAAqB,IAAjBA,EAAMnB,OACN,MAAMC,MAAM,2BAGhB,GAAImB,MAAMC,QAAQF,EAAM,IAAK,CAEzB,GAAIA,EAAMG,MAAKC,GAAKA,EAAEvB,SAAWmB,EAAM,GAAGnB,SACtC,MAAMC,MAAM,8KAGhB,OAAO,IAAI,KAAO,QACduB,cAAcC,KAAKN,EAAMO,OAAOC,KAAIJ,GAAKK,OAAOL,MAChD,CAACJ,EAAMnB,OAAQmB,EAAM,GAAGnB,QAEhC,CAEI,OAAO,IAAI,KAAO,QACdwB,cAAcC,KAAKN,EAAMQ,KAAIJ,GAAKK,OAAOL,MACzC,CAAC,EAAGJ,EAAMnB,QAGtB,CASA,SAAS6B,EAAqBC,EAAMC,GAGhC,IAAIC,EAAeF,EAAKG,OAAOD,cAAgB,KAC3CE,EAAeJ,EAAKG,OAAOC,cAAgB,MAC3C,QAAiBA,KACjBA,EAAe,CAACA,IAGpB,IAAIC,GAA2D,IAAlCJ,EAAOK,QAAQJ,GACxCK,EAA2D,OAAjBH,IAA2BA,EAAazB,SAASuB,GAE/F,GAAIG,GAA0BE,EAAwC,CAClE,IAAIC,EAAOd,cAAcC,KAErBM,EAAOO,KAAKX,KAAIJ,GAAKA,GAAKS,KAE9B,OAAO,IAAI,KAAO,QAASM,EAAMP,EAAOQ,KAC5C,CACI,OAAO,QAAUR,EAEzB,CAQA,SAASS,EAAWC,GAChB,OAAO,IAAI,KAAO,OAAQ,CAACA,GAAQ,CAAC,GACxC,CAUA/D,eAAegE,EAAeZ,EAAMa,GAEhC,IAAI,gBAAEC,EAAe,gBAAEC,GAAoBF,EAEtCC,IAEDA,SAAyBE,EAAehB,EAAMa,IAAeI,mBAEjE,IAAIC,EAAe,CACfC,UAAWN,EAAaO,kBACxBC,sBAAuBP,EACvBQ,iBAAkBZ,IAAaK,IAG/Bf,EAAKuB,uBAAuBxD,WAAWY,SAAS,4BAChDuC,EAAaM,uBAAyBX,EAAaY,gBAEvDzB,EAAK0B,iBAAiBR,EAAcH,GAEpC,MAAMY,QAAuBlE,EAAWuC,EAAKuB,uBAAwBL,GACrE,IAAIU,EAASD,EAAeC,OAC5Bb,EAAkBf,EAAK6B,iBAAiBF,EAAgBZ,GAGxD,MAAMe,EAAQ9B,EAAK+B,cAAcJ,GAEjC,OAAO,IAAIK,GAAgB,CAAEJ,SAAQb,kBAAiBD,qBAAoBgB,GAC9E,CAWA,SAASG,EAAkBjC,EAAMkC,EAAeC,EAAmBC,GAC/D,IAAIC,EAAQ,GACRC,EAAS,EAGb,MAAMC,EAA0BvC,EAAKuC,0BAA2B,EAGhE,IAAInB,EACAe,EAAkBf,mBACfe,EAAkBK,wBAClBL,EAAkBM,cAClBN,EAAkB/B,aAIrBgB,aAA6B,KAC7BA,EAAoBA,EAAkBsB,SAAS9C,OACvCN,MAAMC,QAAQ6B,KACtBA,EAAoB,CAACA,IAGzB,IAAK,IAAInB,KAAUiC,EAAe,CAI9BjC,EAAOQ,KAAO,CAAC,KAAMR,EAAOQ,MAG5B,IAAIkC,EAAQ,CACRhF,OAAQsC,EACRa,gBAAiB,KACjB8B,mBAAoB,KAEpBC,iBAAkBzB,EAClB0B,MAAM,EACNC,MAAO,EACPC,GAAIV,KAGJC,IACAI,EAAMlB,eAAiB1B,EAAqBC,EAAMC,IAGtDoC,EAAMpE,KAAK0E,EACf,CAEA,OAAON,CACX,CAWAzF,eAAeqG,EAAejD,EAAMkD,GAChC,MAAMC,EAAanD,EAAKoD,gBAExB,IAAIhC,EAAoB8B,EAAKL,iBACzBK,EAAKN,qBAGLxB,EAAoBA,EAAkBiC,OAAO,IAIjD,IAAIxC,EAAe,CACf,CAACsC,GAAaD,EAAKvF,OACnByD,kBAAmBhC,EAAYgC,GAC/BN,gBAAiBoC,EAAKpC,gBACtBC,gBAAiBmC,EAAKN,oBAAoB7B,iBAE1CmC,EAAKzB,iBACLZ,EAAaY,eAAiByB,EAAKzB,gBAIvC,IAAI5C,QAAemB,EAAKsD,QAAQzC,GAMhC,OAHAqC,EAAKN,mBAAqB/D,EAC1BqE,EAAKpC,gBAAkBjC,EAAOiC,gBAEvBjC,CACX,CAQA,SAAS0E,EAAkBL,EAAMM,GAC7BN,EAAKL,iBAAmB,IAAIK,EAAKL,iBAAkBW,EACvD,CASA5G,eAAeoE,EAAehB,EAAMa,GAChC,IAAI4C,EAAe,CAAC,EACpB,IAAK,IAAIC,KAAO1D,EAAKtC,QAAQK,WACzB0F,EAAaC,GAAO7C,EAAa6C,GAErC,aAAajG,EAAWuC,EAAKtC,QAAS+F,EAC1C,CAUA7G,eAAe+G,EAAe3D,EAAMa,GAChC,IAAI,UAAEM,EAAS,gBAAEJ,EAAe,eAAEU,GAAmBZ,EACjDK,EAAe,CACfC,UAAWA,EACXM,eAAgBA,GAAkB1B,EAAqBC,EAAMmB,GAC7DG,iBAAkBZ,IAAaK,IAGnCf,EAAK0B,iBAAiBR,EAAcH,GAEpC,IAAIY,QAAuBlE,EAAWuC,EAAKtC,QAASwD,GAEhDU,EAASD,EAAeC,OAG5B,OADAb,EAAkBf,EAAK6B,iBAAiBF,EAAgBZ,GACjD,CAAEa,SAAQb,kBACrB,CAYA,SAAS6C,EAAkB5D,EAAMkC,EAAeC,EAAmBC,EAAiByB,GAChF,IAAIxB,EAAQ,GAERC,EAAS,EACb,IAAK,IAAIrC,KAAUiC,EAAe,CAC9B,IAOI4B,EAPAjB,EAAmB5C,EAAOyC,SAAS7C,IAAIkE,QAK3C9D,EAAOQ,KAAO,CAAC,KAAMR,EAAOQ,MAGxBoD,GACAC,EAAYD,EAAsBvB,GAClCwB,EAAUrD,KAAO,CAAC,KAAMqD,EAAUrD,OAGlCqD,EAAY/D,EAAqBC,EAAMC,GAG3C,IAAI0C,EAAQ,CACRqB,MAAO/D,EACPgE,gBAAiBhE,EACjBwB,eAAgBqC,EAChBlB,mBAAoB,KAEpBC,iBAAkBA,EAClBqB,kBAAmB9B,EAEnBU,MAAM,EACNC,MAAO,EACPC,GAAIV,KAGRD,EAAMpE,KAAK0E,EACf,CACA,OAAON,CACX,CAeAzF,eAAeuH,EAAenE,EAAMkD,GAChC,IAAIkB,EAAe,IAAI1E,cAAcwD,EAAKL,iBAAiB3E,QAAQmG,KAAK,IAGpExD,EAAe,CACfM,UAAW+B,EAAKe,gBAChBxC,eAAgB,IAAI,KAChB,QACA2C,EACA,CAAC,EAAGA,EAAalG,SAErB6C,gBAAiBmC,EAAKN,oBAAoB7B,iBAI1ClC,QAAemB,EAAKsD,QAAQzC,GAKhC,OAFAqC,EAAKN,mBAAqB/D,EAEnBA,CACX,CAQA,SAASyF,EAAkBpB,EAAMM,GAC7BN,EAAKL,iBAAmB,IAAIK,EAAKL,iBAAkBW,GACnDN,EAAKe,gBAAkB,IAAI,KAAO,QAAS,CAACnE,OAAO0D,IAAc,CAAC,EAAG,GACzE,CAQO,MAAMe,UAAwB,KACjCnB,gBAAkB,YAOlBoB,YAAYrE,EAAQzC,GAChB+G,QAEAC,KAAKvE,OAASA,EACduE,KAAKhH,QAAUA,EAEf,MAAMiH,EAAYhI,EAA4BiI,IAAIF,KAAKF,aACjDK,EAAYrI,EAAmBoI,IAAID,GAEzCD,KAAKI,cAAe,EACpBJ,KAAKK,SAAW,KAChBL,KAAKM,eAAiB,KACtBN,KAAKO,YAAc,KACnBP,KAAKQ,SAAW,KACZL,IAActI,GACdmI,KAAKI,cAAe,EAEpBJ,KAAKK,SAAWZ,EAChBO,KAAKM,eAAiBpB,EACtBc,KAAKO,YAAcX,EACnBI,KAAKQ,SAAWvB,GAETkB,IAActI,GAAuBsI,IAActI,GAC1DmI,KAAKI,cAAe,EAEpBJ,KAAKK,SAAW9B,EAChByB,KAAKM,eAAiB/C,EACtByC,KAAKO,YAAc1B,EACnBmB,KAAKQ,SAAWtE,GAGhB8D,KAAKQ,SAAWlE,CAKxB,CAOApE,gBACI,IAAIuI,EAAW,GACf,IAAK,IAAIzB,KAAOpF,OAAOC,KAAKmG,MAAO,CAC/B,IAAIU,EAAOV,KAAKhB,GACZ0B,aAAgBjJ,GAChBgJ,EAASlH,KAAKmH,EAAKC,QAAQC,UAEnC,CACA,aAAaC,QAAQC,IAAIL,EAC7B,CAiBAM,6BAA6B3I,GAA+B,UACxDI,GAAY,EAAI,kBAChBwI,EAAoB,KAAI,OACxBvF,EAAS,KAAI,UACbwF,EAAY,KAAI,iBAChBC,GAAmB,EAAK,SACxBC,EAAW,OAAM,gBACjBC,EAAkB,MAClB,CAAC,GAED,IAAI9I,EAAU,CACVE,YACAwI,oBACAvF,SACAwF,YACAC,mBACAC,WACAC,mBAGJ,MAAMnB,EAAYhI,EAA4BiI,IAAIF,MAC5CG,EAAYrI,EAAmBoI,IAAID,GAEzC,IAAIoB,EAkCJ,OAjCIlB,IAActI,EACdwJ,QAAaR,QAAQC,IAAI,CACrB,oBAA2B1I,EAA+BE,GAC1DH,EAAiBC,EAA+BE,EAAQ8I,iBAAmB,uBAAwB9I,IACnG,QAAaF,EAA+B,0BAA0B,EAAOE,KAG1E6H,IAActI,GAAuBsI,IAActI,EAC1DwJ,QAAaR,QAAQC,IAAI,CACrB,oBAA2B1I,EAA+BE,GAC1DH,EAAiBC,EAA+B,gBAAiBE,GACjEH,EAAiBC,EAA+B,uBAAwBE,IACxE,QAAaF,EAA+B,0BAA0B,EAAOE,KAG1E6H,IAActI,EACrBwJ,QAAaR,QAAQC,IAAI,CACrB,oBAA2B1I,EAA+BE,GAC1DH,EAAiBC,EAA+B,gBAAiBE,GACjEH,EAAiBC,EAA+B,uBAAwBE,MAIxE6H,IAActI,GACdgB,QAAQC,KAAK,mBAAmBmH,wIAEpCoB,QAAaR,QAAQC,IAAI,CACrB,oBAA2B1I,EAA+BE,GAC1DH,EAAiBC,EAA+BE,EAAQ8I,iBAAmB,QAAS9I,MAKrF,IAAI0H,QAAQqB,EACvB,CAOAnJ,YAAYiE,GACR,aAAa6D,KAAKpB,QAAQzC,EAC9B,CASAjE,cAAciE,GACV,aAAa6D,KAAKQ,SAASR,KAAM7D,EACrC,CAQAmF,sBACI7D,EACA8D,EAGAC,EAAmB,MAEnB,MAAMC,EAAa,IAAI,KAuFvB,GAtE6C,OAAzChE,EAAkBiE,oBAAwE,IAAzCjE,EAAkBiE,oBACnED,EAAWlI,KAAK,IAAI,KAAiCkE,EAAkBiE,qBAG5B,OAA3CjE,EAAkBkE,sBAAiClE,EAAkBkE,qBAAuB,GAC5FF,EAAWlI,KAAK,IAAI,KAA6BkE,EAAkBkE,uBAkBlC,OAAjClE,EAAkBmE,YAA0D,OAAnCnE,EAAkB/B,cAAyB+B,EAAkBmE,WAAa,GACnHH,EAAWlI,KAAK,IAAI,KAAyBkE,EAAkBmE,WAAYnE,EAAkB/B,eAGxD,OAArC+B,EAAkBoE,gBAA8D,OAAnCpE,EAAkB/B,cAAyB+B,EAAkBoE,eAAiB,GAC3HJ,EAAWlI,KAAK,IAAI,KAChBgI,EACA9D,EAAkBoE,eAClBpE,EAAkB/B,eAYoB,OAA1C+B,EAAkBqE,qBAClBL,EAAWlI,KAAK,IAAI,KAA8BkE,EAAkBqE,sBAG1B,OAA1CrE,EAAkBsE,qBAClBN,EAAWlI,KAAK,IAAI,KAChBkE,EAAkBuE,WAClBvE,EAAkBsE,sBAoBsB,OAA5CtE,EAAkBwE,sBAAgC,CAClD,IAAIC,EAAeX,EAAuB,GAA+C,OAA1C9D,EAAkBqE,oBAC3DP,EACAA,EAAuB,EAEgB,OAAzC9D,EAAkB0E,qBAElBD,GAAezE,EAAkB0E,mBAAmB1E,EAAkB0E,mBAAmB3I,OAAS,GAAG,IAEzGiI,EAAWlI,KAAK,IAAI,KAAqCkE,EAAkBwE,sBAAuBC,GACtG,CAeA,OAb6C,OAAzCzE,EAAkB0E,oBAClBV,EAAWlI,KAAK,IAAI,IAA2BkE,EAAkB0E,qBAG5C,OAArBX,GACAC,EAAWW,OAAOZ,GAQfC,CACX,CASAY,uBAAuB5E,GAGnB,IAAI6E,EAAa,IAAI,KAAiBtC,KAAKvE,QAY3C,MATI,sBAAuBuE,MACvBpG,OAAO2I,OAAOD,EAAYtC,KAAKvC,mBAKT,OAAtBA,GACA7D,OAAO2I,OAAOD,EAAY7E,GAEvB6E,CACX,CAmBApK,eACIe,EACAwE,EAAoB,KACpB+D,EAAmB,MACnB,sBACIrC,EAAwB,MACxB,CAAC,GAEL,IAAKa,KAAKI,aAAc,CAEpB,IAAIoC,EAAe,4BADDvK,EAA4BiI,IAAIF,KAAKF,kGAGvD,MAAMK,EAAYH,KAAKvE,OAAOgH,WACxBC,EACFC,GAAiCzC,IAAIC,IAClCyC,GAA6C1C,IAAIC,IACjD0C,GAAyC3C,IAAIC,IAE7C2C,GAAqC5C,IAAIC,GAMhD,MAJIuC,IAEAF,GAAgB,6CAA6CE,EAAa,OAExEjJ,MAAM+I,EAChB,CAEA,KAAMvJ,aAAkB,OAAY,QAAaA,IAAY2B,MAAMC,QAAQ5B,IACvE,MAAMQ,MAAM,8DAA8DR,EAAO6G,YAAYiD,UAGjG,IAAIxB,EAIJ,GAAIvB,KAAKvE,OAAOuH,mBAEZzB,EAAuB,OAMvB,GAHAA,EAAuBtI,aAAkB,KAASA,EAAO8C,KAAKkH,IAAI,GAAKhK,EAAOO,OAGjD,IAAzB+H,EACA,MAAM9H,MAAM,qDAKpBgE,EAAoBuC,KAAKqC,uBAAuB5E,GAEhD+D,EAAmBA,GAAoB,IAAI,KAG3CA,EAAmBxB,KAAKsB,sBACpB7D,EACA8D,EACAC,GAIJ,IAAI0B,EAAgBzF,EAAkB/B,aAChB,OAAlBwH,GAA2BtI,MAAMC,QAAQqI,KACzCA,EAAgB,CAACA,IAMrB,IAAIxF,EAAkB,EACtB,MAAMyF,EAAkBzF,GAAmBD,EAAkB2F,gBAAkBC,KAGzEC,EAAejE,OAAOkE,UAAU9F,EAAkBuE,aAA8D,QAA9CvE,EAAkB2F,gBAAkB,MAC5G,IAAII,EAAU,gBAAmB/F,GAG7BE,EAAQqC,KAAKyD,cAAcxK,EAAQwE,EAAmBC,EAAiByB,GAE3E,KAAOxB,EAAM7C,MAAKC,IAAMA,EAAEqD,QAASV,EAAkByF,GAAiB,CAClE,IAAIO,EAAe,GACnB,IAAK,IAAIlF,KAAQb,EAAO,CACpB,GAAIa,EAAKJ,KAAM,CAEXsF,EAAanK,KAAKiF,GAClB,QACJ,CACA,GAAI8E,GAAgB9E,EAAKL,iBAAiB3E,QAAUiE,EAAkBuE,WAAY,CAE9ExD,EAAKJ,MAAO,EACZsF,EAAanK,KAAKiF,GAClB,QACJ,CAGA,IAAIrE,QAAe6F,KAAK2D,QAAQnF,GAG5Bf,EAAkBmG,mBAClB5D,KAAK6D,oBAAoBrF,EAAMrE,GAE/BsD,EAAkBqG,cAQtB,IAAI5G,EAAS/C,EAAO+C,OAAOyB,MAAM,MAAO,EAAG,MAG3C6C,EAAiBhD,EAAKL,iBAAkBjB,GAExC,IAAI6G,EAAgBP,EAAQtG,GAC5B,IAAK,IAAK4B,EAAYkF,KAAYD,EAAe,CAE7C,IAAIE,EAAU,IAAKzF,GAInBwB,KAAKkE,WAAWD,EAASnF,GAEzBmF,EAAQ5F,OAAS2F,EAEbd,GAAiBA,EAAcjJ,SAAS6E,KACxCmF,EAAQ7F,MAAO,GAGnBsF,EAAanK,KAAK0K,EACtB,CACJ,GACEvG,EAGFgG,EAAe1D,KAAKmE,WAAWT,GAAcvI,KACzCiJ,GAASA,EACJC,MAAK,CAACC,EAAGC,IAAMA,EAAElG,MAAQiG,EAAEjG,QAC3BM,MAAM,EAAGlB,EAAkB+G,aAIpC7G,EAAQ+F,EAAaxI,OAGjBuC,EAAkBgH,mBAClBhH,EAAkBgH,kBAAkB9G,EAE5C,CAIA,MAAM+G,EAAe1E,KAAKmE,WAAWxG,GAE/BgH,EAAgB3F,GAAQ0F,EAAavJ,KACvCyJ,GACQnH,EAAkBoH,qBAAuB,EAClCD,EAAMjG,MAAM,EAAGlB,EAAkBoH,sBAAsB1J,KAAIJ,GAAKA,EAAEiE,KAElE,CAAC4F,EAAM,GAAG5F,MAG3B9D,OAEI4J,EAAYH,EAAa,oBAE/B,GAAIlH,EAAkBsH,wBAAyB,CAgB3C,MAAO,CACHD,YAEAE,mBANuBL,EAAa,sBAOpCM,iBANqBN,EAAa,oBAQ1C,CACI,OAAOG,CAEf,CAQAjB,oBAAoBrF,EAAMrE,GACtB,GAAI6F,KAAKvE,OAAOuH,mBAAoB,CAChC,IAAK7I,EAAO8K,kBAAuD,IAAnC9K,EAAO8K,iBAAiBzL,OACpD,MAAMC,MACF,sKAIH+E,EAAKyG,mBACNzG,EAAKyG,iBAAmB,IAE5BzG,EAAKyG,iBAAiB1L,KAAKY,EAAO8K,iBACtC,CAEA,IAAK9K,EAAO6K,oBAA2D,IAArC7K,EAAO6K,mBAAmBxL,OACxD,MAAMC,MACF,wKAIH+E,EAAKwG,qBACNxG,EAAKwG,mBAAqB,IAE9BxG,EAAKwG,mBAAmBzL,KAAKY,EAAO6K,mBACxC,CAQAb,WAAWxG,GAEP,MAAMuH,EAAStL,OAAOlB,OAAO,MAC7B,IAAK,MAAM8B,KAAOmD,OACSrE,IAAnB4L,EAAO1K,EAAI8D,IACX4G,EAAO1K,EAAI8D,IAAM,CAAC9D,GAElB0K,EAAO1K,EAAI8D,IAAI/E,KAAKiB,GAI5B,OAAOZ,OAAOuL,OAAOD,EACzB,CASA/H,iBAAiBF,EAAgBmI,GAE7B,MAAMC,EAAOzL,OAAOlB,OAAO,MAE3B,IAAK,MAAMqK,KAAQ9F,EACf,GAAI8F,EAAKuC,WAAW,WAAY,CAC5B,IAAIC,EAAUxC,EAAKyC,QAAQ,UAAW,mBAElCJ,GAAiBrC,EAAK9I,SAAS,WAI/BoL,EAAKE,GAAWH,EAAcG,GAE9BF,EAAKE,GAAWtI,EAAe8F,EAEvC,CAEJ,OAAOsC,CACX,CAQAhI,cAAcJ,GACV,MAAMG,EAAQxD,OAAOlB,OAAO,MAE5B,IAAK,MAAM+M,IAAY,CAAC,mBAAoB,sBAAuB,CAC/D,MAAMC,EAAS,GACf,IAAK,MAAM3C,KAAQ9F,EACf,GAAI8F,EAAKuC,WAAWG,GAAW,CAE3BC,EADc3C,EAAK4C,MAAM,KAAKC,OACd3I,EAAe8F,EACnC,CAEJ3F,EAAMqI,GAAYC,CACtB,CACA,OAAOtI,CACX,CAQAJ,iBAAiBR,EAAc4I,GAC3B,GAAIA,EACAxL,OAAO2I,OAAO/F,EAAc4I,QAI5B,GAAIpF,KAAKvE,OAAOuH,qBAAuBhD,KAAK6F,iBAAmB,GAAO,CAElE,IAAIC,EAAe,CAAC,EAAG9F,KAAK+F,kBAAmB,EAAG/F,KAAKgG,gBAEnDC,EAAe,CAAC,EAAGjG,KAAKkG,kBAAmB,EAAGlG,KAAKmG,gBAEvD,IAAK,IAAIC,EAAI,EAAGA,EAAIpG,KAAKqG,qBAAsBD,EAC3C5J,EAAa,mBAAmB4J,iBAAmB,IAAI,KAAO,UAAW,GAAIN,GAC7EtJ,EAAa,mBAAmB4J,mBAAqB,IAAI,KAAO,UAAW,GAAIN,GAC/EtJ,EAAa,mBAAmB4J,iBAAmB,IAAI,KAAO,UAAW,GAAIH,GAC7EzJ,EAAa,mBAAmB4J,mBAAqB,IAAI,KAAO,UAAW,GAAIH,EAEvF,MAAO,GAAIjG,KAAKvE,OAAO6K,YAAa,CAEhC,IAAIvK,EAAO,CAAC,EAAG,EAAG,EAAIiE,KAAKuG,QAE3B,IAAK,IAAIH,EAAI,EAAGA,EAAIpG,KAAKwG,aAAcJ,EACnC5J,EAAa,mBAAmB4J,eAAiB,IAAI,KAAO,UAAW,GAAIrK,EAEnF,MAAO,GAA+B,UAA3BiE,KAAKvE,OAAOgH,WAAwB,CAI3C,IAAIgE,EAAU,CAAC,EAAIzG,KAAK0G,UAAW1G,KAAKuG,OAAQ,GAE5CI,EAAY,CAAC,EAAI3G,KAAK0G,UAAW,EAAG1G,KAAKuG,QAE7C,IAAK,IAAIH,EAAI,EAAGA,EAAIpG,KAAKwG,aAAcJ,EACnC5J,EAAa,mBAAmB4J,SAAW,IAAI,KAAO,UAAW,GAAIK,GACrEjK,EAAa,mBAAmB4J,WAAa,IAAI,KAAO,UAAW,GAAIO,EAE/E,KAAO,CAEH,IAAI5K,EAAO,CAAC,EAAGiE,KAAK0G,UAAW,EAAG1G,KAAKuG,QAEvC,IAAK,IAAIH,EAAI,EAAGA,EAAIpG,KAAKwG,aAAcJ,EACnC5J,EAAa,mBAAmB4J,SAAW,IAAI,KAAO,UAAW,GAAIrK,GACrES,EAAa,mBAAmB4J,WAAa,IAAI,KAAO,UAAW,GAAIrK,EAE/E,CAER,CAWA0H,cAAcjG,EAAeC,EAAmBC,EAAiByB,GAC7D,OAAOa,KAAKM,eAAeN,KAAMxC,EAAeC,EAAmBC,EAAiByB,EACxF,CAQAjH,cAAcsG,GACV,aAAawB,KAAKK,SAASL,KAAMxB,EACrC,CAQA0F,WAAW1F,EAAMM,GACb,OAAOkB,KAAKO,YAAY/B,EAAMM,EAClC,EAKG,MAAM8H,GAqBN,MAAMC,UAA4BhH,GAkElC,MAAMiH,UAAiCjH,GAsEvC,MAAMkH,UAA+BlH,GAuErC,MAAMmH,UAAiCnH,GAuEvC,MAAMoH,UAAkCpH,GAoExC,MAAMqH,UAAkCrH,GAmDxC,MAAMsH,UAA6BtH,GAuEnC,MAAMuH,UAAmCvH,GAwCzC,MAAMwH,UAA8BxH,GAwCpC,MAAMyH,UAA0BzH,GAsChC,MAAM0H,UAA8B1H,GAqCpC,MAAM2H,UAA2B3H,GAkCjC,MAAM4H,UAA4B5H,GAsDlC,MAAM6H,UAA6B7H,GA4EnC,MAAM8H,UAAkC9H,GAsCxC,MAAM+H,UAAuC/H,GAsC7C,MAAMgI,UAA+BhI,GAsErC,MAAMiI,WAA2BjI,GAsEjC,MAAMkI,WAAkClI,GAkExC,MAAMmI,WAA+BnI,GAuNrC,MAAMoI,WAAkCpI,EAC3CnB,gBAAkB,eASlBoB,YAAYrE,EAAQzC,EAAS6D,EAAwBY,GACjDsC,MAAMtE,EAAQzC,GACdgH,KAAKnD,uBAAyBA,EAC9BmD,KAAKvC,kBAAoBA,EAGzB,MAAMyK,EAAgBlI,KAAKvE,OAAO0M,QAC5BC,EAAgBpI,KAAKvE,OAAO4M,QAG5BC,EAAmBJ,EAAczF,YAEnC8F,GAAiCrI,IAAIoI,IAClCE,GAAoCtI,IAAIoI,KAE3CzP,QAAQC,KAAK,2BAA2BwP,wIAI5C,MAAMG,EAAe9F,GAAiCzC,IAAIkI,EAAc3F,YACxE,IAAKgG,EACD,MAAM,IAAIhP,MAAM,6EAA6EuG,KAAKvE,OAAO4M,QAAQ5F,eAIrH,MAEM4F,EAAU,IAAIK,EAFMD,EAAa,IAEDL,EAAevL,EAAwBY,GAE7EuC,KAAK6F,gBAAkB,uBAAwBwC,EAC3CrI,KAAK6F,iBAEL7F,KAAKqG,mBAAqBgC,EAAQhC,mBAClCrG,KAAKkG,kBAAoBmC,EAAQnC,kBACjClG,KAAKmG,eAAiBkC,EAAQlC,eAE9BnG,KAAK2I,mBAAqBN,EAAQM,mBAClC3I,KAAK+F,kBAAoBsC,EAAQtC,kBACjC/F,KAAKgG,eAAiBqC,EAAQrC,iBAI9BhG,KAAKwG,WAAa6B,EAAQ7B,WAC1BxG,KAAK0G,UAAY2B,EAAQ3B,UACzB1G,KAAKuG,OAAS8B,EAAQ9B,OAE9B,EAMG,MAAMqC,WAA4B/I,GA0HlC,MAAMgJ,WAA4BhJ,EAOrCC,YAAYrE,EAAQzC,EAASyE,GACzBsC,MAAMtE,EAAQzC,GACdgH,KAAKvC,kBAAoBA,EAGzBuC,KAAKvE,OAAOD,aAAewE,KAAKvE,OAAOC,aAEvCsE,KAAK0G,UAAY1G,KAAKvE,OAAOqN,OAC7B9I,KAAKwG,WAAaxG,KAAKvE,OAAOsN,QAC9B/I,KAAKuG,OAASvG,KAAKvE,OAAOuN,OAAShJ,KAAK0G,SAC5C,EAgBG,MAAMuC,WAA8BpJ,EAOvCC,YAAYrE,EAAQzC,EAASyE,GACzBsC,MAAMtE,EAAQzC,GACdgH,KAAKvC,kBAAoBA,EAGzBuC,KAAKvE,OAAOD,aAAewE,KAAKvE,OAAOC,aAEvCsE,KAAK0G,UAAY1G,KAAKvE,OAAOiL,UAC7B1G,KAAKwG,WAAaxG,KAAKvE,OAAO+K,WAC9BxG,KAAKuG,OAASvG,KAAKvE,OAAOyN,YAAclJ,KAAK0G,SACjD,EASG,MAAMyC,WAA+BtJ,EAOxCC,YAAYrE,EAAQzC,EAASyE,GACzBsC,MAAMtE,EAAQzC,GACdgH,KAAKvC,kBAAoBA,EAGzBuC,KAAKvE,OAAOD,aAAewE,KAAKvE,OAAOC,aAEvCsE,KAAK0G,UAAY1G,KAAKvE,OAAO2N,oBAC7BpJ,KAAKwG,WAAaxG,KAAKvE,OAAO4N,kBAC9BrJ,KAAKuG,OAASvG,KAAKvE,OAAOyN,YAAclJ,KAAK0G,SACjD,EAUG,MAAM4C,WAA4BzJ,EAOrCC,YAAYrE,EAAQzC,EAASyE,GACzBsC,MAAMtE,EAAQzC,GACdgH,KAAKvC,kBAAoBA,EAGzBuC,KAAKvE,OAAOD,aAAewE,KAAKvE,OAAOC,aAEvCsE,KAAK0G,UAAY1G,KAAKvE,OAAOqN,OAC7B9I,KAAKwG,WAAaxG,KAAKvE,OAAOsN,QAC9B/I,KAAKuG,OAASvG,KAAKvE,OAAOuN,OAAShJ,KAAK0G,SAC5C,EAWG,MAAM6C,WAAkC1J,EAO3CC,YAAYrE,EAAQzC,EAASyE,GACzBsC,MAAMtE,EAAQzC,GACdgH,KAAKvC,kBAAoBA,EAGzBuC,KAAKvE,OAAOD,aAAewE,KAAKvE,OAAOC,aAEvCsE,KAAK0G,UAAY1G,KAAKvE,OAAOqN,OAC7B9I,KAAKwG,WAAaxG,KAAKvE,OAAOsN,QAC9B/I,KAAKuG,OAASvG,KAAKvE,OAAOuN,OAAShJ,KAAK0G,SAC5C,EAUG,MAAM8C,WAA+B3J,EAOxCC,YAAYrE,EAAQzC,EAASyE,GACzBsC,MAAMtE,EAAQzC,GACdgH,KAAKvC,kBAAoBA,EAGzBuC,KAAKvE,OAAOD,aAAewE,KAAKvE,OAAOC,aAEvCsE,KAAK0G,UAAY1G,KAAKvE,OAAOqN,OAC7B9I,KAAKwG,WAAaxG,KAAKvE,OAAOsN,QAC9B/I,KAAKuG,OAASvG,KAAKvE,OAAOuN,OAAShJ,KAAK0G,SAC5C,EAoBG,MAAM+C,WAA6B5J,EAOtCC,YAAYrE,EAAQzC,EAASyE,GACzBsC,MAAMtE,EAAQzC,GACdgH,KAAKvC,kBAAoBA,EAGzBuC,KAAKvE,OAAOD,aAAewE,KAAKvE,OAAOC,aAEvCsE,KAAK0G,UAAY1G,KAAKvE,OAAO2N,oBAC7BpJ,KAAKwG,WAAaxG,KAAKvE,OAAO4N,kBAC9BrJ,KAAKuG,OAASvG,KAAKvE,OAAOyN,YAAclJ,KAAK0G,SACjD,EAeG,MAAMgD,WAA6B7J,EAOtCC,YAAYrE,EAAQzC,EAASyE,GACzBsC,MAAMtE,EAAQzC,GACdgH,KAAKvC,kBAAoBA,EAGzBuC,KAAKvE,OAAOD,aAAewE,KAAKvE,OAAOC,aAEvCsE,KAAK0G,UAAY1G,KAAKvE,OAAOqN,OAC7B9I,KAAKwG,WAAaxG,KAAKvE,OAAOsN,QAC9B/I,KAAKuG,OAASvG,KAAKvE,OAAOyN,YAAclJ,KAAK0G,SACjD,EAgBG,MAAMiD,WAA2B9J,EAOpCC,YAAYrE,EAAQzC,EAASyE,GACzBsC,MAAMtE,EAAQzC,GACdgH,KAAKvC,kBAAoBA,EAGzBuC,KAAKvE,OAAOD,aAAewE,KAAKvE,OAAOC,aAEvCsE,KAAK0G,UAAY1G,KAAKvE,OAAOmO,QAC7B5J,KAAKwG,WAAaxG,KAAKvE,OAAOoO,SAC9B7J,KAAKuG,OAASvG,KAAKvE,OAAOqO,QAAU9J,KAAK0G,SAC7C,EAiBG,MAAMqD,WAA2BlK,EAOpCC,YAAYrE,EAAQzC,EAASyE,GACzBsC,MAAMtE,EAAQzC,GACdgH,KAAKvC,kBAAoBA,EAGzBuC,KAAKvE,OAAOD,aAAewE,KAAKvE,OAAOC,aAEvCsE,KAAK0G,UAAY1G,KAAKvE,OAAO2N,oBAC7BpJ,KAAKwG,WAAaxG,KAAKvE,OAAO4N,kBAC9BrJ,KAAKuG,OAASvG,KAAKvE,OAAOyN,YAAclJ,KAAK0G,SACjD,EAeG,MAAMsD,WAA2BnK,GAajC,MAAMoK,WAAiCpK,GAgBvC,MAAMqK,WAA4BrK,GAclC,MAAMsK,WAA4BtK,GAsBlC,MAAMuK,WAAkCxD,EAO3C9G,aAAY,OAAE5C,EAAM,WAAEmN,IAClBtK,QACAC,KAAK9C,OAASA,EACd8C,KAAKqK,WAAaA,CACtB,EAGG,MAAMC,WAA+B1D,EAOxC9G,aAAY,OAAE5C,EAAM,WAAEmN,EAAU,WAAEE,IAC9BxK,QACAC,KAAK9C,OAASA,EACd8C,KAAKqK,WAAaA,EAClBrK,KAAKuK,WAAaA,CACtB,EAMG,MAAMC,WAA4B3K,GAiBlC,MAAM4K,WAA8B5K,GAsBpC,MAAM6K,WAA4B7K,GAalC,MAAM8K,WAAiC9K,GAgFvC,MAAM+K,WAA6B/K,GAWnC,MAAMgL,WAAmCjE,EAO5C9G,aAAY,OAAE5C,EAAM,WAAEmN,IAClBtK,QACAC,KAAK9C,OAASA,EACd8C,KAAKqK,WAAaA,CACtB,EAMG,MAAMS,WAA2BjL,GACjC,MAAMkL,WAAiBD,GAO1B5S,YAAYiE,GACR,OAAO,IAAI6O,SAAiCjL,MAAMkL,MAAM9O,GAC5D,EAOG,MAAM6O,WAAmCpE,EAM5C9G,aAAY,WAAEoL,EAAU,WAAEX,IACtBxK,QACAC,KAAKkL,WAAaA,EAClBlL,KAAKuK,WAAaA,CACtB,EAOG,MAAMY,WAA8BtL,GA+BpC,MAAMuL,WAA8BvL,GAgCpC,MAAMwL,WAAgCxL,GAwDtC,MAAMyL,WAA6BzL,GA+DnC,MAAM0L,WAAgC1L,GAoJtC,MAAM2L,GAKTzK,4BAA8B,KAM9BA,qBAAsB,EAItBA,6BAA6B3I,GAA+B,UACxDI,GAAY,EAAI,kBAChBwI,EAAoB,KAAI,OACxBvF,EAAS,KAAI,UACbwF,EAAY,KAAI,iBAChBC,GAAmB,EAAK,SACxBC,EAAW,OAAM,gBACjBC,EAAkB,MAClB,CAAC,GAED,IAAI9I,EAAU,CACVE,YACAwI,oBACAvF,SACAwF,YACAC,mBACAC,WACAC,mBAQJ,GANA3F,QAAe,oBAA2BrD,EAA+BE,GACpEA,EAAQmD,SAETnD,EAAQmD,OAASA,IAGhBuE,KAAKyL,qBACN,MAAM,IAAIhS,MAAM,wEAA0EuG,KAAK+C,MAGnG,IAAK,IAAI2I,KAAuB1L,KAAKyL,qBAAsB,CACvD,MAAME,EAAYD,EAAoBxL,IAAIzE,EAAOgH,YACjD,GAAKkJ,EAGL,aAAaA,EAAU,GAAGC,gBAAgBxT,EAA+BE,EAC7E,CAEA,GAAI0H,KAAK6L,aAEL,OADAhT,QAAQC,KAAK,wBAAwB2C,EAAOgH,+DAC/B5C,EAAgB+L,gBAAgBxT,EAA+BE,GAE5E,MAAMmB,MAAM,2BAA2BgC,EAAOgH,aAEtD,EAGJ,MAAM8F,GAAmC,IAAIxQ,IAAI,CAC7C,CAAC,OAAQ,CAAC,YA35EP,cAAwB8O,MA45E3B,CAAC,YAAa,CAAC,iBAt1EZ,cAA6BC,MAu1EhC,CAAC,UAAW,CAAC,eAjxEV,cAA2BC,MAkxE9B,CAAC,aAAc,CAAC,iBA3sEb,cAA6BC,MA4sEhC,CAAC,QAAS,CAAC,aA9gER,cAAyBG,MA+gE5B,CAAC,SAAU,CAAC,cAp6DT,cAA0BE,MAq6D7B,CAAC,aAAc,CAAC,kBA3oEb,cAA8BJ,MA4oEjC,CAAC,UAAW,CAAC,eAnkDV,cAA2BY,MAokD9B,CAAC,MAAO,CAAC,WA1/CN,cAAuBC,OA2/C1B,CAAC,cAAe,CAAC,kBAz7Cd,cAA8BC,OA07CjC,CAAC,OAAQ,CAAC,YAvjCP,cAAwBa,OAwjC3B,CAAC,aAAc,CAAC,kBA5kEb,cAA8B1B,MA6kEjC,CAAC,cAAe,CAAC,mBAn9Dd,cAA+BE,MAo9DlC,CAAC,WAAY,CAAC,gBA1TX,cAA4BiE,OA2T/B,CAAC,QAAS,CAAC,aAnQR,cAAyBC,OAqQ5B,CAAC,OAAQ,CAAC,YA/oBP,cAAwBnB,OAgpB3B,CAAC,MAAO,CAAC,WA3rBN,cAAuBH,OA4rB1B,CAAC,YAAa,CAAC,iBA/qBZ,cAA6BC,OAgrBhC,CAAC,OAAQ,CAAC,YAhqBP,cAAwBC,OAiqB3B,CAAC,OAAQ,CAAC,YA7lBP,cAAwBM,OA8lB3B,CAAC,SAAU,CAAC,cAzkBT,cAA0BC,OA0kB7B,CAAC,OAAQ,CAAC,YAxjBP,cAAwBC,OAyjB3B,CAAC,aAAc,CAAC,iBAjeb,cAA6BC,OAkehC,CAAC,QAAS,CAAC,aA7dR,cAAyBC,OA+d5B,CAAC,UAAW,CAAC,kBArGV,cAA8B/K,EACjCnB,gBAAkB,iBAsGlB,CAAC,MAAO,CAAC,WAAYqM,OAGnBvC,GAAsC,IAAIzQ,IAAI,CAChD,CAAC,KAAM,CAAC,UAt5DL,cAAsBuP,MAu5DzB,CAAC,SAAU,CAAC,cA92DT,cAA0BC,MA+2D7B,CAAC,MAAO,CAAC,WA70DN,cAAuBC,MA80D1B,CAAC,OAAQ,CAAC,YAzyDP,cAAwBC,MA0yD3B,CAAC,QAAS,CAAC,aApvDR,cAAyBC,MAqvD5B,CAAC,SAAU,CAAC,cA3aT,cAA0ByD,OA4a7B,CAAC,UAAW,CAAC,eAh5CV,cAA2BnD,OAi5C9B,CAAC,UAAW,CAAC,cA9YV,cAA0BoD,OA+Y7B,CAAC,aAAc,CAAC,kBA5qDb,cAA8BzD,MA6qDjC,CAAC,mBAAoB,CAAC,uBAvoDnB,cAAmCC,QA2oDpCkE,GAAmC,IAAI/T,IAAI,CAC7C,CAAC,QAAS,CAAC,aAryBR,cAAyB2R,OAsyB5B,CAAC,OAAQ,CAAC,YA3/BP,cAAwBb,OA4/B3B,CAAC,OAAQ,CAAC,YAt6BP,cAAwBS,OAu6B3B,CAAC,cAAe,CAAC,kBA34Bd,cAA8BC,OA44BjC,CAAC,UAAW,CAAC,cA99BV,cAA0BN,OA+9B7B,CAAC,WAAY,CAAC,eAr8BX,cAA2BE,OAs8B9B,CAAC,UAAW,CAAC,eAj3BV,cAA2BK,OAk3B9B,CAAC,QAAS,CAAC,aA70BR,cAAyBC,OA80B5B,CAAC,MAAO,CAAC,WA5wBN,cAAuBE,OA6wB1B,CAAC,MAAO,CAAC,WA3uBN,cAAuBI,SA8uBxBlH,GAA2C,IAAI9K,IAAI,CACrD,CAAC,WAAY,CAAC,0BAnQX,cAAsCwT,OAoQzC,CAAC,UAAW,CAAC,kCAj6CV,cAA8CvD,GAEjDnK,yBAA0B,EAC1Ba,gBAAkB,iBASlBoB,YAAYrE,EAAQzC,EAAS6D,EAAwBY,GACjDsC,MAAMtE,EAAQzC,GACdgH,KAAKnD,uBAAyBA,EAC9BmD,KAAKvC,kBAAoBA,EAEzBuC,KAAKqG,mBAAqBrG,KAAKvE,OAAOsQ,eACtC/L,KAAKkG,kBAAoBlG,KAAKvE,OAAOuQ,wBACrChM,KAAKmG,eAAiBnG,KAAKvE,OAAOqO,QAAU9J,KAAKkG,kBAEjDlG,KAAK2I,mBAAqB3I,KAAKvE,OAAOwQ,eACtCjM,KAAK+F,kBAAoB/F,KAAKvE,OAAOyQ,wBACrClM,KAAKgG,eAAiBhG,KAAKvE,OAAOqO,QAAU9J,KAAK+F,iBACrD,CAmBA7N,eACIe,EACAwE,EAAoB,KACpB+D,EAAmB,MAqBnB,GAZA/D,EAAoBuC,KAAKqC,uBAAuB5E,GAIhDA,EAAkB0O,qBAAsB,EAIpC1O,EAAkB0O,oBAClB3K,EAAmB,CAAC,IAAI,KAAgC/D,KAGxDA,EAAkB2O,0BAClB3O,EAAkBmG,mBAAoB,EACtCnG,EAAkBsH,yBAA0B,EAEb,cAA3BtH,EAAkB4O,MAClBxT,QAAQC,KAAK,qEAGZ2E,EAAkB6O,iBACnB,MAAM,IAAI7S,MACN,uNAMZ,MAAM8S,QAAgBxM,MAAMyM,SAASvT,EAAQwE,EAAmB+D,GAUhE,OARI/D,EAAkB2O,yBAA2B3O,EAAkB6O,kBAC/DC,EAA0B,iBAAIvM,KAAKyM,0BAC/BF,EACA9O,EAAkB6O,gBAClB7O,EAAkBiP,aAInBH,CACX,CAcAE,0BAA0BE,EAAkBL,EAAiBI,EAAa,KAAME,EAAiB,KAC7F,IAAKD,EAAiB1H,iBAClB,MAAM,IAAIxL,MACN,4JAKR,IAAIoT,EAAsB7M,KAAKvE,OAAOoR,yBACVvT,IAAxBuT,IACAhU,QAAQC,KAAK,wEACb+T,EAAsB,GAG1B,MAAMC,EAAkBH,EAAiB1H,iBAAiB9J,KAAIyJ,IAG1D,IAAIK,EAAmBrK,MAAMK,KAAK,CAAEzB,OAAQwG,KAAKvE,OAAOsQ,iBACpD,CAACgB,EAAG3G,KAAM,QAAIxB,EAAMzJ,KAAIJ,GAAKA,EAAEqL,KAAK,KAGpC4G,GAAU,QAAMV,EAAgBnR,KAAI,EAAE8R,EAAGC,KAClCR,EACDzH,EAAiBgI,GAAGtO,MAAM,KAAMuO,EAAG,KAAM,CAAC,EAAGR,IAC7CzH,EAAiBgI,GAAGtO,MAAM,KAAMuO,MAE1CF,EAAUA,EAAQG,UAAU,EAAG,EAAG,EAAG,GAErC,IAAKC,EAAKC,IAAkB,QAASL,GAAU,EAAG,GAAG,GAGjDM,EAAkBN,EAAQO,QAE9B,IAAK,IAAIjJ,EAAI,EAAGA,EAAIgJ,EAAgBvR,KAAK,KAAMuI,EAAG,CAC9C,IAAIkJ,EAAUF,EAAgBhJ,GAE9B,IAAK,IAAIC,EAAI,EAAGA,EAAIiJ,EAAQzR,KAAK,KAAMwI,EAAG,CACtC,IAAIkJ,EAAUD,EAAQjJ,GAEtB,MAAMmJ,EAAYN,EAAI9I,GAAGC,GAAG,GACtBoJ,EAAaN,EAAe/I,GAAGC,GAAG,GAExC,IAAK,IAAIqJ,EAAI,EAAGA,EAAIH,EAAQ1R,KAAK,KAAM6R,EAAG,CAEtC,IAAIC,EAAUJ,EAAQG,GACtB,IAAK,IAAIE,EAAI,EAAGA,EAAID,EAAQ/R,KAAKtC,SAAUsU,EACvCD,EAAQ/R,KAAKgS,IAAMD,EAAQ/R,KAAKgS,GAAKH,EAAW7R,KAAKgS,IAAMJ,EAAU5R,KAAKgS,GAI9ED,EAAQ/R,KAAKiS,KAAI,SAAaF,EAAQ/R,KAAM+Q,GAChD,CACJ,CACJ,CAIA,OADe,QAAKS,EAAiB,EACxB,IAGXU,EAAkB,CAACrB,EAAiB7H,UAAUtL,OAAQmT,EAAiB7H,UAAU,GAAGtL,QAEpFyU,EAAa,IAAI,KACnB,UACA,IAAIC,aAAaF,EAAgB,GAAKA,EAAgB,IACtDA,GAIJ,IAAK,IAAIG,EAAY,EAAGA,EAAYH,EAAgB,KAAMG,EAAW,CAGjE,MAAMC,EAAStB,EAAgBqB,GAAWE,MAAMC,SAAS,GACzD,IAAKC,EAAcC,IAAgB,QAAmBJ,GAElDK,EAAQ7T,MAAMK,KAAK,CAAEzB,OAAQ+U,EAAa/U,OAAS,IAAK,CAACkV,EAAGtI,IAAMmI,EAAanI,EAAI,GAAKmI,EAAanI,KACrGuI,GAAQ,QAAY,CAAC,GAAIF,GAAOtT,KAAIJ,KAAOA,IAE3C6T,EAAa,GACjB,IAAK,IAAIxI,EAAI,EAAGA,EAAIuI,EAAMnV,SAAU4M,EAC5BuI,EAAMvI,IACNwI,EAAWrV,KAAKiV,EAAapI,GAAKwG,GAI1CqB,EAAWE,GAAWrS,KAAKiS,IAAIa,EAAY,EAC/C,CAEA,OAAOX,CACX,OA+tCEY,GAA8C,IAAI9W,IAAI,CACxD,CAAC,WAAY,CAAC,0BAnQX,cAAsCwT,GASzCzL,YAAYrE,EAAQzC,EAAS6D,EAAwBY,GACjDsC,MAAMtE,EAAQzC,GACdgH,KAAKnD,uBAAyBA,EAC9BmD,KAAKvC,kBAAoBA,EAEzBuC,KAAKqG,mBAAqBrG,KAAKvE,OAAOsQ,eACtC/L,KAAKkG,kBAAoBlG,KAAKvE,OAAOuQ,wBACrChM,KAAKmG,eAAiBnG,KAAKvE,OAAOyN,YAAclJ,KAAKkG,kBAErDlG,KAAK2I,mBAAqB3I,KAAKvE,OAAOwQ,eACtCjM,KAAK+F,kBAAoB/F,KAAKvE,OAAOyQ,wBACrClM,KAAKgG,eAAiBhG,KAAKvE,OAAOyN,YAAclJ,KAAK+F,iBACzD,CAuBA7N,sBAAsB4W,EAAcC,GAAoB,UACpDC,EAAY,GAAG,YACfC,EAAc,EAAG,YACjBC,EAAc,GAAI,QAClBC,EAAU,MAEV,CAAC,GAED,MAAMhT,EAAe,CACjBM,UAAWqS,IAGT,gBAAE1S,EAAe,uBAAEU,SAAiCR,EAAe0D,KAAM7D,GAEzEiT,EAAIhT,EAAgBL,KAAK,GAAKiE,KAAKvE,OAAO4T,iBAC1CC,EAASC,KAAKC,MAAMJ,EAAIF,GACxBO,EAASF,KAAKC,MAAMJ,EAAIH,GAExBS,EAAe1P,KAAKvE,OAAOiU,aAEjC,IAAIC,EAAmB,GACnBtT,EAAkB,KAClBuT,EAAkB,KAClBC,EAAM,EAEV,OAAa,GACPA,EAEF,MAAMjT,EAAmBZ,IAAa4T,GACtC,IAAIE,EAEAA,EADAF,EACkBA,EAAgBG,oBAEhB,IAAI,KAClB,UACA,IAAI7B,aAAawB,GACjB,CAAC,EAAG,EAAGA,IAGf,IAAIlT,EAAe,CACfI,mBACAkT,kBACAhT,uBAAwBA,EACxBiS,mBAAoBA,EACpBpS,sBAAuBP,GAG3B4D,KAAKhD,iBAAiBR,EAAcH,GACpCuT,QAAwB7W,EAAWiH,KAAKnD,uBAAwBL,GAChEH,EAAkB2D,KAAK7C,iBAAiByS,EAAiBvT,GAEzD,MAAM,KAAE2T,EAAI,SAAEC,GAAaL,EAG3B,GAFAD,EAAiBpW,KAAK0W,GAElBJ,GAAOJ,IAEP7U,MAAMK,KAAK+U,EAAKlU,MAAM9B,QAAOkW,GAAKA,GAAKlB,IAAWxV,OAAS,GAAKqW,GAAOP,GAEvE,KAER,CAEA,MAAMa,GAAc,QAAIR,IAClB,SAAES,SAAmBrX,EAAWoW,EAAQnW,QAAS,CAAEmX,gBAEzD,MAAO,CACHA,cACAC,WAGR,OAoJEC,GAAkD,IAAItY,IAAI,CAC5D,CAAC,OAAQ,CAAC,gCA38EP,cAA4C8O,EAO/C3O,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KAm8EA,CAAC,YAAa,CAAC,qCAt4EZ,cAAiD2K,EAOpD5O,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KA83EA,CAAC,UAAW,CAAC,mCAj0EV,cAA+C4K,EAOlD7O,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KAyzEA,CAAC,aAAc,CAAC,qCA3vEb,cAAiD6K,EAOpD9O,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KAmvEA,CAAC,QAAS,CAAC,iCA9jER,cAA6CgL,EAOhDjP,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KAsjEA,CAAC,SAAU,CAAC,kCAv+DT,cAA8CkL,EAOjDnP,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KA+9DA,CAAC,aAAc,CAAC,sCA1sEb,cAAkD8K,EAOrD/O,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KAksEA,CAAC,UAAW,CAAC,mCAnnDV,cAA+C0L,EAOlD3P,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KA2mDA,CAAC,MAAO,CAAC,+BA1iDN,cAA2C2L,GAO9C5P,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KAkiDA,CAAC,cAAe,CAAC,sCAz+Cd,cAAkD4L,GAOrD7P,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KAi+CA,CAAC,OAAQ,CAAC,gCAnzDP,cAA4CsL,EAO/CvP,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KA2yDA,CAAC,QAAS,CAAC,iCA9vDR,cAA6CuL,EAOhDxP,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KAsvDA,CAAC,aAAc,CAAC,sCA7nEb,cAAkD+K,EAOrDhP,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KAqnEA,CAAC,cAAe,CAAC,uCA5gEd,cAAmDiL,EAOtDlP,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,OAsgEEoU,GAA+C,IAAIxY,IAAI,CACzD,CAAC,OAAQ,CAAC,6BA78EP,cAAyC8O,EAO5C3O,YAAYiE,GACR,OAAO,IAAIqU,SAA4BzQ,MAAMkL,MAAM9O,GACvD,KAq8EA,CAAC,YAAa,CAAC,kCAx4EZ,cAA8C2K,EAOjD5O,YAAYiE,GACR,OAAO,IAAIqU,SAA4BzQ,MAAMkL,MAAM9O,GACvD,KAg4EA,CAAC,UAAW,CAAC,gCAn0EV,cAA4C4K,EAO/C7O,YAAYiE,GACR,OAAO,IAAIqU,SAA4BzQ,MAAMkL,MAAM9O,GACvD,KA2zEA,CAAC,aAAc,CAAC,kCA7vEb,cAA8C6K,EAOjD9O,YAAYiE,GACR,OAAO,IAAIqU,SAA4BzQ,MAAMkL,MAAM9O,GACvD,KAqvEA,CAAC,QAAS,CAAC,8BAhkER,cAA0CgL,EAO7CjP,YAAYiE,GACR,OAAO,IAAIqU,SAA4BzQ,MAAMkL,MAAM9O,GACvD,KAwjEA,CAAC,aAAc,CAAC,mCA3sEb,cAA+C8K,EAOlD/O,YAAYiE,GACR,OAAO,IAAIqU,SAA4BzQ,MAAMkL,MAAM9O,GACvD,KAmsEA,CAAC,UAAW,CAAC,gCApnDV,cAA4C0L,EAO/C3P,YAAYiE,GACR,OAAO,IAAIqU,SAA4BzQ,MAAMkL,MAAM9O,GACvD,KA4mDA,CAAC,MAAO,CAAC,4BA3iDN,cAAwC2L,GAO3C5P,YAAYiE,GACR,OAAO,IAAIqU,SAA4BzQ,MAAMkL,MAAM9O,GACvD,KAmiDA,CAAC,cAAe,CAAC,mCA1+Cd,cAA+C4L,GAOlD7P,YAAYiE,GACR,OAAO,IAAIqU,SAA4BzQ,MAAMkL,MAAM9O,GACvD,OAo+CEyG,GAA+C,IAAI7K,IAAI,CACzD,CAAC,KAAM,CAAC,6BAl9DL,cAAyCuP,EAS5CxH,YAAYrE,EAAQzC,EAAS6D,EAAwBY,GACjDsC,MAAMtE,EAAQzC,GACdgH,KAAKnD,uBAAyBA,EAC9BmD,KAAKvC,kBAAoBA,EAEzBuC,KAAKqG,mBAAqBrG,KAAKvE,OAAO4K,mBACtCrG,KAAKkG,kBAAoBlG,KAAKvE,OAAOiL,UACrC1G,KAAKmG,eAAiBnG,KAAKvE,OAAOgV,KAElCzQ,KAAK2I,mBAAqB3I,KAAKvE,OAAO+K,WACtCxG,KAAK+F,kBAAoB/F,KAAKvE,OAAOiL,UACrC1G,KAAKgG,eAAiBhG,KAAKvE,OAAOgV,IACtC,KA87DA,CAAC,SAAU,CAAC,iCA16DT,cAA6ClJ,EAQhDzH,YAAYrE,EAAQzC,EAAS6D,EAAwBY,GACjDsC,MAAMtE,EAAQzC,GACdgH,KAAKnD,uBAAyBA,EAC9BmD,KAAKvC,kBAAoBA,EAEzBuC,KAAKqG,mBAAqBrG,KAAKvE,OAAO4K,mBACtCrG,KAAKkG,kBAAoBlG,KAAKvE,OAAOiL,UACrC1G,KAAKmG,eAAiBnG,KAAKvE,OAAOgV,KAElCzQ,KAAK2I,mBAAqB3I,KAAKvE,OAAO+K,WACtCxG,KAAK+F,kBAAoB/F,KAAKvE,OAAOiL,UACrC1G,KAAKgG,eAAiBhG,KAAKvE,OAAOgV,IACtC,KAu5DA,CAAC,MAAO,CAAC,8BAz4DN,cAA0CjJ,EAS7C1H,YAAYrE,EAAQzC,EAAS6D,EAAwBY,GACjDsC,MAAMtE,EAAQzC,GACdgH,KAAKnD,uBAAyBA,EAC9BmD,KAAKvC,kBAAoBA,EAEzBuC,KAAKqG,mBAAqBrG,KAAKvE,OAAO4K,mBACtCrG,KAAKkG,kBAAoBlG,KAAKvE,OAAOiL,UACrC1G,KAAKmG,eAAiBnG,KAAKvE,OAAOgV,KAElCzQ,KAAK2I,mBAAqB3I,KAAKvE,OAAO+K,WACtCxG,KAAK+F,kBAAoB/F,KAAKvE,OAAOiL,UACrC1G,KAAKgG,eAAiBhG,KAAKvE,OAAOgV,IACtC,KAq3DA,CAAC,OAAQ,CAAC,+BAr2DP,cAA2ChJ,EAS9C3H,YAAYrE,EAAQzC,EAAS6D,EAAwBY,GACjDsC,MAAMtE,EAAQzC,GACdgH,KAAKnD,uBAAyBA,EAC9BmD,KAAKvC,kBAAoBA,EAEzBuC,KAAKqG,mBAAqBrG,KAAKvE,OAAOsQ,eACtC/L,KAAKkG,kBAAoBlG,KAAKvE,OAAOuQ,wBACrChM,KAAKmG,eAAiBnG,KAAKvE,OAAOqO,QAAU9J,KAAKkG,kBAEjDlG,KAAK2I,mBAAqB3I,KAAKvE,OAAOwQ,eACtCjM,KAAK+F,kBAAoB/F,KAAKvE,OAAOyQ,wBACrClM,KAAKgG,eAAiBhG,KAAKvE,OAAOqO,QAAU9J,KAAK+F,iBACrD,KAi1DA,CAAC,QAAS,CAAC,gCAhzDR,cAA4C2B,EAS/C5H,YAAYrE,EAAQzC,EAAS6D,EAAwBY,GACjDsC,MAAMtE,EAAQzC,GACdgH,KAAKnD,uBAAyBA,EAC9BmD,KAAKvC,kBAAoBA,EAEzBuC,KAAKqG,mBAAqBrG,KAAKvE,OAAOsQ,eACtC/L,KAAKkG,kBAAoBlG,KAAKvE,OAAOuQ,wBACrChM,KAAKmG,eAAiBnG,KAAKvE,OAAOqO,QAAU9J,KAAKkG,kBAEjDlG,KAAK2I,mBAAqB3I,KAAKvE,OAAOwQ,eACtCjM,KAAK+F,kBAAoB/F,KAAKvE,OAAOyQ,wBACrClM,KAAKgG,eAAiBhG,KAAKvE,OAAOqO,QAAU9J,KAAK+F,iBACrD,KA4xDA,CAAC,SAAU,CAAC,gBA1eT,cAA4BoF,GAS/BrL,YAAYrE,EAAQzC,EAAS6D,EAAwBY,GACjDsC,MAAMtE,EAAQzC,GACdgH,KAAKnD,uBAAyBA,EAC9BmD,KAAKvC,kBAAoBA,EAEzBuC,KAAKqG,mBAAqBrG,KAAKvE,OAAOsQ,eACtC/L,KAAKkG,kBAAoBlG,KAAKvE,OAAOuQ,wBACrChM,KAAKmG,eAAiBnG,KAAKvE,OAAOqO,QAAU9J,KAAKkG,kBAEjDlG,KAAK2I,mBAAqB3I,KAAKvE,OAAOwQ,eACtCjM,KAAK+F,kBAAoB/F,KAAKvE,OAAOyQ,wBACrClM,KAAKgG,eAAiBhG,KAAKvE,OAAOqO,QAAU9J,KAAK+F,iBACrD,KAsdA,CAAC,UAAW,CAAC,iCA5cV,cAA6CqF,GAShDtL,YAAYrE,EAAQzC,EAAS6D,EAAwBY,GACjDsC,MAAMtE,EAAQzC,GACdgH,KAAKnD,uBAAyBA,EAC9BmD,KAAKvC,kBAAoBA,EAEzBuC,KAAKqG,mBAAqBrG,KAAKvE,OAAOsQ,eACtC/L,KAAKkG,kBAAoBlG,KAAKvE,OAAOuQ,wBACrChM,KAAKmG,eAAiBnG,KAAKvE,OAAOqO,QAAU9J,KAAKkG,kBAEjDlG,KAAK2I,mBAAqB3I,KAAKvE,OAAOwQ,eACtCjM,KAAK+F,kBAAoB/F,KAAKvE,OAAOyQ,wBACrClM,KAAKgG,eAAiBhG,KAAKvE,OAAOqO,QAAU9J,KAAK+F,iBACrD,KAwbA,CAAC,aAAc,CAAC,qCAvuDb,cAAiD4B,EASpD7H,YAAYrE,EAAQzC,EAAS6D,EAAwBY,GACjDsC,MAAMtE,EAAQzC,GACdgH,KAAKnD,uBAAyBA,EAC9BmD,KAAKvC,kBAAoBA,EAEzBuC,KAAKqG,mBAAqBrG,KAAKvE,OAAOsQ,eACtC/L,KAAKkG,kBAAoBlG,KAAKvE,OAAOuQ,wBACrChM,KAAKmG,eAAiBnG,KAAKvE,OAAOqO,QAAU9J,KAAKkG,kBAEjDlG,KAAK2I,mBAAqB3I,KAAKvE,OAAOwQ,eACtCjM,KAAK+F,kBAAoB/F,KAAKvE,OAAOyQ,wBACrClM,KAAKgG,eAAiBhG,KAAKvE,OAAOqO,QAAU9J,KAAK+F,iBACrD,KAmtDA,CAAC,mBAAoB,CAAC,0CAlsDnB,cAAsD6B,EASzD9H,YAAYrE,EAAQzC,EAAS6D,EAAwBY,GACjDsC,MAAMtE,EAAQzC,GACdgH,KAAKnD,uBAAyBA,EAC9BmD,KAAKvC,kBAAoBA,EAEzBuC,KAAKqG,mBAAqBrG,KAAKvE,OAAOsQ,eACtC/L,KAAKkG,kBAAoBlG,KAAKvE,OAAOuQ,wBACrChM,KAAKmG,eAAiBnG,KAAKvE,OAAOqO,QAAU9J,KAAKkG,kBAEjDlG,KAAK2I,mBAAqB3I,KAAKvE,OAAOwQ,eACtCjM,KAAK+F,kBAAoB/F,KAAKvE,OAAOyQ,wBACrClM,KAAKgG,eAAiBhG,KAAKvE,OAAOqO,QAAU9J,KAAK+F,iBACrD,OAgrDEpD,GAAmC,IAAI5K,IAAI,CAC7C,CAAC,QAAS,CAAC,mBA/1BR,cAA+B2R,OAg2BlC,CAAC,OAAQ,CAAC,kBArjCP,cAA8Bb,OAsjCjC,CAAC,OAAQ,CAAC,kBAn+BP,cAA8BS,OAo+BjC,CAAC,cAAe,CAAC,wBAx8Bd,cAAoCC,OAy8BvC,CAAC,UAAW,CAAC,oBA3hCV,cAAgCN,OA4hCnC,CAAC,WAAY,CAAC,qBAlgCX,cAAiCE,OAmgCpC,CAAC,UAAW,CAAC,qBA36BV,cAAiCK,OA46BpC,CAAC,QAAS,CAAC,mBA14BR,cAA+BC,OA24BlC,CAAC,MAAO,CAAC,iBAt0BN,cAA6BE,OAu0BhC,CAAC,MAAO,CAAC,iBAryBN,cAA6BI,OAsyBhC,CAAC,QAAS,CAAC,mBAzxDR,cAA+BrC,EAOlC5H,YAAYrE,EAAQoB,EAAwBY,GACxCsC,MAAMtE,EAAQoB,GACdmD,KAAKvC,kBAAoBA,EAEzBuC,KAAKqG,mBAAqBrG,KAAKvE,OAAOsQ,eACtC/L,KAAKkG,kBAAoBlG,KAAKvE,OAAOuQ,wBACrChM,KAAKmG,eAAiBnG,KAAKvE,OAAOqO,QAAU9J,KAAKkG,kBAEjDlG,KAAK2I,mBAAqB3I,KAAKvE,OAAOwQ,eACtCjM,KAAK+F,kBAAoB/F,KAAKvE,OAAOyQ,wBACrClM,KAAKgG,eAAiBhG,KAAKvE,OAAOqO,QAAU9J,KAAK+F,iBACrD,OA0wDE2K,GAAoC,IAAI3Y,IAAI,CAC9C,CAAC,OAAQ,CAAC,kBAjhFP,cAA8B8O,EAOjC3O,YAAYiE,GACR,OAAO,IAAIwU,SAAqB5Q,MAAMkL,MAAM9O,GAChD,KAygFA,CAAC,YAAa,CAAC,uBA58EZ,cAAmC2K,EAOtC5O,YAAYiE,GACR,OAAO,IAAIwU,SAAqB5Q,MAAMkL,MAAM9O,GAChD,KAo8EA,CAAC,UAAW,CAAC,qBAv4EV,cAAiC4K,EAOpC7O,YAAYiE,GACR,OAAO,IAAIwU,SAAqB5Q,MAAMkL,MAAM9O,GAChD,KA+3EA,CAAC,aAAc,CAAC,uBAj0Eb,cAAmC6K,EAOtC9O,YAAYiE,GACR,OAAO,IAAIwU,SAAqB5Q,MAAMkL,MAAM9O,GAChD,KAyzEA,CAAC,QAAS,CAAC,mBApoER,cAA+BgL,EAOlCjP,YAAYiE,GACR,OAAO,IAAIwU,SAAqB5Q,MAAMkL,MAAM9O,GAChD,KA4nEA,CAAC,SAAU,CAAC,oBAxgET,cAAgCkL,EAOnCnP,YAAYiE,GACR,OAAO,IAAIwU,SAAqB5Q,MAAMkL,MAAM9O,GAChD,KAggEA,CAAC,aAAc,CAAC,wBAntEb,cAAoC8K,EAOvC/O,YAAYiE,GACR,OAAO,IAAIwU,SAAqB5Q,MAAMkL,MAAM9O,GAChD,KA2sEA,CAAC,UAAW,CAAC,qBAzrDV,cAAiC0L,EAOpC3P,YAAYiE,GACR,OAAO,IAAIwU,SAAqB5Q,MAAMkL,MAAM9O,GAChD,KAirDA,CAAC,MAAO,CAAC,qBAhnDN,cAAiC2L,GAOpC5P,YAAYiE,GACR,OAAO,IAAIwU,SAAqB5Q,MAAMkL,MAAM9O,GAChD,KAwmDA,CAAC,cAAe,CAAC,wBA/iDd,cAAoC4L,GAOvC7P,YAAYiE,GACR,OAAO,IAAIwU,SAAqB5Q,MAAMkL,MAAM9O,GAChD,KAuiDA,CAAC,aAAc,CAAC,wBAjsEb,cAAoC+K,EAOvChP,YAAYiE,GACR,OAAO,IAAIwU,SAAqB5Q,MAAMkL,MAAM9O,GAChD,KAyrEA,CAAC,cAAe,CAAC,yBA5kEd,cAAqCiL,EAOxClP,YAAYiE,GACR,OAAO,IAAIwU,SAAqB5Q,MAAMkL,MAAM9O,GAChD,OAskEEyU,GAA6C,IAAI7Y,IAAI,CACvD,CAAC,OAAQ,CAAC,2BAn/EP,cAAuC8O,EAO1C3O,YAAYiE,GACR,OAAO,IAAI0U,SAAmC9Q,MAAMkL,MAAM9O,GAC9D,KA2+EA,CAAC,YAAa,CAAC,gCA96EZ,cAA4C2K,EAO/C5O,YAAYiE,GACR,OAAO,IAAI0U,SAAmC9Q,MAAMkL,MAAM9O,GAC9D,KAs6EA,CAAC,UAAW,CAAC,8BAx2EV,cAA0C4K,EAO7C7O,YAAYiE,GACR,OAAO,IAAI0U,SAAmC9Q,MAAMkL,MAAM9O,GAC9D,KAg2EA,CAAC,aAAc,CAAC,gCAlyEb,cAA4C6K,EAO/C9O,YAAYiE,GACR,OAAO,IAAI0U,SAAmC9Q,MAAMkL,MAAM9O,GAC9D,KA0xEA,CAAC,QAAS,CAAC,4BAtmER,cAAwCgL,EAO3CjP,YAAYiE,GACR,OAAO,IAAI0U,SAAmC9Q,MAAMkL,MAAM9O,GAC9D,KA8lEA,CAAC,SAAU,CAAC,6BAliET,cAAyCkL,EAO5CnP,YAAYiE,GACR,OAAO,IAAI0U,SAAmC9Q,MAAMkL,MAAM9O,GAC9D,KA0hEA,CAAC,aAAc,CAAC,iCAjvEb,cAA6C8K,EAOhD/O,YAAYiE,GACR,OAAO,IAAI0U,SAAmC9Q,MAAMkL,MAAM9O,GAC9D,KAyuEA,CAAC,UAAW,CAAC,8BA3pDV,cAA0C0L,EAO7C3P,YAAYiE,GACR,OAAO,IAAI0U,SAAmC9Q,MAAMkL,MAAM9O,GAC9D,KAmpDA,CAAC,MAAO,CAAC,0BAllDN,cAAsC2L,GAOzC5P,YAAYiE,GACR,OAAO,IAAI0U,SAAmC9Q,MAAMkL,MAAM9O,GAC9D,KA0kDA,CAAC,cAAe,CAAC,iCAjhDd,cAA6C4L,GAOhD7P,YAAYiE,GACR,OAAO,IAAI0U,SAAmC9Q,MAAMkL,MAAM9O,GAC9D,KAygDA,CAAC,aAAc,CAAC,iCAlrEb,cAA6C+K,EAOhDhP,YAAYiE,GACR,OAAO,IAAI0U,SAAmC9Q,MAAMkL,MAAM9O,GAC9D,KA0qEA,CAAC,cAAe,CAAC,kCArkEd,cAA8CiL,EAOjDlP,YAAYiE,GACR,OAAO,IAAI0U,SAAmC9Q,MAAMkL,MAAM9O,GAC9D,OA+jEE2G,GAAuC,IAAI/K,IAAI,CACjD,CAAC,yBAA0B,CAAC,4BAA6BkQ,OAGvD6I,GAAsD,IAAI/Y,IAAI,CAChE,CAAC,yBAA0B,CAAC,4BAA6BkQ,OAGvD8I,GAA+C,IAAIhZ,IAAI,CACzD,CAAC,MAAO,CAAC,4BA10BN,cAAwCiS,GAI3C9R,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KAq0BA,CAAC,YAAa,CAAC,kCA9zBZ,cAA8C8N,GAIjD/R,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KAyzBA,CAAC,OAAQ,CAAC,6BA/yBP,cAAyC+N,GAI5ChS,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KA0yBA,CAAC,OAAQ,CAAC,6BA5uBP,cAAyCqO,GAI5CtS,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KAuuBA,CAAC,SAAU,CAAC,+BAptBT,cAA2CsO,GAI9CvS,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KA+sBA,CAAC,OAAQ,CAAC,6BAvsBP,cAAyCuO,GAI5CxS,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,OAosBE6U,GAA2C,IAAIjZ,IAAI,CACrD,CAAC,OAAQ,CAAC,yBAxyBP,cAAqCoS,GAIxCjS,YAAYiE,GACR,OAAO,IAAIiO,SAAgCrK,MAAMkL,MAAM9O,GAC3D,KAmyBA,CAAC,QAAS,CAAC,0BA/mBR,cAAsCyO,GAIzC1S,YAAYiE,GACR,OAAO,IAAI0O,SAAiC9K,MAAMkL,MAAM9O,GAC5D,OA4mBE8U,GAA6C,IAAIlZ,IAAI,CACvD,CAAC,OAAQ,CAAC,sBApyBP,cAAkCoS,GAMrCjS,YAAYiE,GACR,OAAO,IAAImO,SAA6BvK,MAAMkL,MAAM9O,GACxD,OA+xBE+U,GAA0C,IAAInZ,IAAI,CACpD,CAAC,MAAO,CAAC,WAAYgT,OAGnBoG,GAA8B,IAAIpZ,IAAI,CACxC,CAAC,WAAY,CAAC,iBAleX,cAA6BsT,GAMhCnT,YAAYiE,GACR,OAAO,IAAIiV,SAAqBrR,MAAMkL,MAAM9O,GAChD,KA2dA,CAAC,QAAS,CAAC,cAxaR,cAA0BmP,GAM7BpT,YAAYiE,GACR,OAAO,IAAIiV,SAAqBrR,MAAMkL,MAAM9O,GAChD,OAmaEkV,GAA+C,IAAItZ,IAAI,CACzD,CAAC,WAAY,CAAC,oCA5dX,cAAgDsT,GAMnDnT,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,KAqdA,CAAC,QAAS,CAAC,iCA/ZR,cAA6CmP,GAMhDpT,YAAYiE,GACR,OAAO,IAAImU,SAA+BvQ,MAAMkL,MAAM9O,GAC1D,OA2ZEmV,GAA2B,CAC7B,CAAC/I,GAAkC1Q,GACnC,CAAC2Q,GAAqC3Q,GACtC,CAACiU,GAAkCjU,GACnC,CAACwY,GAAiDxY,GAClD,CAAC0Y,GAA8C1Y,GAC/C,CAAC+K,GAA8C/K,GAC/C,CAACgL,GAA0ChL,GAC3C,CAAC8K,GAAkC9K,GACnC,CAAC6Y,GAAmC7Y,GACpC,CAAC+Y,GAA4C/Y,GAC7C,CAACiL,GAAsCjL,GACvC,CAACkZ,GAA8ClZ,GAC/C,CAACoZ,GAA4CpZ,GAC7C,CAACmZ,GAA0CnZ,GAC3C,CAACqZ,GAAyCrZ,GAC1C,CAACsZ,GAA6BtZ,GAC9B,CAACwZ,GAA8CxZ,GAC/C,CAACgX,GAA6ChX,IAGlD,IAAK,MAAO0Z,GAAUC,MAASF,GAE3B,IAAK,MAAOvO,EAAM0O,KAAUF,GAASpM,SACjCrN,EAAmBiW,IAAIhL,EAAMyO,IAC7BvZ,EAA4B8V,IAAI0D,EAAO1O,GACvC/K,EAA4B+V,IAAIhL,EAAM0O,GAI9C,MAAMC,GAAiB,CACnB,CAAC,8BAjvCE,cAA0C9I,GAG7C7H,6BAA6B3I,EAA+BE,EAAU,CAAC,GAGnE,OADAA,EAAQ8I,kBAAoB,aACrBrB,MAAM6L,gBAAgBxT,EAA+BE,EAChE,GA0uC6DT,GAC7D,CAAC,gCA9sCE,cAA4C+Q,GAE/C7H,6BAA6B3I,EAA+BE,EAAU,CAAC,GAGnE,OADAA,EAAQ8I,kBAAoB,eACrBrB,MAAM6L,gBAAgBxT,EAA+BE,EAChE,GAwsCiET,IAErE,IAAK,MAAOkL,GAAM0O,GAAOD,MAASE,GAC9B5Z,EAAmBiW,IAAIhL,GAAMyO,IAC7BvZ,EAA4B8V,IAAI0D,GAAO1O,IACvC/K,EAA4B+V,IAAIhL,GAAM0O,IAWnC,MAAME,WAAkBnG,GAC3BzK,4BAA8B,CAACwH,GAAkCC,GAAqCsD,IACtG/K,qBAAsB,EAUnB,MAAM6Q,WAA2CpG,GACpDzK,4BAA8B,CAACsP,IAU5B,MAAMwB,WAAwCrG,GACjDzK,4BAA8B,CAACwP,IAU5B,MAAMuB,WAA8BtG,GACvCzK,4BAA8B,CAAC6B,IAU5B,MAAMmP,WAAkCvG,GAC3CzK,4BAA8B,CAAC8B,IAU5B,MAAMmP,WAAsCxG,GAC/CzK,4BAA8B,CAAC8N,IAU5B,MAAMoD,WAA6BzG,GACtCzK,4BAA8B,CAAC4B,IAU5B,MAAMuP,WAA6B1G,GACtCzK,4BAA8B,CAAC2P,IAU5B,MAAMyB,WAAsC3G,GAC/CzK,4BAA8B,CAAC6P,IAU5B,MAAMwB,WAA+B5G,GACxCzK,4BAA8B,CAAC+B,IAU5B,MAAMuP,WAAwC7G,GACjDzK,4BAA8B,CAACgQ,IAU5B,MAAMuB,WAAsC9G,GAC/CzK,4BAA8B,CAACkQ,IAU5B,MAAMsB,WAAoC/G,GAC7CzK,4BAA8B,CAACiQ,IAc5B,MAAMwB,WAAwBhH,GACjCzK,4BAA8B,CAACoQ,IAG5B,MAAMsB,WAAwCjH,GACjDzK,4BAA8B,CAACsQ,IAG5B,MAAMqB,WAA8ClH,GACvDzK,4BAA8B,CAAC+P,IAM5B,MAAMxT,WAAwBsJ,EASjC9G,aAAY,OAAE5C,EAAM,gBAAEb,EAAe,gBAAED,EAAe,mBAAE4I,EAAqB,KAAI,iBAAEC,EAAmB,OAClGlF,QACAC,KAAK9C,OAASA,EACd8C,KAAK3D,gBAAkBA,EACvB2D,KAAK5D,gBAAkBA,EACvB4D,KAAKgF,mBAAqBA,EAC1BhF,KAAKiF,iBAAmBA,CAC5B,EAMG,MAAMqL,WAAiC1J,EAK1C9G,aAAY,OAAE5C,IACV6C,QACAC,KAAK9C,OAASA,CAClB,EAMG,MAAMsT,WAA8B5J,EAKvC9G,aAAY,OAAE5C,IACV6C,QACAC,KAAK9C,OAASA,CAClB,EAMG,MAAMyT,WAAuB/J,EAKhC9G,aAAY,OAAE5C,IACV6C,QACAC,KAAK9C,OAASA,CAClB,EAMG,MAAM2T,WAAqCjK,EAM9C9G,aAAY,aAAE6S,EAAY,WAAEC,IACxB7S,QACAC,KAAK2S,aAAeA,EACpB3S,KAAK4S,WAAaA,CACtB,EAOG,MAAMxB,WAAuBxK,EAKhC9G,aAAY,OAAE5C,IACV6C,QACAC,KAAK9C,OAASA,CAClB,E","sources":["webpack://vad-var-site/./node_modules/@xenova/transformers/src/models.js"],"sourcesContent":["\n/**\n * @file Definitions of all models available in Transformers.js.\n * \n * **Example:** Load and run an `AutoModel`.\n * \n * ```javascript\n * import { AutoModel, AutoTokenizer } from '@xenova/transformers';\n *\n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/bert-base-uncased');\n * let model = await AutoModel.from_pretrained('Xenova/bert-base-uncased');\n *\n * let inputs = await tokenizer('I love transformers!');\n * let { logits } = await model(inputs);\n * // Tensor {\n * //     data: Float32Array(183132) [-7.117443084716797, -7.107812881469727, -7.092104911804199, ...]\n * //     dims: (3) [1, 6, 30522],\n * //     type: \"float32\",\n * //     size: 183132,\n * // }\n * ```\n * \n * We also provide other `AutoModel`s (listed below), which you can use in the same way as the Python library. For example:\n * \n * **Example:** Load and run a `AutoModelForSeq2SeqLM`.\n * ```javascript\n * import { AutoModelForSeq2SeqLM, AutoTokenizer } from '@xenova/transformers';\n * \n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/t5-small');\n * let model = await AutoModelForSeq2SeqLM.from_pretrained('Xenova/t5-small');\n *\n * let { input_ids } = await tokenizer('translate English to German: I love transformers!');\n * let outputs = await model.generate(input_ids);\n * let decoded = tokenizer.decode(outputs[0], { skip_special_tokens: true });\n * // 'Ich liebe Transformatoren!'\n * ```\n * \n * @module models\n */\n\nimport {\n    AutoConfig,\n} from './configs.js';\n\nimport {\n    Callable,\n    isIntegralNumber,\n    isTypedArray,\n    mergeArrays,\n} from './utils/core.js';\n\nimport {\n    getModelFile,\n    getModelJSON,\n} from './utils/hub.js';\n\nimport {\n    LogitsProcessorList,\n    GenerationConfig,\n    ForceTokensLogitsProcessor,\n    ForcedBOSTokenLogitsProcessor,\n    ForcedEOSTokenLogitsProcessor,\n    SuppressTokensAtBeginLogitsProcessor,\n    WhisperTimeStampLogitsProcessor,\n    NoRepeatNGramLogitsProcessor,\n    RepetitionPenaltyLogitsProcessor,\n    MinLengthLogitsProcessor,\n    MinNewTokensLengthLogitsProcessor,\n\n    Sampler,\n} from './utils/generation.js';\n\nimport {\n    cat,\n    dynamicTimeWarping,\n    mean,\n    ones_like,\n    stack,\n    std_mean,\n    Tensor,\n} from './utils/tensor.js';\n\nimport { executionProviders, ONNX } from './backends/onnx.js';\nimport { medianFilter } from './transformers.js';\nconst { InferenceSession, Tensor: ONNXTensor } = ONNX;\n\n//////////////////////////////////////////////////\n// Model types: used internally\nconst MODEL_TYPES = {\n    EncoderOnly: 0,\n    EncoderDecoder: 1,\n    Seq2Seq: 2,\n    Vision2Seq: 3,\n    DecoderOnly: 4,\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Helper functions\n\n// NOTE: These will be populated fully later\nconst MODEL_TYPE_MAPPING = new Map();\nconst MODEL_NAME_TO_CLASS_MAPPING = new Map();\nconst MODEL_CLASS_TO_NAME_MAPPING = new Map();\n\n\n/**\n * Constructs an InferenceSession using a model file located at the specified path.\n * @param {string} pretrained_model_name_or_path The path to the directory containing the model file.\n * @param {string} fileName The name of the model file.\n * @param {import('./utils/hub.js').PretrainedOptions} options Additional options for loading the model.\n * @returns {Promise<InferenceSession>} A Promise that resolves to an InferenceSession object.\n * @private\n */\nasync function constructSession(pretrained_model_name_or_path, fileName, options) {\n    // TODO add option for user to force specify their desired execution provider\n    let modelFileName = `onnx/${fileName}${options.quantized ? '_quantized' : ''}.onnx`;\n    let buffer = await getModelFile(pretrained_model_name_or_path, modelFileName, true, options);\n\n    try {\n        return await InferenceSession.create(buffer, {\n            executionProviders,\n        });\n    } catch (err) {\n        // If the execution provided was only wasm, throw the error\n        if (executionProviders.length === 1 && executionProviders[0] === 'wasm') {\n            throw err;\n        }\n\n        console.warn(err);\n        console.warn(\n            'Something went wrong during model construction (most likely a missing operation). ' +\n            'Using `wasm` as a fallback. '\n        )\n        return await InferenceSession.create(buffer, {\n            executionProviders: ['wasm']\n        });\n    }\n}\n\n/**\n * Validate model inputs\n * @param {InferenceSession} session The InferenceSession object that will be run.\n * @param {Object} inputs The inputs to check.\n * @returns {Promise<Object>} A Promise that resolves to the checked inputs.\n * @throws {Error} If any inputs are missing.\n * @private\n */\nasync function validateInputs(session, inputs) {\n    // NOTE: Only create a shallow copy\n    const checkedInputs = {};\n    const missingInputs = [];\n    for (let inputName of session.inputNames) {\n        if (inputs[inputName] === undefined) {\n            missingInputs.push(inputName);\n        } else {\n            checkedInputs[inputName] = inputs[inputName];\n        }\n    }\n    if (missingInputs.length > 0) {\n        throw new Error(\n            `An error occurred during model execution: \"Missing the following inputs: ${missingInputs.join(', ')}.`);\n    }\n\n    const numInputsProvided = Object.keys(inputs).length;\n    const numInputsNeeded = session.inputNames.length;\n    if (numInputsProvided > numInputsNeeded) {\n        // No missing inputs, but too many inputs were provided.\n        // Warn the user and ignore the extra inputs.\n        let ignored = Object.keys(inputs).filter(inputName => !session.inputNames.includes(inputName));\n        console.warn(`WARNING: Too many inputs were provided (${numInputsProvided} > ${numInputsNeeded}). The following inputs will be ignored: \"${ignored.join(', ')}\".`);\n    }\n\n    return checkedInputs;\n}\n\n/**\n * Executes an InferenceSession using the specified inputs.\n * NOTE: `inputs` must contain at least the input names of the model.\n *  - If additional inputs are passed, they will be ignored.\n *  - If inputs are missing, an error will be thrown.\n * \n * @param {InferenceSession} session The InferenceSession object to run.\n * @param {Object} inputs An object that maps input names to input tensors.\n * @returns {Promise<Object>} A Promise that resolves to an object that maps output names to output tensors.\n * @private\n */\nasync function sessionRun(session, inputs) {\n    const checkedInputs = await validateInputs(session, inputs);\n    try {\n        let output = await session.run(checkedInputs);\n        output = replaceTensors(output);\n        return output;\n    } catch (e) {\n        // This usually occurs when the inputs are of the wrong type.\n        console.error(`An error occurred during model execution: \"${e}\".`);\n        console.error('Inputs given to model:', checkedInputs);\n        throw e;\n    }\n}\n\n/**\n * Replaces ONNX Tensor objects with custom Tensor objects to support additional functions.\n * @param {Object} obj The object to replace tensor objects in.\n * @returns {Object} The object with tensor objects replaced by custom Tensor objects.\n * @private\n */\nfunction replaceTensors(obj) {\n    for (let prop in obj) {\n        if (obj[prop] instanceof ONNXTensor) {\n            obj[prop] = new Tensor(obj[prop]);\n        } else if (typeof obj[prop] === 'object') {\n            replaceTensors(obj[prop]);\n        }\n    }\n    return obj;\n}\n\n\n/**\n * Converts an array or Tensor of integers to an int64 Tensor.\n * @param {Array|Tensor} items The input integers to be converted.\n * @returns {Tensor} The int64 Tensor with the converted values.\n * @throws {Error} If the input array is empty or the input is a batched Tensor and not all sequences have the same length.\n * @private\n */\nfunction toI64Tensor(items) {\n    if (items instanceof Tensor) {\n        return items;\n    }\n    // items is an array\n    if (items.length === 0) {\n        throw Error(\"items must be non-empty\");\n    }\n\n    if (Array.isArray(items[0])) {\n        // batched\n        if (items.some(x => x.length !== items[0].length)) {\n            throw Error(\"Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' and/or 'truncation=True' to have batched tensors with the same length.\")\n        }\n\n        return new Tensor('int64',\n            BigInt64Array.from(items.flat().map(x => BigInt(x))),\n            [items.length, items[0].length]\n        );\n    } else {\n        //flat\n        return new Tensor('int64',\n            BigInt64Array.from(items.map(x => BigInt(x))),\n            [1, items.length]\n        );\n    }\n}\n\n/**\n * Prepares an attention mask for a sequence of tokens based on configuration options.\n * @param {Object} self The calling object instance.\n * @param {Tensor} tokens The input tokens.\n * @returns {Tensor} The attention mask tensor.\n * @private\n */\nfunction prepareAttentionMask(self, tokens) {\n\n    // Prepare attention mask\n    let pad_token_id = self.config.pad_token_id ?? null;\n    let eos_token_id = self.config.eos_token_id ?? null;\n    if (isIntegralNumber(eos_token_id)) {\n        eos_token_id = [eos_token_id];\n    }\n\n    let is_pad_token_in_inputs = tokens.indexOf(pad_token_id) !== -1;\n    let is_pad_token_not_equal_to_eos_token_id = (eos_token_id === null) || !eos_token_id.includes(pad_token_id)\n\n    if (is_pad_token_in_inputs && is_pad_token_not_equal_to_eos_token_id) {\n        let data = BigInt64Array.from(\n            // Note: != so that int matches bigint\n            tokens.data.map(x => x != pad_token_id)\n        )\n        return new Tensor('int64', data, tokens.dims)\n    } else {\n        return ones_like(tokens);\n    }\n}\n\n/**\n * Creates a boolean tensor with a single value.\n * @param {boolean} value The value of the tensor.\n * @returns {Tensor} The boolean tensor.\n * @private\n */\nfunction boolTensor(value) {\n    return new Tensor('bool', [value], [1]);\n}\n\n// JS doesn't support mixins, so we define some reused functions here, and allow \"this\" to be passed in\n/**\n * Perform forward pass on the seq2seq model (both encoder and decoder).\n * @param {Object} self The seq2seq model object.\n * @param {Object} model_inputs The input object for the model containing encoder and decoder inputs.\n * @returns {Promise<Seq2SeqLMOutput>} Promise that resolves with the output of the seq2seq model.\n * @private\n */\nasync function seq2seqForward(self, model_inputs) {\n\n    let { encoder_outputs, past_key_values } = model_inputs;\n\n    if (!encoder_outputs) {\n        // Encoder outputs are not given, so we must compute them.\n        encoder_outputs = (await encoderForward(self, model_inputs)).last_hidden_state;\n    }\n    let decoderFeeds = {\n        input_ids: model_inputs.decoder_input_ids,\n        encoder_hidden_states: encoder_outputs,\n        use_cache_branch: boolTensor(!!past_key_values)\n    };\n\n    if (self.decoder_merged_session.inputNames.includes('encoder_attention_mask')) {\n        decoderFeeds.encoder_attention_mask = model_inputs.attention_mask\n    }\n    self.addPastKeyValues(decoderFeeds, past_key_values);\n\n    const decoderResults = await sessionRun(self.decoder_merged_session, decoderFeeds);\n    let logits = decoderResults.logits;\n    past_key_values = self.getPastKeyValues(decoderResults, past_key_values);\n\n    // Get cross attention and/or decoder attentions if they are present\n    const attns = self.getAttentions(decoderResults);\n\n    return new Seq2SeqLMOutput({ logits, past_key_values, encoder_outputs, ...attns });\n}\n\n/**\n * Start the beam search process for the seq2seq model.\n * @param {PreTrainedModel} self The seq2seq model object.\n * @param {Tensor} inputTokenIds Array of input token ids for each input sequence.\n * @param {Object} generation_config The generation config.\n * @param {number} numOutputTokens The maximum number of output tokens for the model.\n * @returns {Object[]} Array of beam search objects.\n * @private\n */\nfunction seq2seqStartBeams(self, inputTokenIds, generation_config, numOutputTokens) {\n    let beams = [];\n    let beamId = 0;\n\n    // @ts-ignore\n    const requires_attention_mask = self.requires_attention_mask ?? true;\n\n    // decoder_input_ids == output_token_ids\n    let decoder_input_ids =\n        generation_config.decoder_input_ids\n        ?? generation_config.decoder_start_token_id\n        ?? generation_config.bos_token_id\n        ?? generation_config.eos_token_id;\n\n    // Support input as tensor or list\n    // TODO support batched decoder_input_ids\n    if (decoder_input_ids instanceof Tensor) {\n        decoder_input_ids = decoder_input_ids.tolist().flat();\n    } else if (!Array.isArray(decoder_input_ids)) {\n        decoder_input_ids = [decoder_input_ids];\n    }\n\n    for (let tokens of inputTokenIds) {\n        // TODO: Improve\n        // Currently, just add back batch dimension.\n        // In future, allow for true parallel execution\n        tokens.dims = [1, ...tokens.dims]\n\n        // Create beam\n        let start = {\n            inputs: tokens,\n            encoder_outputs: null,\n            prev_model_outputs: null,\n\n            output_token_ids: decoder_input_ids,\n            done: false,\n            score: 0,\n            id: beamId++ // assign unique id to beams\n        }\n\n        if (requires_attention_mask) {\n            start.attention_mask = prepareAttentionMask(self, tokens);\n        }\n\n        beams.push(start);\n    }\n\n    return beams;\n}\n\n/**\n * Run beam search on the seq2seq model for a single beam.\n * @param {PreTrainedModel} self The seq2seq model object.\n * @param {Object} beam The beam search object for which to run the model.\n * @param {Object} options options\n * @param {string} [options.input_name='input_ids'] The name of the input tensor for the encoder.\n * @returns {Promise<Object>} Promise that resolves with the output of the seq2seq model for the given beam.\n * @private\n */\nasync function seq2seqRunBeam(self, beam) {\n    const input_name = self.main_input_name;\n\n    let decoder_input_ids = beam.output_token_ids;\n    if (beam.prev_model_outputs) {\n        // After the first step, `prev_model_outputs` won't be null.\n        // So, we cut decoder_input_ids if past is used\n        decoder_input_ids = decoder_input_ids.slice(-1);\n    }\n\n    // 1. Prepare\n    let model_inputs = {\n        [input_name]: beam.inputs,\n        decoder_input_ids: toI64Tensor(decoder_input_ids),\n        encoder_outputs: beam.encoder_outputs,\n        past_key_values: beam.prev_model_outputs?.past_key_values,\n    }\n    if (beam.attention_mask) {\n        model_inputs.attention_mask = beam.attention_mask\n    }\n\n    // 2. Run\n    let output = await self.forward(model_inputs);\n\n    // 3. Update\n    beam.prev_model_outputs = output;\n    beam.encoder_outputs = output.encoder_outputs;\n\n    return output;\n}\n\n/**\n * Update a beam with a new token ID.\n * @param {Object} beam The beam to update.\n * @param {number} newTokenId The new token ID to add to the beam's output.\n * @private\n */\nfunction seq2seqUpdatebeam(beam, newTokenId) {\n    beam.output_token_ids = [...beam.output_token_ids, newTokenId];\n}\n\n/**\n * Forward pass of an encoder model.\n * @param {Object} self The encoder model.\n * @param {Object} model_inputs The input data to be used for the forward pass.\n * @returns {Promise<Object>} Promise that resolves with an object containing the model's outputs.\n * @private\n */\nasync function encoderForward(self, model_inputs) {\n    let encoderFeeds = {};\n    for (let key of self.session.inputNames) {\n        encoderFeeds[key] = model_inputs[key];\n    }\n    return await sessionRun(self.session, encoderFeeds);\n}\n\n\n/**\n * Forward pass of a decoder model.\n * @param {Object} self The decoder model.\n * @param {Object} model_inputs The input data to be used for the forward pass.\n * @returns {Promise<Object>} Promise that resolves with an object containing the logits and past key values.\n * @private\n */\nasync function decoderForward(self, model_inputs) {\n    let { input_ids, past_key_values, attention_mask } = model_inputs;\n    let decoderFeeds = {\n        input_ids: input_ids,\n        attention_mask: attention_mask ?? prepareAttentionMask(self, input_ids),\n        use_cache_branch: boolTensor(!!past_key_values)\n    }\n\n    self.addPastKeyValues(decoderFeeds, past_key_values);\n\n    let decoderResults = await sessionRun(self.session, decoderFeeds);\n\n    let logits = decoderResults.logits;\n\n    past_key_values = self.getPastKeyValues(decoderResults, past_key_values);\n    return { logits, past_key_values };\n}\n\n/**\n * Starts the generation of text by initializing the beams for the given input token IDs.\n * @param {Object} self The text generation model object.\n * @param {Tensor} inputTokenIds An tensor of input token IDs to generate text from.\n * @param {Object} generation_config The generation config.\n * @param {number} numOutputTokens The maximum number of tokens to generate for each beam.\n * @param {Tensor} [inputs_attention_mask] The attention mask tensor for the input token IDs.\n * @returns {Object[]} An array of beams initialized with the given inputs and parameters.\n * @private\n */\nfunction decoderStartBeams(self, inputTokenIds, generation_config, numOutputTokens, inputs_attention_mask) {\n    let beams = [];\n\n    let beamId = 0;\n    for (let tokens of inputTokenIds) {\n        let output_token_ids = tokens.tolist().map(Number);\n\n        // TODO: Improve\n        // Currently, just add back batch dimension.\n        // In future, allow for true parallel execution\n        tokens.dims = [1, ...tokens.dims]\n\n        let attn_mask;\n        if (inputs_attention_mask) {\n            attn_mask = inputs_attention_mask[beamId];\n            attn_mask.dims = [1, ...attn_mask.dims]\n\n        } else {\n            attn_mask = prepareAttentionMask(self, tokens)\n        }\n\n        let start = {\n            input: tokens,\n            model_input_ids: tokens,\n            attention_mask: attn_mask,\n            prev_model_outputs: null,\n\n            output_token_ids: output_token_ids,\n            num_output_tokens: numOutputTokens,\n\n            done: false,\n            score: 0,\n            id: beamId++ // assign unique id to beams\n        }\n\n        beams.push(start);\n    }\n    return beams;\n}\n\n/**\n * Runs a single step of the text generation process for a given beam.\n *\n * @param {Object} self The decoder object.\n * @param {Object} beam The beam to run.\n * @param {Tensor} beam.input The input tensor.\n * @param {Tensor} beam.model_input_ids The input ids to the model.\n * @param {Tensor} beam.attention_mask The attention mask.\n * @param {Object} beam.prev_model_outputs The past key values.\n * @param {number[]} beam.output_token_ids The output token ids.\n * @returns {Promise<Object>} The output of the generation step.\n * @private\n */\nasync function decoderRunBeam(self, beam) {\n    let attnMaskData = new BigInt64Array(beam.output_token_ids.length).fill(1n)\n\n    // 1. Prepare\n    let model_inputs = {\n        input_ids: beam.model_input_ids,\n        attention_mask: new Tensor(\n            'int64',\n            attnMaskData,\n            [1, attnMaskData.length]\n        ),\n        past_key_values: beam.prev_model_outputs?.past_key_values,\n    }\n\n    // 2. Run\n    let output = await self.forward(model_inputs);\n\n    // 3. Update\n    beam.prev_model_outputs = output;\n\n    return output;\n}\n\n/**\n * Update a beam with a new token ID.\n * @param {Object} beam The beam to update.\n * @param {number} newTokenId The new token ID to add to the beam's output.\n * @private\n */\nfunction decoderUpdatebeam(beam, newTokenId) {\n    beam.output_token_ids = [...beam.output_token_ids, newTokenId];\n    beam.model_input_ids = new Tensor('int64', [BigInt(newTokenId)], [1, 1]);\n}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n/**\n * A base class for pre-trained models that provides the model configuration and an ONNX session.\n */\nexport class PreTrainedModel extends Callable {\n    main_input_name = 'input_ids';\n\n    /**\n     * Creates a new instance of the `PreTrainedModel` class.\n     * @param {Object} config The model configuration.\n     * @param {any} session session for the model.\n     */\n    constructor(config, session) {\n        super();\n\n        this.config = config;\n        this.session = session;\n\n        const modelName = MODEL_CLASS_TO_NAME_MAPPING.get(this.constructor);\n        const modelType = MODEL_TYPE_MAPPING.get(modelName);\n\n        this.can_generate = false;\n        this._runBeam = null;\n        this._getStartBeams = null;\n        this._updateBeam = null;\n        this._forward = null;\n        if (modelType === MODEL_TYPES.DecoderOnly) {\n            this.can_generate = true;\n\n            this._runBeam = decoderRunBeam;\n            this._getStartBeams = decoderStartBeams;\n            this._updateBeam = decoderUpdatebeam;\n            this._forward = decoderForward;\n\n        } else if (modelType === MODEL_TYPES.Seq2Seq || modelType === MODEL_TYPES.Vision2Seq) {\n            this.can_generate = true;\n\n            this._runBeam = seq2seqRunBeam;\n            this._getStartBeams = seq2seqStartBeams;\n            this._updateBeam = seq2seqUpdatebeam;\n            this._forward = seq2seqForward;\n\n        } else if (modelType === MODEL_TYPES.EncoderDecoder) {\n            this._forward = encoderForward;\n\n        } else { // should be MODEL_TYPES.EncoderOnly\n            this._forward = encoderForward;\n        }\n    }\n\n    /**\n    * Disposes of all the ONNX sessions that were created during inference.\n    * @returns {Promise<unknown[]>} An array of promises, one for each ONNX session that is being disposed.\n    * @todo Use https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/FinalizationRegistry\n    */\n    async dispose() {\n        let promises = [];\n        for (let key of Object.keys(this)) {\n            let item = this[key];\n            if (item instanceof InferenceSession) {\n                promises.push(item.handler.dispose())\n            }\n        }\n        return await Promise.all(promises);\n    }\n\n    /**\n     * Instantiate one of the model classes of the library from a pretrained model.\n     * \n     * The model class to instantiate is selected based on the `model_type` property of the config object\n     * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)\n     * \n     * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:\n     * - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n     *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n     *   user or organization name, like `dbmdz/bert-base-german-cased`.\n     * - A path to a *directory* containing model weights, e.g., `./my_model_directory/`.\n     * @param {import('./utils/hub.js').PretrainedOptions} options Additional options for loading the model.\n     * \n     * @returns {Promise<PreTrainedModel>} A new instance of the `PreTrainedModel` class.\n     */\n    static async from_pretrained(pretrained_model_name_or_path, {\n        quantized = true,\n        progress_callback = null,\n        config = null,\n        cache_dir = null,\n        local_files_only = false,\n        revision = 'main',\n        model_file_name = null,\n    } = {}) {\n\n        let options = {\n            quantized,\n            progress_callback,\n            config,\n            cache_dir,\n            local_files_only,\n            revision,\n            model_file_name,\n        }\n\n        const modelName = MODEL_CLASS_TO_NAME_MAPPING.get(this);\n        const modelType = MODEL_TYPE_MAPPING.get(modelName);\n\n        let info;\n        if (modelType === MODEL_TYPES.DecoderOnly) {\n            info = await Promise.all([\n                AutoConfig.from_pretrained(pretrained_model_name_or_path, options),\n                constructSession(pretrained_model_name_or_path, options.model_file_name ?? 'decoder_model_merged', options),\n                getModelJSON(pretrained_model_name_or_path, 'generation_config.json', false, options),\n            ]);\n\n        } else if (modelType === MODEL_TYPES.Seq2Seq || modelType === MODEL_TYPES.Vision2Seq) {\n            info = await Promise.all([\n                AutoConfig.from_pretrained(pretrained_model_name_or_path, options),\n                constructSession(pretrained_model_name_or_path, 'encoder_model', options),\n                constructSession(pretrained_model_name_or_path, 'decoder_model_merged', options),\n                getModelJSON(pretrained_model_name_or_path, 'generation_config.json', false, options),\n            ]);\n\n        } else if (modelType === MODEL_TYPES.EncoderDecoder) {\n            info = await Promise.all([\n                AutoConfig.from_pretrained(pretrained_model_name_or_path, options),\n                constructSession(pretrained_model_name_or_path, 'encoder_model', options),\n                constructSession(pretrained_model_name_or_path, 'decoder_model_merged', options),\n            ]);\n\n        } else { // should be MODEL_TYPES.EncoderOnly\n            if (modelType !== MODEL_TYPES.EncoderOnly) {\n                console.warn(`Model type for '${modelName}' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose.`)\n            }\n            info = await Promise.all([\n                AutoConfig.from_pretrained(pretrained_model_name_or_path, options),\n                constructSession(pretrained_model_name_or_path, options.model_file_name ?? 'model', options)\n            ]);\n        }\n\n        // @ts-ignore\n        return new this(...info);\n    }\n\n    /**\n     * Runs the model with the provided inputs\n     * @param {Object} model_inputs Object containing input tensors\n     * @returns {Promise<Object>} Object containing output tensors\n     */\n    async _call(model_inputs) {\n        return await this.forward(model_inputs);\n    }\n\n    /**\n     * Forward method for a pretrained model. If not overridden by a subclass, the correct forward method\n     * will be chosen based on the model type.\n     * @param {Object} model_inputs The input data to the model in the format specified in the ONNX model.\n     * @returns {Promise<Object>} The output data from the model in the format specified in the ONNX model.\n     * @throws {Error} This method must be implemented in subclasses.\n     */\n    async forward(model_inputs) {\n        return await this._forward(this, model_inputs);\n    }\n\n    /**\n     * @param {GenerationConfig} generation_config \n     * @param {number} input_ids_seq_length The starting sequence length for the input ids.\n     * @returns {LogitsProcessorList}\n     * @private\n     */\n    _get_logits_processor(\n        generation_config,\n        input_ids_seq_length,\n        // encoder_input_ids, TODO\n        // prefix_allowed_tokens_fn, TODO\n        logits_processor = null\n    ) {\n        const processors = new LogitsProcessorList();\n\n        // if (generation_config.diversity_penalty !== null && generation_config.diversity_penalty > 0.0) {\n        //     processors.push(new HammingDiversityLogitsProcessor(\n        //         generation_config.diversity_penalty,\n        //         generation_config.num_beams,\n        //         generation_config.num_beam_groups\n        //     ));\n        // }\n\n        // if (generation_config.encoder_repetition_penalty !== null && generation_config.encoder_repetition_penalty !== 1.0) {\n        //     processors.push(new EncoderRepetitionPenaltyLogitsProcessor(\n        //         generation_config.encoder_repetition_penalty,\n        //         encoder_input_ids\n        //     ));\n        // }\n\n        if (generation_config.repetition_penalty !== null && generation_config.repetition_penalty !== 1.0) {\n            processors.push(new RepetitionPenaltyLogitsProcessor(generation_config.repetition_penalty));\n        }\n\n        if (generation_config.no_repeat_ngram_size !== null && generation_config.no_repeat_ngram_size > 0) {\n            processors.push(new NoRepeatNGramLogitsProcessor(generation_config.no_repeat_ngram_size));\n        }\n\n        // if (generation_config.encoder_no_repeat_ngram_size !== null && generation_config.encoder_no_repeat_ngram_size > 0) {\n        //     if (this.config.is_encoder_decoder) {\n        //         processors.push(new EncoderNoRepeatNGramLogitsProcessor(\n        //             generation_config.encoder_no_repeat_ngram_size,\n        //             encoder_input_ids\n        //         ));\n        //     } else {\n        //         throw new Error(\"It's impossible to use `encoder_no_repeat_ngram_size` with decoder-only architecture\");\n        //     }\n        // }\n\n        // if (generation_config.bad_words_ids !== null) {\n        //     processors.push(new NoBadWordsLogitsProcessor(generation_config.bad_words_ids, generation_config.eos_token_id));\n        // }\n\n        if (generation_config.min_length !== null && generation_config.eos_token_id !== null && generation_config.min_length > 0) {\n            processors.push(new MinLengthLogitsProcessor(generation_config.min_length, generation_config.eos_token_id));\n        }\n\n        if (generation_config.min_new_tokens !== null && generation_config.eos_token_id !== null && generation_config.min_new_tokens > 0) {\n            processors.push(new MinNewTokensLengthLogitsProcessor(\n                input_ids_seq_length,\n                generation_config.min_new_tokens,\n                generation_config.eos_token_id\n            ));\n        }\n\n        // if (prefix_allowed_tokens_fn !== null) {\n        //     processors.push(new PrefixConstrainedLogitsProcessor(\n        //         prefix_allowed_tokens_fn,\n        //         generation_config.num_beams / generation_config.num_beam_groups\n        //     ));\n        // }\n\n\n        if (generation_config.forced_bos_token_id !== null) {\n            processors.push(new ForcedBOSTokenLogitsProcessor(generation_config.forced_bos_token_id));\n        }\n\n        if (generation_config.forced_eos_token_id !== null) {\n            processors.push(new ForcedEOSTokenLogitsProcessor(\n                generation_config.max_length,\n                generation_config.forced_eos_token_id\n            ));\n        }\n\n        // if (generation_config.remove_invalid_values === true) {\n        //     processors.push(new InfNanRemoveLogitsProcessor());\n        // }\n\n        // if (generation_config.exponential_decay_length_penalty !== null) {\n        //     processors.push(new ExponentialDecayLengthPenalty(\n        //         generation_config.exponential_decay_length_penalty,\n        //         generation_config.eos_token_id,\n        //         input_ids_seq_length\n        //     ));\n        // }\n\n        // if (generation_config.suppress_tokens !== null) {\n        //     processors.push(new SuppressTokensLogitsProcessor(generation_config.suppress_tokens));\n        // }\n\n        if (generation_config.begin_suppress_tokens !== null) {\n            let begin_index = (input_ids_seq_length > 1 || generation_config.forced_bos_token_id === null)\n                ? input_ids_seq_length\n                : input_ids_seq_length + 1;\n\n            if (generation_config.forced_decoder_ids !== null) {\n                // generation starts after the last token that is forced\n                begin_index += generation_config.forced_decoder_ids[generation_config.forced_decoder_ids.length - 1][0];\n            }\n            processors.push(new SuppressTokensAtBeginLogitsProcessor(generation_config.begin_suppress_tokens, begin_index));\n        }\n\n        if (generation_config.forced_decoder_ids !== null) {\n            processors.push(new ForceTokensLogitsProcessor(generation_config.forced_decoder_ids));\n        }\n\n        if (logits_processor !== null) {\n            processors.extend(logits_processor)\n        }\n\n        // `LogitNormalization` should always be the last logit processor, when present\n        // if (generation_config.renormalize_logits === true) {\n        //     processors.push(new LogitNormalization());\n        // }\n\n        return processors;\n    }\n\n    /**\n     * This function merges multiple generation configs together to form a final generation config to be used by the model for text generation.\n     * It first creates an empty `GenerationConfig` object, then it applies the model's own `generation_config` property to it. Finally, if a `generation_config` object was passed in the arguments, it overwrites the corresponding properties in the final config with those of the passed config object.\n     *\n     * @param {GenerationConfig} generation_config A `GenerationConfig` object containing generation parameters.\n     * @returns {GenerationConfig} The final generation config object to be used by the model for text generation.\n     */\n    _get_generation_config(generation_config) {\n        // Create empty generation config (contains defaults)\n        // We pass `this.config` so that if `eos_token_id` or `bos_token_id` exist in the model's config, we will use them\n        let gen_config = new GenerationConfig(this.config);\n\n        // Apply model's generation config, if it exists\n        if ('generation_config' in this) {\n            Object.assign(gen_config, this.generation_config);\n        }\n\n        // Finally, use any generation config specified by the user\n        // when calling `generate`\n        if (generation_config !== null) {\n            Object.assign(gen_config, generation_config);\n        }\n        return gen_config;\n    }\n\n    /**\n     * @typedef {import('./utils/maths.js').TypedArray} TypedArray\n     */\n\n    /**\n     * @typedef {{ sequences: Tensor, decoder_attentions: Tensor, cross_attentions: Tensor }} EncoderDecoderOutput\n     * @typedef {Object} DecoderOutput\n     * \n     * Generates text based on the given inputs and generation configuration using the model.\n     * @param {Tensor|Array|TypedArray} inputs An array of input token IDs.\n     * @param {Object|GenerationConfig|null} generation_config The generation configuration to use. If null, default configuration will be used.\n     * @param {Object|null} logits_processor An optional logits processor to use. If null, a new LogitsProcessorList instance will be created.\n     * @param {Object} options options\n     * @param {Object} [options.inputs_attention_mask=null] An optional attention mask for the inputs.\n     * @returns {Promise<number[][]|EncoderDecoderOutput|DecoderOutput>} An array of generated output sequences, where each sequence is an array of token IDs.\n     * @throws {Error} Throws an error if the inputs array is empty.\n     */\n    async generate(\n        inputs,\n        generation_config = null,\n        logits_processor = null,\n        {\n            inputs_attention_mask = null\n        } = {},\n    ) {\n        if (!this.can_generate) {\n            const modelName = MODEL_CLASS_TO_NAME_MAPPING.get(this.constructor);\n            let errorMessage = `The current model class (${modelName}) is not compatible with \\`.generate()\\`, as it doesn't have a language model head.`\n\n            const modelType = this.config.model_type;\n            const possibleInfo =\n                MODEL_WITH_LM_HEAD_MAPPING_NAMES.get(modelType)\n                ?? MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES.get(modelType)\n                ?? MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES.get(modelType)\n                // ?? MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES.get(modelType) // TODO\n                ?? MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES.get(modelType);\n\n            if (possibleInfo) {\n                // TODO: support multiple possible classes\n                errorMessage += ` Please use the following class instead: '${possibleInfo[0]}'`;\n            }\n            throw Error(errorMessage);\n        }\n\n        if (!(inputs instanceof Tensor) && !isTypedArray(inputs) && !Array.isArray(inputs)) {\n            throw Error(`\\`inputs\\` must be a Tensor, TypedArray, or Array, but is \"${inputs.constructor.name}\".`);\n        }\n\n        let input_ids_seq_length;\n\n        // Prepare `input_ids` which will be used for auto-regressive generation\n        // TODO: Update to align with HF transformers' implementation\n        if (this.config.is_encoder_decoder) {\n            // Generating from the encoder outputs\n            input_ids_seq_length = 0;\n\n        } else {\n            input_ids_seq_length = inputs instanceof Tensor ? inputs.dims.at(-1) : inputs.length;\n\n            // decoder-only\n            if (input_ids_seq_length === 0) {\n                throw Error(\"Must supply a non-empty array of input token ids.\")\n            }\n        }\n\n        // Update generation config with defaults\n        generation_config = this._get_generation_config(generation_config);\n\n        logits_processor = logits_processor ?? new LogitsProcessorList()\n\n        // Update logits processor\n        logits_processor = this._get_logits_processor(\n            generation_config,\n            input_ids_seq_length,\n            logits_processor\n        )\n\n        /** @type {number[]} */\n        let eos_token_ids = generation_config.eos_token_id;\n        if (eos_token_ids !== null && !Array.isArray(eos_token_ids)) {\n            eos_token_ids = [eos_token_ids];\n        }\n\n        // TODO implement early_stopping\n        // https://huggingface.co/blog/how-to-generate\n\n        let numOutputTokens = 1;\n        const maxOutputTokens = numOutputTokens + (generation_config.max_new_tokens ?? Infinity);\n\n        // Only use max length if max_new_tokens is not provided\n        const useMaxLength = Number.isInteger(generation_config.max_length) && (generation_config.max_new_tokens ?? null) === null;\n        let sampler = Sampler.getSampler(generation_config);\n\n        // @ts-ignore\n        let beams = this.getStartBeams(inputs, generation_config, numOutputTokens, inputs_attention_mask);\n\n        while (beams.some(x => !x.done) && numOutputTokens < maxOutputTokens) {\n            let newest_beams = [];\n            for (let beam of beams) {\n                if (beam.done) {\n                    // Add this beam back into the pool\n                    newest_beams.push(beam);\n                    continue\n                }\n                if (useMaxLength && beam.output_token_ids.length >= generation_config.max_length) {\n                    // Set this beam to done and add it back into the pool\n                    beam.done = true;\n                    newest_beams.push(beam);\n                    continue\n                }\n\n                // @ts-ignore\n                let output = await this.runBeam(beam);\n\n                // add attentions/scores to beam only if user requested\n                if (generation_config.output_attentions) {\n                    this.addAttentionsToBeam(beam, output);\n                }\n                if (generation_config.output_scores) {\n                    // TODO add\n                }\n\n                // Logits are of the form [batch_size, out_seq_length, vocab_size]\n                // In most cases, this will be [batch_size, 1, vocab_size]\n                // So, we select the last token's logits:\n                // (equivalent to `logits = outputs.logits[:, -1, :]`)\n                let logits = output.logits.slice(null, -1, null);\n\n                // Apply logits processor\n                logits_processor(beam.output_token_ids, logits);\n\n                let sampledTokens = sampler(logits);\n                for (let [newTokenId, logProb] of sampledTokens) {\n                    // use previous beam as a starting point\n                    let newBeam = { ...beam };\n\n                    // update new beam\n                    // @ts-ignore\n                    this.updateBeam(newBeam, newTokenId);\n\n                    newBeam.score += logProb;\n\n                    if (eos_token_ids && eos_token_ids.includes(newTokenId)) {\n                        newBeam.done = true;\n                    }\n\n                    newest_beams.push(newBeam);\n                }\n            }\n            ++numOutputTokens;\n\n            // Next, we get the best beams, per ID\n            newest_beams = this.groupBeams(newest_beams).map(\n                group => group\n                    .sort((a, b) => b.score - a.score)      // sort by score\n                    .slice(0, generation_config.num_beams)  // remove outside beam width\n            );\n\n            // Flatten beams\n            beams = newest_beams.flat();\n\n            // Run callback\n            if (generation_config.callback_function) {\n                generation_config.callback_function(beams);\n            }\n        }\n\n        // TODO: Ensure that we can return non-batched outputs\n\n        const groupedBeams = this.groupBeams(beams);\n\n        const getFlattened = (key) => groupedBeams.map(\n            batch => {\n                if (generation_config.num_return_sequences > 1) {\n                    return batch.slice(0, generation_config.num_return_sequences).map(x => x[key]);\n                } else {\n                    return [batch[0][key]];\n                }\n            }\n        ).flat(); // Flatten across batches (depth=1)\n\n        const sequences = getFlattened('output_token_ids'); // [1, seqLength]\n\n        if (generation_config.return_dict_in_generate) {\n            // NOTE: `decoder_attentions` and `cross_attentions` should be:\n            //    list (one element for each generated token)\n            //    of list (one element for each layer of the decoder)\n            //    of torch.FloatTensor of shape (batch_size, num_heads, generated_length, sequence_length)\n            // However, since we are only generating one batch at a time, they are of the form:\n            //   list (batches)\n            //   of list (one element for each generated token)\n            //   of list (one element for each layer of the decoder)\n            //   of torch.FloatTensor of shape (1, num_heads, generated_length, sequence_length)\n            // \n            // TODO: In future (when true parallelism, we should be able to return the correct shape)\n\n            const decoder_attentions = getFlattened('decoder_attentions');\n            const cross_attentions = getFlattened('cross_attentions');\n\n            return {\n                sequences,\n\n                decoder_attentions,\n                cross_attentions,\n            }\n        } else {\n            return sequences;\n        }\n    }\n\n    /**\n     * Helper function to add attentions to beam\n     * @param {Object} beam \n     * @param {Object} output\n     * @private \n     */\n    addAttentionsToBeam(beam, output) {\n        if (this.config.is_encoder_decoder) {\n            if (!output.cross_attentions || output.cross_attentions.length === 0) {\n                throw Error(\n                    \"`output_attentions` is true, but the model did not produce cross-attentions. \" +\n                    \"This is most likely because the model was not exported with `output_attentions=True`.\"\n                )\n            }\n            if (!beam.cross_attentions) {\n                beam.cross_attentions = [];\n            }\n            beam.cross_attentions.push(output.cross_attentions);\n        }\n\n        if (!output.decoder_attentions || output.decoder_attentions.length === 0) {\n            throw Error(\n                \"`output_attentions` is true, but the model did not produce decoder-attentions. \" +\n                \"This is most likely because the model was not exported with `output_attentions=True`.\"\n            )\n        }\n        if (!beam.decoder_attentions) {\n            beam.decoder_attentions = [];\n        }\n        beam.decoder_attentions.push(output.decoder_attentions);\n    }\n\n    /**\n     * Groups an array of beam objects by their ids.\n     *\n     * @param {Array} beams The array of beam objects to group.\n     * @returns {Array} An array of arrays, where each inner array contains beam objects with the same id.\n     */\n    groupBeams(beams) {\n        // Group beams by their ids\n        const groups = Object.create(null);\n        for (const obj of beams) {\n            if (groups[obj.id] === undefined) {\n                groups[obj.id] = [obj];\n            } else {\n                groups[obj.id].push(obj);\n            }\n        }\n\n        return Object.values(groups);\n    }\n\n    /**\n     * Returns an object containing past key values from the given decoder results object.\n     *\n     * @param {Object} decoderResults The decoder results object.\n     * @param {Object} pastKeyValues The previous past key values.\n     * @returns {Object} An object containing past key values.\n     */\n    getPastKeyValues(decoderResults, pastKeyValues) {\n\n        const pkvs = Object.create(null);\n\n        for (const name in decoderResults) {\n            if (name.startsWith('present')) {\n                let newName = name.replace('present', 'past_key_values');\n\n                if (pastKeyValues && name.includes('encoder')) {\n                    // Optimization introduced by optimum to reuse past key values. So, we just replace the constant\n                    // outputs with the previous past key values.\n                    // https://github.com/huggingface/optimum/blob/0bf2c05fb7e1182b52d21b703cfc95fd9e4ea3dc/optimum/onnxruntime/base.py#L677-L704\n                    pkvs[newName] = pastKeyValues[newName];\n                } else {\n                    pkvs[newName] = decoderResults[name];\n                }\n            }\n        }\n        return pkvs;\n    }\n\n    /**\n     * Returns an object containing attentions from the given decoder results object.\n     *\n     * @param {Object} decoderResults The decoder results object.\n     * @returns {Object} An object containing attentions.\n     */\n    getAttentions(decoderResults) {\n        const attns = Object.create(null);\n\n        for (const attnName of ['cross_attentions', 'decoder_attentions']) {\n            const result = [];\n            for (const name in decoderResults) {\n                if (name.startsWith(attnName)) {\n                    const index = name.split('.').pop()\n                    result[index] = decoderResults[name];\n                }\n            }\n            attns[attnName] = result;\n        }\n        return attns;\n    }\n\n    /**\n     * Adds past key values to the decoder feeds object. If pastKeyValues is null, creates new tensors for past key values.\n     *\n     * @param {Object} decoderFeeds The decoder feeds object to add past key values to.\n     * @param {Object} pastKeyValues An object containing past key values.\n     */\n    addPastKeyValues(decoderFeeds, pastKeyValues) {\n        if (pastKeyValues) {\n            Object.assign(decoderFeeds, pastKeyValues)\n        } else {\n            // TODO support batches (i.e., batch_size > 1)\n            // @ts-ignore\n            if (this.config.is_encoder_decoder && (this.add_encoder_pkv ?? true)) {\n                // @ts-ignore\n                let encoder_dims = [1, this.num_encoder_heads, 0, this.encoder_dim_kv];\n                // @ts-ignore\n                let decoder_dims = [1, this.num_decoder_heads, 0, this.decoder_dim_kv];\n                // @ts-ignore\n                for (let i = 0; i < this.num_decoder_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.encoder.key`] = new Tensor('float32', [], encoder_dims)\n                    decoderFeeds[`past_key_values.${i}.encoder.value`] = new Tensor('float32', [], encoder_dims)\n                    decoderFeeds[`past_key_values.${i}.decoder.key`] = new Tensor('float32', [], decoder_dims)\n                    decoderFeeds[`past_key_values.${i}.decoder.value`] = new Tensor('float32', [], decoder_dims)\n                }\n            } else if (this.config.multi_query) { // e.g., for `gpt_bigcode`\n                // @ts-ignore\n                let dims = [1, 0, 2 * this.dim_kv]\n                // @ts-ignore\n                for (let i = 0; i < this.num_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.key_value`] = new Tensor('float32', [], dims)\n                }\n            } else if (this.config.model_type === 'bloom') {\n                // NOTE: Custom implementation for Bloom\n\n                // @ts-ignore\n                let keyDims = [1 * this.num_heads, this.dim_kv, 0] // [batch_size x num_heads,64,past_sequence_length]\n                // @ts-ignore\n                let valueDims = [1 * this.num_heads, 0, this.dim_kv] // [batch_size x num_heads,past_sequence_length,64]\n                // @ts-ignore\n                for (let i = 0; i < this.num_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.key`] = new Tensor('float32', [], keyDims)\n                    decoderFeeds[`past_key_values.${i}.value`] = new Tensor('float32', [], valueDims)\n                }\n            } else { // Decoder-only\n                // @ts-ignore\n                let dims = [1, this.num_heads, 0, this.dim_kv]\n                // @ts-ignore\n                for (let i = 0; i < this.num_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.key`] = new Tensor('float32', [], dims)\n                    decoderFeeds[`past_key_values.${i}.value`] = new Tensor('float32', [], dims)\n                }\n            }\n        }\n    }\n\n    /**\n     * Initializes and returns the beam for text generation task\n     * @param {Tensor} inputTokenIds The input token ids.\n     * @param {Object} generation_config The generation config.\n     * @param {number} numOutputTokens The number of tokens to be generated.\n     * @param {Tensor} inputs_attention_mask Optional input attention mask.\n     * @returns {any} A Beam object representing the initialized beam.\n     * @private\n     */\n    getStartBeams(inputTokenIds, generation_config, numOutputTokens, inputs_attention_mask) {\n        return this._getStartBeams(this, inputTokenIds, generation_config, numOutputTokens, inputs_attention_mask)\n    }\n\n    /**\n     * Runs a single step of the beam search generation algorithm.\n     * @param {any} beam The current beam being generated.\n     * @returns {Promise<any>} The updated beam after a single generation step.\n     * @private\n     */\n    async runBeam(beam) {\n        return await this._runBeam(this, beam);\n    }\n\n    /**\n     * Update a beam with a new token ID.\n     * @param {Object} beam The beam to update.\n     * @param {number} newTokenId The new token ID to add to the beam's output.\n     * @private\n     */\n    updateBeam(beam, newTokenId) {\n        return this._updateBeam(beam, newTokenId);\n    }\n}\n\n//////////////////////////////////////////////////\n// Base model output class\nexport class ModelOutput { }\n\n/**\n * Base class for model's outputs, with potential hidden states and attentions.\n */\nexport class BaseModelOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.last_hidden_state Sequence of hidden-states at the output of the last layer of the model.\n     * @param {Tensor} [output.hidden_states] Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n     * @param {Tensor} [output.attentions] Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n     */\n    constructor({ last_hidden_state, hidden_states = null, attentions = null }) {\n        super();\n        this.last_hidden_state = last_hidden_state;\n        this.hidden_states = hidden_states;\n        this.attentions = attentions;\n    }\n}\n//////////////////////////////////////////////////\n// Bert models\nexport class BertPreTrainedModel extends PreTrainedModel { }\nexport class BertModel extends BertPreTrainedModel { }\n\n/**\n * BertForMaskedLM is a class representing a BERT model for masked language modeling.\n */\nexport class BertForMaskedLM extends BertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * BertForSequenceClassification is a class representing a BERT model for sequence classification.\n */\nexport class BertForSequenceClassification extends BertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * BertForTokenClassification is a class representing a BERT model for token classification.\n */\nexport class BertForTokenClassification extends BertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * BertForQuestionAnswering is a class representing a BERT model for question answering.\n */\nexport class BertForQuestionAnswering extends BertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// CamemBERT models\nexport class CamembertPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class CamembertModel extends CamembertPreTrainedModel { }\n\n/**\n * CamemBERT Model with a `language modeling` head on top.\n */\nexport class CamembertForMaskedLM extends CamembertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.\n */\nexport class CamembertForSequenceClassification extends CamembertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class CamembertForTokenClassification extends CamembertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * CamemBERT Model with a span classification head on top for extractive question-answering tasks\n */\nexport class CamembertForQuestionAnswering extends CamembertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// DeBERTa models\nexport class DebertaPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare DeBERTa Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class DebertaModel extends DebertaPreTrainedModel { }\n\n/**\n * DeBERTa Model with a `language modeling` head on top.\n */\nexport class DebertaForMaskedLM extends DebertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class DebertaForSequenceClassification extends DebertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class DebertaForTokenClassification extends DebertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n * layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n */\nexport class DebertaForQuestionAnswering extends DebertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// DeBERTa-v2 models\nexport class DebertaV2PreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare DeBERTa-V2 Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class DebertaV2Model extends DebertaV2PreTrainedModel { }\n\n/**\n * DeBERTa-V2 Model with a `language modeling` head on top.\n */\nexport class DebertaV2ForMaskedLM extends DebertaV2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa-V2 Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class DebertaV2ForSequenceClassification extends DebertaV2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa-V2 Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class DebertaV2ForTokenClassification extends DebertaV2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa-V2 Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n * layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n */\nexport class DebertaV2ForQuestionAnswering extends DebertaV2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// DistilBert models\nexport class DistilBertPreTrainedModel extends PreTrainedModel { }\nexport class DistilBertModel extends DistilBertPreTrainedModel { }\n\n/**\n * DistilBertForSequenceClassification is a class representing a DistilBERT model for sequence classification.\n */\nexport class DistilBertForSequenceClassification extends DistilBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DistilBertForTokenClassification is a class representing a DistilBERT model for token classification.\n */\nexport class DistilBertForTokenClassification extends DistilBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n\n/**\n * DistilBertForQuestionAnswering is a class representing a DistilBERT model for question answering.\n */\nexport class DistilBertForQuestionAnswering extends DistilBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DistilBertForMaskedLM is a class representing a DistilBERT model for masking task.\n */\nexport class DistilBertForMaskedLM extends DistilBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// MobileBert models\nexport class MobileBertPreTrainedModel extends PreTrainedModel { }\nexport class MobileBertModel extends MobileBertPreTrainedModel { }\n\n/**\n * MobileBertForMaskedLM is a class representing a MobileBERT model for masking task.\n */\nexport class MobileBertForMaskedLM extends MobileBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MobileBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class MobileBertForSequenceClassification extends MobileBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MobileBert Model with a span classification head on top for extractive question-answering tasks\n */\nexport class MobileBertForQuestionAnswering extends MobileBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MPNet models\nexport class MPNetPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare MPNet Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class MPNetModel extends MPNetPreTrainedModel { }\n\n/**\n * MPNetForMaskedLM is a class representing a MPNet model for masked language modeling.\n */\nexport class MPNetForMaskedLM extends MPNetPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MPNetForSequenceClassification is a class representing a MPNet model for sequence classification.\n */\nexport class MPNetForSequenceClassification extends MPNetPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MPNetForTokenClassification is a class representing a MPNet model for token classification.\n */\nexport class MPNetForTokenClassification extends MPNetPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MPNetForQuestionAnswering is a class representing a MPNet model for question answering.\n */\nexport class MPNetForQuestionAnswering extends MPNetPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// SqueezeBert models\nexport class SqueezeBertPreTrainedModel extends PreTrainedModel { }\nexport class SqueezeBertModel extends SqueezeBertPreTrainedModel { }\nexport class SqueezeBertForMaskedLM extends SqueezeBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\nexport class SqueezeBertForSequenceClassification extends SqueezeBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\nexport class SqueezeBertForQuestionAnswering extends SqueezeBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Albert models\nexport class AlbertPreTrainedModel extends PreTrainedModel { }\nexport class AlbertModel extends AlbertPreTrainedModel { }\nexport class AlbertForSequenceClassification extends AlbertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\nexport class AlbertForQuestionAnswering extends AlbertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\nexport class AlbertForMaskedLM extends AlbertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// T5 models\nexport class T5PreTrainedModel extends PreTrainedModel { };\n\nexport class T5Model extends T5PreTrainedModel { }\n\n/**\n * T5Model is a class representing a T5 model for conditional generation.\n */\nexport class T5ForConditionalGeneration extends T5PreTrainedModel {\n\n    /**\n     * Creates a new instance of the `T5ForConditionalGeneration` class.\n     * @param {Object} config The model configuration.\n     * @param {any} session session for the model.\n     * @param {any} decoder_merged_session session for the decoder.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.num_decoder_layers;\n        this.num_decoder_heads = this.config.num_heads;\n        this.decoder_dim_kv = this.config.d_kv;\n\n        this.num_encoder_layers = this.config.num_layers;\n        this.num_encoder_heads = this.config.num_heads;\n        this.encoder_dim_kv = this.config.d_kv;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// LONGT5 models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class LongT5PreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare LONGT5 Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class LongT5Model extends LongT5PreTrainedModel { }\n\n/**\n * LONGT5 Model with a `language modeling` head on top.\n */\nexport class LongT5ForConditionalGeneration extends LongT5PreTrainedModel {\n    /**\n     * Creates a new instance of the `LongT5ForConditionalGeneration` class.\n     * @param {Object} config The model configuration.\n     * @param {any} session session for the model.\n     * @param {any} decoder_merged_session session for the decoder.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.num_decoder_layers;\n        this.num_decoder_heads = this.config.num_heads;\n        this.decoder_dim_kv = this.config.d_kv;\n\n        this.num_encoder_layers = this.config.num_layers;\n        this.num_encoder_heads = this.config.num_heads;\n        this.encoder_dim_kv = this.config.d_kv;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// MT5 models\nexport class MT5PreTrainedModel extends PreTrainedModel { };\n\nexport class MT5Model extends MT5PreTrainedModel { }\n\n/**\n * A class representing a conditional sequence-to-sequence model based on the MT5 architecture.\n */\nexport class MT5ForConditionalGeneration extends MT5PreTrainedModel {\n\n    /**\n     * Creates a new instance of the `MT5ForConditionalGeneration` class.\n     * @param {any} config The model configuration.\n     * @param {any} session The ONNX session containing the encoder weights.\n     * @param {any} decoder_merged_session The ONNX session containing the merged decoder weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.num_decoder_layers;\n        this.num_decoder_heads = this.config.num_heads;\n        this.decoder_dim_kv = this.config.d_kv;\n\n        this.num_encoder_layers = this.config.num_layers;\n        this.num_encoder_heads = this.config.num_heads;\n        this.encoder_dim_kv = this.config.d_kv;\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Bart models\nexport class BartPretrainedModel extends PreTrainedModel { };\n\n/**\n * The bare BART Model outputting raw hidden-states without any specific head on top.\n */\nexport class BartModel extends BartPretrainedModel { }\n\n/**\n * The BART Model with a language modeling head. Can be used for summarization.\n */\nexport class BartForConditionalGeneration extends BartPretrainedModel {\n\n    /**\n     * Creates a new instance of the `BartForConditionalGeneration` class.\n     * @param {Object} config The configuration object for the Bart model.\n     * @param {Object} session The ONNX session used to execute the model.\n     * @param {Object} decoder_merged_session The ONNX session used to execute the decoder.\n     * @param {Object} generation_config The generation configuration object.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n\n}\n\n/**\n * Bart model with a sequence classification/head on top (a linear layer on top of the pooled output)\n */\nexport class BartForSequenceClassification extends BartPretrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MBart models\nexport class MBartPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare MBART Model outputting raw hidden-states without any specific head on top.\n */\nexport class MBartModel extends MBartPreTrainedModel { }\n\n/**\n * The MBART Model with a language modeling head. Can be used for summarization, after fine-tuning the pretrained models.\n */\nexport class MBartForConditionalGeneration extends MBartPreTrainedModel {\n\n    /**\n     * Creates a new instance of the `MBartForConditionalGeneration` class.\n     * @param {Object} config The configuration object for the Bart model.\n     * @param {Object} session The ONNX session used to execute the model.\n     * @param {Object} decoder_merged_session The ONNX session used to execute the decoder.\n     * @param {Object} generation_config The generation configuration object.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n\n}\n\n/**\n * MBart model with a sequence classification/head on top (a linear layer on top of the pooled output).\n */\nexport class MBartForSequenceClassification extends MBartPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n\nexport class MBartForCausalLM extends MBartPreTrainedModel {\n    /**\n     * Creates a new instance of the `MBartForCausalLM` class.\n     * @param {Object} config Configuration object for the model.\n     * @param {Object} decoder_merged_session ONNX Session object for the decoder.\n     * @param {Object} generation_config Configuration object for the generation process.\n     */\n    constructor(config, decoder_merged_session, generation_config) {\n        super(config, decoder_merged_session);\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Blenderbot models\nexport class BlenderbotPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare Blenderbot Model outputting raw hidden-states without any specific head on top.\n */\nexport class BlenderbotModel extends BlenderbotPreTrainedModel { }\n\n/**\n * The Blenderbot Model with a language modeling head. Can be used for summarization.\n */\nexport class BlenderbotForConditionalGeneration extends BlenderbotPreTrainedModel {\n\n    /**\n     * Creates a new instance of the `BlenderbotForConditionalGeneration` class.\n     * @param {any} config The model configuration.\n     * @param {any} session The ONNX session containing the encoder weights.\n     * @param {any} decoder_merged_session The ONNX session containing the merged decoder weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Blenderbot models\nexport class BlenderbotSmallPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare BlenderbotSmall Model outputting raw hidden-states without any specific head on top.\n */\nexport class BlenderbotSmallModel extends BlenderbotSmallPreTrainedModel { }\n\n/**\n * The BlenderbotSmall Model with a language modeling head. Can be used for summarization.\n */\nexport class BlenderbotSmallForConditionalGeneration extends BlenderbotSmallPreTrainedModel {\n\n    /**\n     * Creates a new instance of the `BlenderbotForConditionalGeneration` class.\n     * @param {any} config The model configuration.\n     * @param {any} session The ONNX session containing the encoder weights.\n     * @param {any} decoder_merged_session The ONNX session containing the merged decoder weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Roberta models\nexport class RobertaPreTrainedModel extends PreTrainedModel { }\nexport class RobertaModel extends RobertaPreTrainedModel { }\n\n/**\n * RobertaForMaskedLM class for performing masked language modeling on Roberta models.\n */\nexport class RobertaForMaskedLM extends RobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * RobertaForSequenceClassification class for performing sequence classification on Roberta models.\n */\nexport class RobertaForSequenceClassification extends RobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * RobertaForTokenClassification class for performing token classification on Roberta models.\n */\nexport class RobertaForTokenClassification extends RobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * RobertaForQuestionAnswering class for performing question answering on Roberta models.\n */\nexport class RobertaForQuestionAnswering extends RobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// XLM models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class XLMPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare XLM Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class XLMModel extends XLMPreTrainedModel { }\n\n/**\n * The XLM Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class XLMWithLMHeadModel extends XLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLM Model with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class XLMForSequenceClassification extends XLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLM Model with a token classification head on top (a linear layer on top of the hidden-states output)\n */\nexport class XLMForTokenClassification extends XLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLM Model with a span classification head on top for extractive question-answering tasks\n */\nexport class XLMForQuestionAnswering extends XLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// XLMRoberta models\nexport class XLMRobertaPreTrainedModel extends PreTrainedModel { }\nexport class XLMRobertaModel extends XLMRobertaPreTrainedModel { }\n\n/**\n * XLMRobertaForMaskedLM class for performing masked language modeling on XLMRoberta models.\n */\nexport class XLMRobertaForMaskedLM extends XLMRobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLMRobertaForSequenceClassification class for performing sequence classification on XLMRoberta models.\n */\nexport class XLMRobertaForSequenceClassification extends XLMRobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLMRobertaForTokenClassification class for performing token classification on XLMRoberta models.\n */\nexport class XLMRobertaForTokenClassification extends XLMRobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLMRobertaForQuestionAnswering class for performing question answering on XLMRoberta models.\n */\nexport class XLMRobertaForQuestionAnswering extends XLMRobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Whisper models\nexport class WhisperPreTrainedModel extends PreTrainedModel { };\n\n/**\n * WhisperModel class for training Whisper models without a language model head.\n */\nexport class WhisperModel extends WhisperPreTrainedModel { }\n\n/**\n * WhisperForConditionalGeneration class for generating conditional outputs from Whisper models.\n */\nexport class WhisperForConditionalGeneration extends WhisperPreTrainedModel {\n\n    requires_attention_mask = false;\n    main_input_name = 'input_features';\n\n    /**\n     * Creates a new instance of the `WhisperForConditionalGeneration` class.\n     * @param {Object} config Configuration object for the model.\n     * @param {Object} session ONNX Session object for the model.\n     * @param {Object} decoder_merged_session ONNX Session object for the decoder.\n     * @param {Object} generation_config Configuration object for the generation process.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n\n    /**\n     * @typedef {Object} WhisperGenerationConfig\n     * @extends GenerationConfig\n     * @property {boolean} [return_timestamps=null] Whether to return the timestamps with the text. This enables the `WhisperTimestampsLogitsProcessor`.\n     * @property {boolean} [return_token_timestamps=null] Whether to return token-level timestamps\n     * with the text. This can be used with or without the `return_timestamps` option. To get word-level\n     * timestamps, use the tokenizer to group the tokens into words.\n     * @property {number} [num_frames=null]  The number of audio frames available in this chunk. This is only used generating word-level timestamps.\n     */\n\n    /**\n     * Generates outputs based on input and generation configuration.\n     * @param {Object} inputs Input data for the model.\n     * @param {WhisperGenerationConfig} generation_config Configuration object for the generation process.\n     * @param {Object} logits_processor Optional logits processor object.\n     * @returns {Promise<Object>} Promise object represents the generated outputs.\n     */\n    async generate(\n        inputs,\n        generation_config = null,\n        logits_processor = null,\n        // {\n        //     return_timestamps = null,\n        //     return_token_timestamps = null,\n        //     language = null,\n        //     task = null,\n        // } = {},\n    ) {\n        // Create generation config object\n        generation_config = this._get_generation_config(generation_config);\n\n\n        // Whisper has additional options for returning timestamps\n        generation_config.return_timestamps ??= false;\n\n        // TODO add language and task\n\n        if (generation_config.return_timestamps) {\n            logits_processor = [new WhisperTimeStampLogitsProcessor(generation_config)]\n        }\n\n        if (generation_config.return_token_timestamps) {\n            generation_config.output_attentions = true;\n            generation_config.return_dict_in_generate = true;\n\n            if (generation_config.task === 'translate') {\n                console.warn(\"Token-level timestamps may not be reliable for task 'translate'.\")\n            }\n\n            if (!generation_config.alignment_heads) {\n                throw new Error(\n                    \"Model generation config has no `alignment_heads`, token-level timestamps not available. \" +\n                    \"See https://gist.github.com/hollance/42e32852f24243b748ae6bc1f985b13a on how to add this property to the generation config.\"\n                )\n            }\n        }\n\n        const outputs = await super.generate(inputs, generation_config, logits_processor);\n\n        if (generation_config.return_token_timestamps && generation_config.alignment_heads) {\n            outputs[\"token_timestamps\"] = this._extract_token_timestamps(\n                outputs,\n                generation_config.alignment_heads,\n                generation_config.num_frames,\n            )\n        }\n\n        return outputs\n    }\n\n    /**\n     * Calculates token-level timestamps using the encoder-decoder cross-attentions and\n     * dynamic time-warping (DTW) to map each output token to a position in the input audio.\n     * @param {Object} generate_outputs Outputs generated by the model\n     * @param {Tensor[][][]} generate_outputs.cross_attentions The cross attentions output by the model\n     * @param {Tensor[][][]} generate_outputs.decoder_attentions The decoder attentions output by the model\n     * @param {number[][]} generate_outputs.sequences The sequences output by the model\n     * @param {number[][]} alignment_heads Alignment heads of the model\n     * @param {number} [num_frames=null] Number of frames in the input audio.\n     * @param {number} [time_precision=0.02] Precision of the timestamps in seconds\n     * @returns {Tensor} tensor containing the timestamps in seconds for each predicted token\n     */\n    _extract_token_timestamps(generate_outputs, alignment_heads, num_frames = null, time_precision = 0.02) {\n        if (!generate_outputs.cross_attentions) {\n            throw new Error(\n                \"Model outputs must contain cross attentions to extract timestamps. \" +\n                \"This is most likely because the model was not exported with `output_attentions=True`.\"\n            )\n        }\n\n        let median_filter_width = this.config.median_filter_width;\n        if (median_filter_width === undefined) {\n            console.warn(\"Model config has no `median_filter_width`, using default value of 7.\")\n            median_filter_width = 7;\n        }\n\n        const batchedMatrices = generate_outputs.cross_attentions.map(batch => {\n            // Create a list with `decoder_layers` elements, each a tensor of shape\n            // (batch size, attention_heads, output length, input length).\n            let cross_attentions = Array.from({ length: this.config.decoder_layers },\n                (_, i) => cat(batch.map(x => x[i]), 2)\n            );\n\n            let weights = stack(alignment_heads.map(([l, h]) => {\n                return num_frames\n                    ? cross_attentions[l].slice(null, h, null, [0, num_frames])\n                    : cross_attentions[l].slice(null, h);\n            }));\n            weights = weights.transpose(1, 0, 2, 3)\n\n            let [std, calculatedMean] = std_mean(weights, -2, 0, true);\n\n            // Normalize and smoothen the weights.\n            let smoothedWeights = weights.clone(); // [1, 8, seqLength, 1500]\n\n            for (let a = 0; a < smoothedWeights.dims[0]; ++a) {\n                let aTensor = smoothedWeights[a]; // [8, seqLength, 1500]\n\n                for (let b = 0; b < aTensor.dims[0]; ++b) {\n                    let bTensor = aTensor[b]; // [seqLength, 1500]\n\n                    const stdTensor = std[a][b][0]; // [1500]\n                    const meanTensor = calculatedMean[a][b][0]; // [1500]\n\n                    for (let c = 0; c < bTensor.dims[0]; ++c) {\n\n                        let cTensor = bTensor[c]; // [1500]\n                        for (let d = 0; d < cTensor.data.length; ++d) {\n                            cTensor.data[d] = (cTensor.data[d] - meanTensor.data[d]) / stdTensor.data[d]\n                        }\n\n                        // Apply median filter.\n                        cTensor.data.set(medianFilter(cTensor.data, median_filter_width))\n                    }\n                }\n            }\n\n            // Average the different cross-attention heads.\n            const matrix = mean(smoothedWeights, 1);\n            return matrix;\n        });\n\n        const timestampsShape = [generate_outputs.sequences.length, generate_outputs.sequences[0].length];\n\n        const timestamps = new Tensor(\n            'float32',\n            new Float32Array(timestampsShape[0] * timestampsShape[1]),\n            timestampsShape\n        );\n\n        // Perform dynamic time warping on each element of the batch.\n        for (let batch_idx = 0; batch_idx < timestampsShape[0]; ++batch_idx) {\n            // NOTE: Since we run only one batch at a time, we can squeeze to get the same dimensions\n            // as the python implementation\n            const matrix = batchedMatrices[batch_idx].neg().squeeze_(0);\n            let [text_indices, time_indices] = dynamicTimeWarping(matrix);\n\n            let diffs = Array.from({ length: text_indices.length - 1 }, (v, i) => text_indices[i + 1] - text_indices[i]);\n            let jumps = mergeArrays([1], diffs).map(x => !!x); // convert to boolean\n\n            let jump_times = [];\n            for (let i = 0; i < jumps.length; ++i) {\n                if (jumps[i]) {\n                    jump_times.push(time_indices[i] * time_precision);\n                    // NOTE: No point in rounding here, since we set to Float32Array later\n                }\n            }\n            timestamps[batch_idx].data.set(jump_times, 1)\n        }\n\n        return timestamps;\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n/**\n * Vision Encoder-Decoder model based on OpenAI's GPT architecture for image captioning and other vision tasks\n */\nexport class VisionEncoderDecoderModel extends PreTrainedModel {\n    main_input_name = 'pixel_values';\n\n    /**\n     * Creates a new instance of the `VisionEncoderDecoderModel` class.\n     * @param {Object} config The configuration object specifying the hyperparameters and other model settings.\n     * @param {Object} session The ONNX session containing the encoder model.\n     * @param {any} decoder_merged_session The ONNX session containing the merged decoder model.\n     * @param {Object} generation_config Configuration object for the generation process.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        // Extract configs\n        const encoderConfig = this.config.encoder;\n        const decoderConfig = this.config.decoder;\n\n        // Validate encoder\n        const encoderModelType = encoderConfig.model_type;\n        const encoderModel =\n            MODEL_MAPPING_NAMES_ENCODER_ONLY.get(encoderModelType)\n            ?? MODEL_MAPPING_NAMES_ENCODER_DECODER.get(encoderModelType);\n        if (!encoderModel) {\n            console.warn(`Model type for encoder '${encoderModelType}' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose.`);\n        }\n\n        // Validate decoder\n        const decoderModel = MODEL_WITH_LM_HEAD_MAPPING_NAMES.get(decoderConfig.model_type);\n        if (!decoderModel) {\n            throw new Error(`Unable to construct \\`VisionEncoderDecoder\\` due to unsupported decoder: \"${this.config.decoder.model_type}\"`);\n        }\n\n        // @ts-ignore\n        const decoderModelClass = decoderModel[1];\n        // @ts-ignore\n        const decoder = new decoderModelClass(decoderConfig, decoder_merged_session, generation_config);\n\n        this.add_encoder_pkv = 'num_decoder_layers' in decoder;\n        if (this.add_encoder_pkv) {\n            // Decoder is part of an encoder-decoder model\n            this.num_decoder_layers = decoder.num_decoder_layers;\n            this.num_decoder_heads = decoder.num_decoder_heads;\n            this.decoder_dim_kv = decoder.decoder_dim_kv;\n\n            this.num_encoder_layers = decoder.num_encoder_layers;\n            this.num_encoder_heads = decoder.num_encoder_heads;\n            this.encoder_dim_kv = decoder.encoder_dim_kv;\n\n        } else {\n            // Decoder is a decoder-only model\n            this.num_layers = decoder.num_layers;\n            this.num_heads = decoder.num_heads;\n            this.dim_kv = decoder.dim_kv;\n        }\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// CLIP models\nexport class CLIPPreTrainedModel extends PreTrainedModel { }\n\n/**\n * CLIP Text and Vision Model with a projection layers on top\n * \n * **Example:** Perform zero-shot image classification with a `CLIPModel`.\n * \n * ```javascript\n * import { AutoTokenizer, AutoProcessor, CLIPModel, RawImage } from '@xenova/transformers';\n * \n * // Load tokenizer, processor, and model\n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');\n * let processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');\n * let model = await CLIPModel.from_pretrained('Xenova/clip-vit-base-patch16');\n * \n * // Run tokenization\n * let texts = ['a photo of a car', 'a photo of a football match']\n * let text_inputs = tokenizer(texts, { padding: true, truncation: true });\n * \n * // Read image and run processor\n * let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\n * let image_inputs = await processor(image);\n * \n * // Run model with both text and pixel inputs\n * let output = await model({ ...text_inputs, ...image_inputs });\n * // {\n * //   logits_per_image: Tensor {\n * //     dims: [ 1, 2 ],\n * //     data: Float32Array(2) [ 18.579734802246094, 24.31830596923828 ],\n * //   },\n * //   logits_per_text: Tensor {\n * //     dims: [ 2, 1 ],\n * //     data: Float32Array(2) [ 18.579734802246094, 24.31830596923828 ],\n * //   },\n * //   text_embeds: Tensor {\n * //     dims: [ 2, 512 ],\n * //     data: Float32Array(1024) [ ... ],\n * //   },\n * //   image_embeds: Tensor {\n * //     dims: [ 1, 512 ],\n * //     data: Float32Array(512) [ ... ],\n * //   }\n * // }\n * ```\n */\nexport class CLIPModel extends CLIPPreTrainedModel { }\n\n/**\n * CLIP Text Model with a projection layer on top (a linear layer on top of the pooled output)\n * \n * **Example:** Compute text embeddings with `CLIPTextModelWithProjection`.\n * \n * ```javascript\n * import { AutoTokenizer, CLIPTextModelWithProjection } from '@xenova/transformers';\n * \n * // Load tokenizer and text model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');\n * const text_model = await CLIPTextModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');\n * \n * // Run tokenization\n * let texts = ['a photo of a car', 'a photo of a football match'];\n * let text_inputs = tokenizer(texts, { padding: true, truncation: true });\n * \n * // Compute embeddings\n * const { text_embeds } = await text_model(text_inputs);\n * // Tensor {\n * //   dims: [ 2, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(1024) [ ... ],\n * //   size: 1024\n * // }\n * ```\n */\nexport class CLIPTextModelWithProjection extends CLIPPreTrainedModel {\n\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n        // Update default model file name if not provided\n        options.model_file_name ??= 'text_model';\n        return super.from_pretrained(pretrained_model_name_or_path, options);\n    }\n}\n\n/**\n * CLIP Vision Model with a projection layer on top (a linear layer on top of the pooled output)\n * \n * **Example:** Compute vision embeddings with `CLIPVisionModelWithProjection`.\n * \n * ```javascript\n * import { AutoProcessor, CLIPVisionModelWithProjection, RawImage} from '@xenova/transformers';\n * \n * // Load processor and vision model\n * const processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');\n * const vision_model = await CLIPVisionModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');\n * \n * // Read image and run processor\n * let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\n * let image_inputs = await processor(image);\n * \n * // Compute embeddings\n * const { image_embeds } = await vision_model(image_inputs);\n * // Tensor {\n * //   dims: [ 1, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(512) [ ... ],\n * //   size: 512\n * // }\n * ```\n */\nexport class CLIPVisionModelWithProjection extends CLIPPreTrainedModel {\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n        // Update default model file name if not provided\n        options.model_file_name ??= 'vision_model';\n        return super.from_pretrained(pretrained_model_name_or_path, options);\n    }\n}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// GPT2 models\nexport class GPT2PreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPT2PreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.n_embd / this.num_heads;\n    }\n}\n\nexport class GPT2Model extends GPT2PreTrainedModel { }\n\n/**\n * GPT-2 language model head on top of the GPT-2 base model. This model is suitable for text generation tasks.\n */\nexport class GPT2LMHeadModel extends GPT2PreTrainedModel { }\n// export class GPT2ForSequenceClassification extends GPT2PreTrainedModel {\n// TODO\n// }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// GPTNeo models\nexport class GPTNeoPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPTNeoPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_heads;\n        this.num_layers = this.config.num_layers;\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\nexport class GPTNeoModel extends GPTNeoPreTrainedModel { }\n\nexport class GPTNeoForCausalLM extends GPTNeoPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// GPTNeoX models\nexport class GPTNeoXPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPTNeoXPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_attention_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\nexport class GPTNeoXModel extends GPTNeoXPreTrainedModel { }\n\nexport class GPTNeoXForCausalLM extends GPTNeoXPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// GPT-J models\nexport class GPTJPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPTJPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.n_embd / this.num_heads;\n    }\n}\n\nexport class GPTJModel extends GPTJPreTrainedModel { }\n\nexport class GPTJForCausalLM extends GPTJPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// GPTBigCode models\nexport class GPTBigCodePreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPTBigCodePreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.n_embd / this.num_heads;\n    }\n}\n\nexport class GPTBigCodeModel extends GPTBigCodePreTrainedModel { }\n\nexport class GPTBigCodeForCausalLM extends GPTBigCodePreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// CodeGen models\nexport class CodeGenPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `CodeGenPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.n_embd / this.num_heads;\n    }\n}\n/**\n * CodeGenModel is a class representing a code generation model without a language model head.\n */\nexport class CodeGenModel extends CodeGenPreTrainedModel { }\n\n/**\n * CodeGenForCausalLM is a class that represents a code generation model based on the GPT-2 architecture. It extends the `CodeGenPreTrainedModel` class.\n */\nexport class CodeGenForCausalLM extends CodeGenPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// LLama models\n\n/**\n * The bare LLama Model outputting raw hidden-states without any specific head on top.\n */\nexport class LlamaPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `LlamaPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_attention_heads\n        this.num_layers = this.config.num_hidden_layers\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\n/**\n * The bare LLaMA Model outputting raw hidden-states without any specific head on top.\n */\nexport class LlamaModel extends LlamaPreTrainedModel { }\n\nexport class LlamaForCausalLM extends LlamaPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Bloom models\n/**\n * The Bloom Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class BloomPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `BloomPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\n\n/**\n * The bare Bloom Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class BloomModel extends BloomPreTrainedModel { }\n\n/**\n * The Bloom Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class BloomForCausalLM extends BloomPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MPT models\nexport class MptPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `MptPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_heads\n        this.num_layers = this.config.n_layers\n        this.dim_kv = this.config.d_model / this.num_heads;\n    }\n}\n\n/**\n * The bare Mpt Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class MptModel extends MptPreTrainedModel { }\n\n/**\n * The MPT Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class MptForCausalLM extends MptPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// OPT models\nexport class OPTPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `OPTPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_attention_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\n\n/**\n * The bare OPT Model outputting raw hidden-states without any specific head on top.\n */\nexport class OPTModel extends OPTPreTrainedModel { }\n\n/**\n * The OPT Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class OPTForCausalLM extends OPTPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class ViTPreTrainedModel extends PreTrainedModel { }\nexport class ViTModel extends ViTPreTrainedModel { }\nexport class ViTForImageClassification extends ViTPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class MobileViTPreTrainedModel extends PreTrainedModel { }\nexport class MobileViTModel extends MobileViTPreTrainedModel { }\nexport class MobileViTForImageClassification extends MobileViTPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n// TODO: MobileViTForSemanticSegmentation\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Beit Models\nexport class BeitPreTrainedModel extends PreTrainedModel { }\nexport class BeitModel extends BeitPreTrainedModel { }\nexport class BeitForImageClassification extends BeitPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class DetrPreTrainedModel extends PreTrainedModel { }\nexport class DetrModel extends DetrPreTrainedModel { }\nexport class DetrForObjectDetection extends DetrPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new DetrObjectDetectionOutput(await super._call(model_inputs));\n    }\n}\n\nexport class DetrForSegmentation extends DetrPreTrainedModel {\n    /**\n     * Runs the model with the provided inputs\n     * @param {Object} model_inputs Model inputs\n     * @returns {Promise<DetrSegmentationOutput>} Object containing segmentation outputs\n     */\n    async _call(model_inputs) {\n        return new DetrSegmentationOutput(await super._call(model_inputs));\n    }\n}\n\nexport class DetrObjectDetectionOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Classification logits (including no-object) for all queries.\n     * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).\n     * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).\n     */\n    constructor({ logits, pred_boxes }) {\n        super();\n        this.logits = logits;\n        this.pred_boxes = pred_boxes;\n    }\n}\n\nexport class DetrSegmentationOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits The output logits of the model.\n     * @param {Tensor} output.pred_boxes Predicted boxes.\n     * @param {Tensor} output.pred_masks Predicted masks.\n     */\n    constructor({ logits, pred_boxes, pred_masks }) {\n        super();\n        this.logits = logits;\n        this.pred_boxes = pred_boxes;\n        this.pred_masks = pred_masks;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class DeiTPreTrainedModel extends PreTrainedModel { }\nexport class DeiTModel extends DeiTPreTrainedModel { }\nexport class DeiTForImageClassification extends DeiTPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class ResNetPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare ResNet model outputting raw features without any specific head on top.\n */\nexport class ResNetModel extends ResNetPreTrainedModel { }\n\n/**\n * ResNet Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n */\nexport class ResNetForImageClassification extends ResNetPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class SwinPreTrainedModel extends PreTrainedModel { }\nexport class SwinModel extends SwinPreTrainedModel { }\nexport class SwinForImageClassification extends SwinPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class DonutSwinPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare Donut Swin Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Step-by-step Document Parsing.\n * \n * ```javascript\n * import { AutoProcessor, AutoTokenizer, AutoModelForVision2Seq, RawImage } from '@xenova/transformers';\n * \n * // Choose model to use\n * const model_id = 'Xenova/donut-base-finetuned-cord-v2';\n * \n * // Prepare image inputs\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/receipt.png';\n * const image = await RawImage.read(url);\n * const image_inputs = await processor(image);\n * \n * // Prepare decoder inputs\n * const tokenizer = await AutoTokenizer.from_pretrained(model_id);\n * const task_prompt = '<s_cord-v2>';\n * const decoder_input_ids = tokenizer(task_prompt, {\n *   add_special_tokens: false,\n * }).input_ids;\n * \n * // Create the model\n * const model = await AutoModelForVision2Seq.from_pretrained(model_id);\n * \n * // Run inference\n * const output = await model.generate(image_inputs.pixel_values, {\n *   decoder_input_ids,\n *   max_length: model.config.decoder.max_position_embeddings,\n * });\n * \n * // Decode output\n * const decoded = tokenizer.batch_decode(output)[0];\n * // <s_cord-v2><s_menu><s_nm> CINNAMON SUGAR</s_nm><s_unitprice> 17,000</s_unitprice><s_cnt> 1 x</s_cnt><s_price> 17,000</s_price></s_menu><s_sub_total><s_subtotal_price> 17,000</s_subtotal_price></s_sub_total><s_total><s_total_price> 17,000</s_total_price><s_cashprice> 20,000</s_cashprice><s_changeprice> 3,000</s_changeprice></s_total></s>\n * ```\n * \n * **Example:** Step-by-step Document Visual Question Answering (DocVQA)\n * \n * ```javascript\n * import { AutoProcessor, AutoTokenizer, AutoModelForVision2Seq, RawImage } from '@xenova/transformers';\n * \n * // Choose model to use\n * const model_id = 'Xenova/donut-base-finetuned-docvqa';\n * \n * // Prepare image inputs\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/invoice.png';\n * const image = await RawImage.read(url);\n * const image_inputs = await processor(image);\n * \n * // Prepare decoder inputs\n * const tokenizer = await AutoTokenizer.from_pretrained(model_id);\n * const question = 'What is the invoice number?';\n * const task_prompt = `<s_docvqa><s_question>${question}</s_question><s_answer>`;\n * const decoder_input_ids = tokenizer(task_prompt, {\n *   add_special_tokens: false,\n * }).input_ids;\n * \n * // Create the model\n * const model = await AutoModelForVision2Seq.from_pretrained(model_id);\n * \n * // Run inference\n * const output = await model.generate(image_inputs.pixel_values, {\n *   decoder_input_ids,\n *   max_length: model.config.decoder.max_position_embeddings,\n * });\n * \n * // Decode output\n * const decoded = tokenizer.batch_decode(output)[0];\n * // <s_docvqa><s_question> What is the invoice number?</s_question><s_answer> us-001</s_answer></s>\n * ```\n */\nexport class DonutSwinModel extends DonutSwinPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class YolosPreTrainedModel extends PreTrainedModel { }\nexport class YolosModel extends YolosPreTrainedModel { }\nexport class YolosForObjectDetection extends YolosPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new YolosObjectDetectionOutput(await super._call(model_inputs));\n    }\n}\n\nexport class YolosObjectDetectionOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Classification logits (including no-object) for all queries.\n     * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).\n     * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).\n     */\n    constructor({ logits, pred_boxes }) {\n        super();\n        this.logits = logits;\n        this.pred_boxes = pred_boxes;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class SamPreTrainedModel extends PreTrainedModel { }\nexport class SamModel extends SamPreTrainedModel {\n    /**\n     * @param {Object} model_inputs\n     * @param {Tensor} model_inputs.pixel_values Pixel values as a Tensor with shape `(batch_size, num_channels, height, width)`.\n     * @param {Tensor} model_inputs.input_points Input 2D spatial points with shape `(batch_size, num_points, 2)`. This is used by the prompt encoder to encode the prompt.\n     * @todo Add support for `input_labels`, `input_boxes`, `input_masks`, and `image_embeddings`.\n     */\n    async _call(model_inputs) {\n        return new SamImageSegmentationOutput(await super._call(model_inputs));\n    }\n}\n\n\n/**\n * Base class for Segment-Anything model's output.\n */\nexport class SamImageSegmentationOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.iou_scores The output logits of the model.\n     * @param {Tensor} output.pred_masks Predicted boxes.\n     */\n    constructor({ iou_scores, pred_masks }) {\n        super();\n        this.iou_scores = iou_scores;\n        this.pred_masks = pred_masks;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// MarianMT models\nexport class MarianPreTrainedModel extends PreTrainedModel { };\n\nexport class MarianModel extends MarianPreTrainedModel { }\n\nexport class MarianMTModel extends MarianPreTrainedModel {\n\n    /**\n     * Creates a new instance of the `MarianMTModel` class.\n    * @param {Object} config The model configuration object.\n    * @param {Object} session The ONNX session object.\n    * @param {any} decoder_merged_session \n    * @param {any} generation_config \n    */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// M2M100 models\nexport class M2M100PreTrainedModel extends PreTrainedModel { };\n\nexport class M2M100Model extends M2M100PreTrainedModel { }\n\nexport class M2M100ForConditionalGeneration extends M2M100PreTrainedModel {\n\n    /**\n     * Creates a new instance of the `M2M100ForConditionalGeneration` class.\n    * @param {Object} config The model configuration object.\n    * @param {Object} session The ONNX session object.\n    * @param {any} decoder_merged_session \n    * @param {any} generation_config \n    */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Wav2Vec2 models\nexport class Wav2Vec2PreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare Wav2Vec2 Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Load and run an `Wav2Vec2Model` for feature extraction.\n * \n * ```javascript\n * import { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/mms-300m');\n * const audio = await read_audio('https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac', 16000);\n * const inputs = await processor(audio);\n * \n * // Run model with inputs\n * const model = await AutoModel.from_pretrained('Xenova/mms-300m');\n * const output = await model(inputs);\n * // {\n * //   last_hidden_state: Tensor {\n * //     dims: [ 1, 1144, 1024 ],\n * //     type: 'float32',\n * //     data: Float32Array(1171456) [ ... ],\n * //     size: 1171456\n * //   }\n * // }\n * ```\n */\nexport class Wav2Vec2Model extends Wav2Vec2PreTrainedModel { }\n\nexport class Wav2Vec2ForCTC extends Wav2Vec2PreTrainedModel {\n    /**\n     * @param {Object} model_inputs\n     * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n     * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n     */\n    async _call(model_inputs) {\n        return new CausalLMOutput(await super._call(model_inputs));\n    }\n}\n\nexport class Wav2Vec2ForSequenceClassification extends Wav2Vec2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n// WavLM models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class WavLMPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare WavLM Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Load and run an `WavLMModel` for feature extraction.\n * \n * ```javascript\n * import { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/wavlm-base');\n * const audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav', 16000);\n * const inputs = await processor(audio);\n * \n * // Run model with inputs\n * const model = await AutoModel.from_pretrained('Xenova/wavlm-base');\n * const output = await model(inputs);\n * // {\n * //   last_hidden_state: Tensor {\n * //     dims: [ 1, 549, 768 ],\n * //     type: 'float32',\n * //     data: Float32Array(421632) [-0.349443256855011, -0.39341306686401367,  0.022836603224277496, ...],\n * //     size: 421632\n * //   }\n * // }\n * ```\n */\nexport class WavLMModel extends WavLMPreTrainedModel { }\n\n/**\n * WavLM Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n */\nexport class WavLMForCTC extends WavLMPreTrainedModel {\n    /**\n     * @param {Object} model_inputs\n     * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n     * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n     */\n    async _call(model_inputs) {\n        return new CausalLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * WavLM Model with a sequence classification head on top (a linear layer over the pooled output).\n */\nexport class WavLMForSequenceClassification extends WavLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n//////////////////////////////////////////////////\n// SpeechT5 models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class SpeechT5PreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare SpeechT5 Encoder-Decoder Model outputting raw hidden-states without any specific pre- or post-nets.\n */\nexport class SpeechT5Model extends SpeechT5PreTrainedModel { };\n\n/**\n * SpeechT5 Model with a speech encoder and a text decoder.\n */\nexport class SpeechT5ForSpeechToText extends SpeechT5PreTrainedModel { }\n\n/**\n * SpeechT5 Model with a text encoder and a speech decoder.\n */\nexport class SpeechT5ForTextToSpeech extends SpeechT5PreTrainedModel {\n\n    /**\n     * Creates a new instance of the `SpeechT5ForTextToSpeech` class.\n     * @param {Object} config The model configuration.\n     * @param {any} session session for the model.\n     * @param {any} decoder_merged_session session for the decoder.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.hidden_size / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.hidden_size / this.num_encoder_heads;\n    }\n\n    /**\n     * @typedef {Object} SpeechOutput\n     * @property {Tensor} [spectrogram] The predicted log-mel spectrogram of shape\n     * `(output_sequence_length, config.num_mel_bins)`. Returned when no `vocoder` is provided\n     * @property {Tensor} [waveform] The predicted waveform of shape `(num_frames,)`. Returned when a `vocoder` is provided.\n     * @property {Tensor} [cross_attentions] The outputs of the decoder's cross-attention layers of shape\n     * `(config.decoder_layers, config.decoder_attention_heads, output_sequence_length, input_sequence_length)`. returned when `output_cross_attentions` is `true`.\n     */\n\n    /**\n     * Converts a sequence of input tokens into a sequence of mel spectrograms, which are subsequently turned into a speech waveform using a vocoder.\n     * @param {Tensor} input_values Indices of input sequence tokens in the vocabulary.\n     * @param {Tensor} speaker_embeddings Tensor containing the speaker embeddings.\n     * @param {Object} options Optional parameters for generating speech.\n     * @param {number} [options.threshold=0.5] The generated sequence ends when the predicted stop token probability exceeds this value.\n     * @param {number} [options.minlenratio=0.0] Used to calculate the minimum required length for the output sequence.\n     * @param {number} [options.maxlenratio=20.0] Used to calculate the maximum allowed length for the output sequence.\n     * @param {Object} [options.vocoder=null] The vocoder that converts the mel spectrogram into a speech waveform. If `null`, the output is the mel spectrogram.\n     * @param {boolean} [options.output_cross_attentions=false] Whether or not to return the attentions tensors of the decoder's cross-attention layers.\n     * @returns {Promise<SpeechOutput>} A promise which resolves to an object containing the spectrogram, waveform, and cross-attention tensors.\n     */\n    async generate_speech(input_values, speaker_embeddings, {\n        threshold = 0.5,\n        minlenratio = 0.0,\n        maxlenratio = 20.0,\n        vocoder = null,\n        // output_cross_attentions = false, // TODO add\n    } = {}) {\n\n        const model_inputs = {\n            input_ids: input_values\n        }\n\n        const { encoder_outputs, encoder_attention_mask } = await encoderForward(this, model_inputs);\n\n        const r = encoder_outputs.dims[1] / this.config.reduction_factor;\n        const maxlen = Math.floor(r * maxlenratio);\n        const minlen = Math.floor(r * minlenratio);\n\n        const num_mel_bins = this.config.num_mel_bins;\n\n        let spectrogramParts = [];\n        let past_key_values = null;\n        let decoder_outputs = null;\n        let idx = 0;\n\n        while (true) {\n            ++idx;\n\n            const use_cache_branch = boolTensor(!!decoder_outputs);\n            let output_sequence;\n            if (decoder_outputs) {\n                output_sequence = decoder_outputs.output_sequence_out;\n            } else {\n                output_sequence = new Tensor(\n                    'float32',\n                    new Float32Array(num_mel_bins),\n                    [1, 1, num_mel_bins],\n                )\n            }\n            let decoderFeeds = {\n                use_cache_branch,\n                output_sequence,\n                encoder_attention_mask: encoder_attention_mask,\n                speaker_embeddings: speaker_embeddings,\n                encoder_hidden_states: encoder_outputs,\n            };\n\n            this.addPastKeyValues(decoderFeeds, past_key_values);\n            decoder_outputs = await sessionRun(this.decoder_merged_session, decoderFeeds);\n            past_key_values = this.getPastKeyValues(decoder_outputs, past_key_values);\n\n            const { prob, spectrum } = decoder_outputs;\n            spectrogramParts.push(spectrum);\n\n            if (idx >= minlen && (\n                // Finished when stop token or maximum length is reached.\n                Array.from(prob.data).filter(p => p >= threshold).length > 0 || idx >= maxlen\n            )) {\n                break;\n            }\n        }\n\n        const spectrogram = cat(spectrogramParts);\n        const { waveform } = await sessionRun(vocoder.session, { spectrogram });\n\n        return {\n            spectrogram,\n            waveform,\n            // cross_attentions: null, // TODO add\n        }\n    }\n}\n\n/**\n * HiFi-GAN vocoder.\n */\nexport class SpeechT5HifiGan extends PreTrainedModel {\n    main_input_name = 'spectrogram';\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// AutoModels, used to simplify construction of PreTrainedModels\n// (uses config to instantiate correct class)\n\n/**\n * Base class of all AutoModels. Contains the `from_pretrained` function\n * which is used to instantiate pretrained models.\n */\nexport class PretrainedMixin {\n    /**\n     * Mapping from model type to model class.\n     * @type {Map<string, Object>[]}\n     */\n    static MODEL_CLASS_MAPPINGS = null;\n\n    /**\n     * Whether to attempt to instantiate the base class (`PretrainedModel`) if \n     * the model type is not found in the mapping.\n     */\n    static BASE_IF_FAIL = false;\n\n\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, {\n        quantized = true,\n        progress_callback = null,\n        config = null,\n        cache_dir = null,\n        local_files_only = false,\n        revision = 'main',\n        model_file_name = null,\n    } = {}) {\n\n        let options = {\n            quantized,\n            progress_callback,\n            config,\n            cache_dir,\n            local_files_only,\n            revision,\n            model_file_name,\n        }\n        config = await AutoConfig.from_pretrained(pretrained_model_name_or_path, options);\n        if (!options.config) {\n            // If no config was passed, reuse this config for future processing\n            options.config = config;\n        }\n\n        if (!this.MODEL_CLASS_MAPPINGS) {\n            throw new Error(\"`MODEL_CLASS_MAPPINGS` not implemented for this type of `AutoClass`: \" + this.name);\n        }\n\n        for (let MODEL_CLASS_MAPPING of this.MODEL_CLASS_MAPPINGS) {\n            const modelInfo = MODEL_CLASS_MAPPING.get(config.model_type);\n            if (!modelInfo) {\n                continue; // Item not found in this mapping\n            }\n            return await modelInfo[1].from_pretrained(pretrained_model_name_or_path, options);\n        }\n\n        if (this.BASE_IF_FAIL) {\n            console.warn(`Unknown model class \"${config.model_type}\", attempting to construct from base class.`);\n            return await PreTrainedModel.from_pretrained(pretrained_model_name_or_path, options);\n        } else {\n            throw Error(`Unsupported model type: ${config.model_type}`)\n        }\n    }\n}\n\nconst MODEL_MAPPING_NAMES_ENCODER_ONLY = new Map([\n    ['bert', ['BertModel', BertModel]],\n    ['camembert', ['CamembertModel', CamembertModel]],\n    ['deberta', ['DebertaModel', DebertaModel]],\n    ['deberta-v2', ['DebertaV2Model', DebertaV2Model]],\n    ['mpnet', ['MPNetModel', MPNetModel]],\n    ['albert', ['AlbertModel', AlbertModel]],\n    ['distilbert', ['DistilBertModel', DistilBertModel]],\n    ['roberta', ['RobertaModel', RobertaModel]],\n    ['xlm', ['XLMModel', XLMModel]],\n    ['xlm-roberta', ['XLMRobertaModel', XLMRobertaModel]],\n    ['clip', ['CLIPModel', CLIPModel]],\n    ['mobilebert', ['MobileBertModel', MobileBertModel]],\n    ['squeezebert', ['SqueezeBertModel', SqueezeBertModel]],\n    ['wav2vec2', ['Wav2Vec2Model', Wav2Vec2Model]],\n    ['wavlm', ['WavLMModel', WavLMModel]],\n\n    ['detr', ['DetrModel', DetrModel]],\n    ['vit', ['ViTModel', ViTModel]],\n    ['mobilevit', ['MobileViTModel', MobileViTModel]],\n    ['beit', ['BeitModel', BeitModel]],\n    ['deit', ['DeiTModel', DeiTModel]],\n    ['resnet', ['ResNetModel', ResNetModel]],\n    ['swin', ['SwinModel', SwinModel]],\n    ['donut-swin', ['DonutSwinModel', DonutSwinModel]],\n    ['yolos', ['YolosModel', YolosModel]],\n\n    ['hifigan', ['SpeechT5HifiGan', SpeechT5HifiGan]],\n\n    ['sam', ['SamModel', SamModel]], // TODO change to encoder-decoder when model is split correctly\n]);\n\nconst MODEL_MAPPING_NAMES_ENCODER_DECODER = new Map([\n    ['t5', ['T5Model', T5Model]],\n    ['longt5', ['LongT5Model', LongT5Model]],\n    ['mt5', ['MT5Model', MT5Model]],\n    ['bart', ['BartModel', BartModel]],\n    ['mbart', ['MBartModel', MBartModel]],\n    ['marian', ['MarianModel', MarianModel]],\n    ['whisper', ['WhisperModel', WhisperModel]],\n    ['m2m_100', ['M2M100Model', M2M100Model]],\n    ['blenderbot', ['BlenderbotModel', BlenderbotModel]],\n    ['blenderbot-small', ['BlenderbotSmallModel', BlenderbotSmallModel]],\n]);\n\n\nconst MODEL_MAPPING_NAMES_DECODER_ONLY = new Map([\n    ['bloom', ['BloomModel', BloomModel]],\n    ['gpt2', ['GPT2Model', GPT2Model]],\n    ['gptj', ['GPTJModel', GPTJModel]],\n    ['gpt_bigcode', ['GPTBigCodeModel', GPTBigCodeModel]],\n    ['gpt_neo', ['GPTNeoModel', GPTNeoModel]],\n    ['gpt_neox', ['GPTNeoXModel', GPTNeoXModel]],\n    ['codegen', ['CodeGenModel', CodeGenModel]],\n    ['llama', ['LlamaModel', LlamaModel]],\n    ['mpt', ['MptModel', MptModel]],\n    ['opt', ['OPTModel', OPTModel]],\n]);\n\nconst MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES = new Map([\n    ['speecht5', ['SpeechT5ForSpeechToText', SpeechT5ForSpeechToText]],\n    ['whisper', ['WhisperForConditionalGeneration', WhisperForConditionalGeneration]],\n])\n\nconst MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES = new Map([\n    ['speecht5', ['SpeechT5ForTextToSpeech', SpeechT5ForTextToSpeech]],\n])\n\nconst MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES = new Map([\n    ['bert', ['BertForSequenceClassification', BertForSequenceClassification]],\n    ['camembert', ['CamembertForSequenceClassification', CamembertForSequenceClassification]],\n    ['deberta', ['DebertaForSequenceClassification', DebertaForSequenceClassification]],\n    ['deberta-v2', ['DebertaV2ForSequenceClassification', DebertaV2ForSequenceClassification]],\n    ['mpnet', ['MPNetForSequenceClassification', MPNetForSequenceClassification]],\n    ['albert', ['AlbertForSequenceClassification', AlbertForSequenceClassification]],\n    ['distilbert', ['DistilBertForSequenceClassification', DistilBertForSequenceClassification]],\n    ['roberta', ['RobertaForSequenceClassification', RobertaForSequenceClassification]],\n    ['xlm', ['XLMForSequenceClassification', XLMForSequenceClassification]],\n    ['xlm-roberta', ['XLMRobertaForSequenceClassification', XLMRobertaForSequenceClassification]],\n    ['bart', ['BartForSequenceClassification', BartForSequenceClassification]],\n    ['mbart', ['MBartForSequenceClassification', MBartForSequenceClassification]],\n    ['mobilebert', ['MobileBertForSequenceClassification', MobileBertForSequenceClassification]],\n    ['squeezebert', ['SqueezeBertForSequenceClassification', SqueezeBertForSequenceClassification]],\n]);\n\nconst MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES = new Map([\n    ['bert', ['BertForTokenClassification', BertForTokenClassification]],\n    ['camembert', ['CamembertForTokenClassification', CamembertForTokenClassification]],\n    ['deberta', ['DebertaForTokenClassification', DebertaForTokenClassification]],\n    ['deberta-v2', ['DebertaV2ForTokenClassification', DebertaV2ForTokenClassification]],\n    ['mpnet', ['MPNetForTokenClassification', MPNetForTokenClassification]],\n    ['distilbert', ['DistilBertForTokenClassification', DistilBertForTokenClassification]],\n    ['roberta', ['RobertaForTokenClassification', RobertaForTokenClassification]],\n    ['xlm', ['XLMForTokenClassification', XLMForTokenClassification]],\n    ['xlm-roberta', ['XLMRobertaForTokenClassification', XLMRobertaForTokenClassification]],\n]);\n\nconst MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES = new Map([\n    ['t5', ['T5ForConditionalGeneration', T5ForConditionalGeneration]],\n    ['longt5', ['LongT5ForConditionalGeneration', LongT5ForConditionalGeneration]],\n    ['mt5', ['MT5ForConditionalGeneration', MT5ForConditionalGeneration]],\n    ['bart', ['BartForConditionalGeneration', BartForConditionalGeneration]],\n    ['mbart', ['MBartForConditionalGeneration', MBartForConditionalGeneration]],\n    ['marian', ['MarianMTModel', MarianMTModel]],\n    ['m2m_100', ['M2M100ForConditionalGeneration', M2M100ForConditionalGeneration]],\n    ['blenderbot', ['BlenderbotForConditionalGeneration', BlenderbotForConditionalGeneration]],\n    ['blenderbot-small', ['BlenderbotSmallForConditionalGeneration', BlenderbotSmallForConditionalGeneration]],\n]);\n\nconst MODEL_WITH_LM_HEAD_MAPPING_NAMES = new Map([\n    ['bloom', ['BloomForCausalLM', BloomForCausalLM]],\n    ['gpt2', ['GPT2LMHeadModel', GPT2LMHeadModel]],\n    ['gptj', ['GPTJForCausalLM', GPTJForCausalLM]],\n    ['gpt_bigcode', ['GPTBigCodeForCausalLM', GPTBigCodeForCausalLM]],\n    ['gpt_neo', ['GPTNeoForCausalLM', GPTNeoForCausalLM]],\n    ['gpt_neox', ['GPTNeoXForCausalLM', GPTNeoXForCausalLM]],\n    ['codegen', ['CodeGenForCausalLM', CodeGenForCausalLM]],\n    ['llama', ['LlamaForCausalLM', LlamaForCausalLM]],\n    ['mpt', ['MptForCausalLM', MptForCausalLM]],\n    ['opt', ['OPTForCausalLM', OPTForCausalLM]],\n    ['mbart', ['MBartForCausalLM', MBartForCausalLM]],\n]);\n\nconst MODEL_FOR_MASKED_LM_MAPPING_NAMES = new Map([\n    ['bert', ['BertForMaskedLM', BertForMaskedLM]],\n    ['camembert', ['CamembertForMaskedLM', CamembertForMaskedLM]],\n    ['deberta', ['DebertaForMaskedLM', DebertaForMaskedLM]],\n    ['deberta-v2', ['DebertaV2ForMaskedLM', DebertaV2ForMaskedLM]],\n    ['mpnet', ['MPNetForMaskedLM', MPNetForMaskedLM]],\n    ['albert', ['AlbertForMaskedLM', AlbertForMaskedLM]],\n    ['distilbert', ['DistilBertForMaskedLM', DistilBertForMaskedLM]],\n    ['roberta', ['RobertaForMaskedLM', RobertaForMaskedLM]],\n    ['xlm', ['XLMWithLMHeadModel', XLMWithLMHeadModel]],\n    ['xlm-roberta', ['XLMRobertaForMaskedLM', XLMRobertaForMaskedLM]],\n    ['mobilebert', ['MobileBertForMaskedLM', MobileBertForMaskedLM]],\n    ['squeezebert', ['SqueezeBertForMaskedLM', SqueezeBertForMaskedLM]],\n]);\n\nconst MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES = new Map([\n    ['bert', ['BertForQuestionAnswering', BertForQuestionAnswering]],\n    ['camembert', ['CamembertForQuestionAnswering', CamembertForQuestionAnswering]],\n    ['deberta', ['DebertaForQuestionAnswering', DebertaForQuestionAnswering]],\n    ['deberta-v2', ['DebertaV2ForQuestionAnswering', DebertaV2ForQuestionAnswering]],\n    ['mpnet', ['MPNetForQuestionAnswering', MPNetForQuestionAnswering]],\n    ['albert', ['AlbertForQuestionAnswering', AlbertForQuestionAnswering]],\n    ['distilbert', ['DistilBertForQuestionAnswering', DistilBertForQuestionAnswering]],\n    ['roberta', ['RobertaForQuestionAnswering', RobertaForQuestionAnswering]],\n    ['xlm', ['XLMForQuestionAnswering', XLMForQuestionAnswering]],\n    ['xlm-roberta', ['XLMRobertaForQuestionAnswering', XLMRobertaForQuestionAnswering]],\n    ['mobilebert', ['MobileBertForQuestionAnswering', MobileBertForQuestionAnswering]],\n    ['squeezebert', ['SqueezeBertForQuestionAnswering', SqueezeBertForQuestionAnswering]],\n]);\n\nconst MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES = new Map([\n    ['vision-encoder-decoder', ['VisionEncoderDecoderModel', VisionEncoderDecoderModel]],\n]);\n\nconst MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES = new Map([\n    ['vision-encoder-decoder', ['VisionEncoderDecoderModel', VisionEncoderDecoderModel]],\n]);\n\nconst MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES = new Map([\n    ['vit', ['ViTForImageClassification', ViTForImageClassification]],\n    ['mobilevit', ['MobileViTForImageClassification', MobileViTForImageClassification]],\n    ['beit', ['BeitForImageClassification', BeitForImageClassification]],\n    ['deit', ['DeiTForImageClassification', DeiTForImageClassification]],\n    ['resnet', ['ResNetForImageClassification', ResNetForImageClassification]],\n    ['swin', ['SwinForImageClassification', SwinForImageClassification]],\n]);\n\nconst MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES = new Map([\n    ['detr', ['DetrForObjectDetection', DetrForObjectDetection]],\n    ['yolos', ['YolosForObjectDetection', YolosForObjectDetection]],\n]);\n\nconst MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES = new Map([\n    ['detr', ['DetrForSegmentation', DetrForSegmentation]],\n]);\n\nconst MODEL_FOR_MASK_GENERATION_MAPPING_NAMES = new Map([\n    ['sam', ['SamModel', SamModel]],\n]);\n\nconst MODEL_FOR_CTC_MAPPING_NAMES = new Map([\n    ['wav2vec2', ['Wav2Vec2ForCTC', Wav2Vec2ForCTC]],\n    ['wavlm', ['WavLMForCTC', WavLMForCTC]],\n]);\n\nconst MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES = new Map([\n    ['wav2vec2', ['Wav2Vec2ForSequenceClassification', Wav2Vec2ForSequenceClassification]],\n    ['wavlm', ['WavLMForSequenceClassification', WavLMForSequenceClassification]],\n]);\n\n\nconst MODEL_CLASS_TYPE_MAPPING = [\n    [MODEL_MAPPING_NAMES_ENCODER_ONLY, MODEL_TYPES.EncoderOnly],\n    [MODEL_MAPPING_NAMES_ENCODER_DECODER, MODEL_TYPES.EncoderDecoder],\n    [MODEL_MAPPING_NAMES_DECODER_ONLY, MODEL_TYPES.DecoderOnly],\n    [MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES, MODEL_TYPES.Seq2Seq],\n    [MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES, MODEL_TYPES.Seq2Seq],\n    [MODEL_WITH_LM_HEAD_MAPPING_NAMES, MODEL_TYPES.DecoderOnly],\n    [MODEL_FOR_MASKED_LM_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES, MODEL_TYPES.Vision2Seq],\n    [MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_MASK_GENERATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_CTC_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES, MODEL_TYPES.Seq2Seq],\n];\n\nfor (const [mappings, type] of MODEL_CLASS_TYPE_MAPPING) {\n    // @ts-ignore\n    for (const [name, model] of mappings.values()) {\n        MODEL_TYPE_MAPPING.set(name, type);\n        MODEL_CLASS_TO_NAME_MAPPING.set(model, name);\n        MODEL_NAME_TO_CLASS_MAPPING.set(name, model);\n    }\n}\n\nconst CUSTOM_MAPPING = [\n    ['CLIPTextModelWithProjection', CLIPTextModelWithProjection, MODEL_TYPES.EncoderOnly],\n    ['CLIPVisionModelWithProjection', CLIPVisionModelWithProjection, MODEL_TYPES.EncoderOnly],\n]\nfor (const [name, model, type] of CUSTOM_MAPPING) {\n    MODEL_TYPE_MAPPING.set(name, type);\n    MODEL_CLASS_TO_NAME_MAPPING.set(model, name);\n    MODEL_NAME_TO_CLASS_MAPPING.set(name, model);\n}\n\n\n/**\n * Helper class which is used to instantiate pretrained models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModel.from_pretrained('bert-base-uncased');\n */\nexport class AutoModel extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_MAPPING_NAMES_ENCODER_ONLY, MODEL_MAPPING_NAMES_ENCODER_DECODER, MODEL_MAPPING_NAMES_DECODER_ONLY];\n    static BASE_IF_FAIL = true;\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence classification models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english');\n */\nexport class AutoModelForSequenceClassification extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained token classification models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl');\n */\nexport class AutoModelForTokenClassification extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence-to-sequence models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSeq2SeqLM.from_pretrained('t5-small');\n */\nexport class AutoModelForSeq2SeqLM extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence-to-sequence speech-to-text models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSpeechSeq2Seq.from_pretrained('openai/whisper-tiny.en');\n */\nexport class AutoModelForSpeechSeq2Seq extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence-to-sequence text-to-spectrogram models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForTextToSpectrogram.from_pretrained('microsoft/speecht5_tts');\n */\nexport class AutoModelForTextToSpectrogram extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained causal language models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForCausalLM.from_pretrained('gpt2');\n */\nexport class AutoModelForCausalLM extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_WITH_LM_HEAD_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained masked language models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForMaskedLM.from_pretrained('bert-base-uncased');\n */\nexport class AutoModelForMaskedLM extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_MASKED_LM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained question answering models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad');\n */\nexport class AutoModelForQuestionAnswering extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained vision-to-sequence models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForVision2Seq.from_pretrained('nlpconnect/vit-gpt2-image-captioning');\n */\nexport class AutoModelForVision2Seq extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained image classification models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForImageClassification.from_pretrained('google/vit-base-patch16-224');\n */\nexport class AutoModelForImageClassification extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained image segmentation models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForImageSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic');\n */\nexport class AutoModelForImageSegmentation extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained object detection models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForObjectDetection.from_pretrained('facebook/detr-resnet-50');\n */\nexport class AutoModelForObjectDetection extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained object detection models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForMaskGeneration.from_pretrained('Xenova/sam-vit-base');\n */\nexport class AutoModelForMaskGeneration extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_MASK_GENERATION_MAPPING_NAMES];\n}\n\nexport class AutoModelForCTC extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_CTC_MAPPING_NAMES];\n}\n\nexport class AutoModelForAudioClassification extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES];\n}\n\nexport class AutoModelForDocumentQuestionAnswering extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES];\n}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class Seq2SeqLMOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits The output logits of the model.\n     * @param {Tensor} output.past_key_values An tensor of key/value pairs that represent the previous state of the model.\n     * @param {Tensor} output.encoder_outputs The output of the encoder in a sequence-to-sequence model.\n     * @param {Tensor} [output.decoder_attentions] Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the self-attention heads.\n     * @param {Tensor} [output.cross_attentions] Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the weighted average in the cross-attention heads.\n     */\n    constructor({ logits, past_key_values, encoder_outputs, decoder_attentions = null, cross_attentions = null }) {\n        super();\n        this.logits = logits;\n        this.past_key_values = past_key_values;\n        this.encoder_outputs = encoder_outputs;\n        this.decoder_attentions = decoder_attentions;\n        this.cross_attentions = cross_attentions;\n    }\n}\n\n/**\n * Base class for outputs of sentence classification models.\n */\nexport class SequenceClassifierOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits classification (or regression if config.num_labels==1) scores (before SoftMax).\n     */\n    constructor({ logits }) {\n        super();\n        this.logits = logits;\n    }\n}\n\n/**\n * Base class for outputs of token classification models.\n */\nexport class TokenClassifierOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Classification scores (before SoftMax).\n     */\n    constructor({ logits }) {\n        super();\n        this.logits = logits;\n    }\n}\n\n/**\n * Base class for masked language models outputs.\n */\nexport class MaskedLMOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     */\n    constructor({ logits }) {\n        super();\n        this.logits = logits;\n    }\n}\n\n/**\n * Base class for outputs of question answering models.\n */\nexport class QuestionAnsweringModelOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.start_logits Span-start scores (before SoftMax).\n     * @param {Tensor} output.end_logits Span-end scores (before SoftMax).\n     */\n    constructor({ start_logits, end_logits }) {\n        super();\n        this.start_logits = start_logits;\n        this.end_logits = end_logits;\n    }\n}\n\n\n/**\n * Base class for causal language model (or autoregressive) outputs.\n */\nexport class CausalLMOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before softmax).\n     */\n    constructor({ logits }) {\n        super();\n        this.logits = logits;\n    }\n}\n\n/**\n * Base class for causal language model (or autoregressive) outputs.\n */\nexport class CausalLMOutputWithPast extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before softmax).\n     * @param {Tensor} output.past_key_values Contains pre-computed hidden-states (key and values in the self-attention blocks)\n     * that can be used (see `past_key_values` input) to speed up sequential decoding.\n     */\n    constructor({ logits, past_key_values }) {\n        super();\n        this.logits = logits;\n        this.past_key_values = past_key_values;\n    }\n}\n"],"names":["InferenceSession","Tensor","ONNXTensor","ONNX","MODEL_TYPES","MODEL_TYPE_MAPPING","Map","MODEL_NAME_TO_CLASS_MAPPING","MODEL_CLASS_TO_NAME_MAPPING","async","constructSession","pretrained_model_name_or_path","fileName","options","modelFileName","quantized","buffer","create","executionProviders","err","console","warn","sessionRun","session","inputs","checkedInputs","missingInputs","inputName","inputNames","undefined","push","length","Error","join","numInputsProvided","Object","keys","numInputsNeeded","ignored","filter","includes","validateInputs","output","run","replaceTensors","e","error","obj","prop","toI64Tensor","items","Array","isArray","some","x","BigInt64Array","from","flat","map","BigInt","prepareAttentionMask","self","tokens","pad_token_id","config","eos_token_id","is_pad_token_in_inputs","indexOf","is_pad_token_not_equal_to_eos_token_id","data","dims","boolTensor","value","seq2seqForward","model_inputs","encoder_outputs","past_key_values","encoderForward","last_hidden_state","decoderFeeds","input_ids","decoder_input_ids","encoder_hidden_states","use_cache_branch","decoder_merged_session","encoder_attention_mask","attention_mask","addPastKeyValues","decoderResults","logits","getPastKeyValues","attns","getAttentions","Seq2SeqLMOutput","seq2seqStartBeams","inputTokenIds","generation_config","numOutputTokens","beams","beamId","requires_attention_mask","decoder_start_token_id","bos_token_id","tolist","start","prev_model_outputs","output_token_ids","done","score","id","seq2seqRunBeam","beam","input_name","main_input_name","slice","forward","seq2seqUpdatebeam","newTokenId","encoderFeeds","key","decoderForward","decoderStartBeams","inputs_attention_mask","attn_mask","Number","input","model_input_ids","num_output_tokens","decoderRunBeam","attnMaskData","fill","decoderUpdatebeam","PreTrainedModel","constructor","super","this","modelName","get","modelType","can_generate","_runBeam","_getStartBeams","_updateBeam","_forward","promises","item","handler","dispose","Promise","all","static","progress_callback","cache_dir","local_files_only","revision","model_file_name","info","_get_logits_processor","input_ids_seq_length","logits_processor","processors","repetition_penalty","no_repeat_ngram_size","min_length","min_new_tokens","forced_bos_token_id","forced_eos_token_id","max_length","begin_suppress_tokens","begin_index","forced_decoder_ids","extend","_get_generation_config","gen_config","assign","errorMessage","model_type","possibleInfo","MODEL_WITH_LM_HEAD_MAPPING_NAMES","MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES","MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES","MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES","name","is_encoder_decoder","at","eos_token_ids","maxOutputTokens","max_new_tokens","Infinity","useMaxLength","isInteger","sampler","getStartBeams","newest_beams","runBeam","output_attentions","addAttentionsToBeam","output_scores","sampledTokens","logProb","newBeam","updateBeam","groupBeams","group","sort","a","b","num_beams","callback_function","groupedBeams","getFlattened","batch","num_return_sequences","sequences","return_dict_in_generate","decoder_attentions","cross_attentions","groups","values","pastKeyValues","pkvs","startsWith","newName","replace","attnName","result","split","pop","add_encoder_pkv","encoder_dims","num_encoder_heads","encoder_dim_kv","decoder_dims","num_decoder_heads","decoder_dim_kv","i","num_decoder_layers","multi_query","dim_kv","num_layers","keyDims","num_heads","valueDims","ModelOutput","BertPreTrainedModel","CamembertPreTrainedModel","DebertaPreTrainedModel","DebertaV2PreTrainedModel","DistilBertPreTrainedModel","MobileBertPreTrainedModel","MPNetPreTrainedModel","SqueezeBertPreTrainedModel","AlbertPreTrainedModel","T5PreTrainedModel","LongT5PreTrainedModel","MT5PreTrainedModel","BartPretrainedModel","MBartPreTrainedModel","BlenderbotPreTrainedModel","BlenderbotSmallPreTrainedModel","RobertaPreTrainedModel","XLMPreTrainedModel","XLMRobertaPreTrainedModel","WhisperPreTrainedModel","VisionEncoderDecoderModel","encoderConfig","encoder","decoderConfig","decoder","encoderModelType","MODEL_MAPPING_NAMES_ENCODER_ONLY","MODEL_MAPPING_NAMES_ENCODER_DECODER","decoderModel","decoderModelClass","num_encoder_layers","CLIPPreTrainedModel","GPT2PreTrainedModel","n_head","n_layer","n_embd","GPTNeoPreTrainedModel","hidden_size","GPTNeoXPreTrainedModel","num_attention_heads","num_hidden_layers","GPTJPreTrainedModel","GPTBigCodePreTrainedModel","CodeGenPreTrainedModel","LlamaPreTrainedModel","BloomPreTrainedModel","MptPreTrainedModel","n_heads","n_layers","d_model","OPTPreTrainedModel","ViTPreTrainedModel","MobileViTPreTrainedModel","BeitPreTrainedModel","DetrPreTrainedModel","DetrObjectDetectionOutput","pred_boxes","DetrSegmentationOutput","pred_masks","DeiTPreTrainedModel","ResNetPreTrainedModel","SwinPreTrainedModel","DonutSwinPreTrainedModel","YolosPreTrainedModel","YolosObjectDetectionOutput","SamPreTrainedModel","SamModel","SamImageSegmentationOutput","_call","iou_scores","MarianPreTrainedModel","M2M100PreTrainedModel","Wav2Vec2PreTrainedModel","WavLMPreTrainedModel","SpeechT5PreTrainedModel","PretrainedMixin","MODEL_CLASS_MAPPINGS","MODEL_CLASS_MAPPING","modelInfo","from_pretrained","BASE_IF_FAIL","MODEL_MAPPING_NAMES_DECODER_ONLY","decoder_layers","decoder_attention_heads","encoder_layers","encoder_attention_heads","return_timestamps","return_token_timestamps","task","alignment_heads","outputs","generate","_extract_token_timestamps","num_frames","generate_outputs","time_precision","median_filter_width","batchedMatrices","_","weights","l","h","transpose","std","calculatedMean","smoothedWeights","clone","aTensor","bTensor","stdTensor","meanTensor","c","cTensor","d","set","timestampsShape","timestamps","Float32Array","batch_idx","matrix","neg","squeeze_","text_indices","time_indices","diffs","v","jumps","jump_times","MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES","input_values","speaker_embeddings","threshold","minlenratio","maxlenratio","vocoder","r","reduction_factor","maxlen","Math","floor","minlen","num_mel_bins","spectrogramParts","decoder_outputs","idx","output_sequence","output_sequence_out","prob","spectrum","p","spectrogram","waveform","MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES","SequenceClassifierOutput","MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES","TokenClassifierOutput","d_kv","MODEL_FOR_MASKED_LM_MAPPING_NAMES","MaskedLMOutput","MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES","QuestionAnsweringModelOutput","MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES","MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES","MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES","MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES","MODEL_FOR_MASK_GENERATION_MAPPING_NAMES","MODEL_FOR_CTC_MAPPING_NAMES","CausalLMOutput","MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES","MODEL_CLASS_TYPE_MAPPING","mappings","type","model","CUSTOM_MAPPING","AutoModel","AutoModelForSequenceClassification","AutoModelForTokenClassification","AutoModelForSeq2SeqLM","AutoModelForSpeechSeq2Seq","AutoModelForTextToSpectrogram","AutoModelForCausalLM","AutoModelForMaskedLM","AutoModelForQuestionAnswering","AutoModelForVision2Seq","AutoModelForImageClassification","AutoModelForImageSegmentation","AutoModelForObjectDetection","AutoModelForCTC","AutoModelForAudioClassification","AutoModelForDocumentQuestionAnswering","start_logits","end_logits"],"sourceRoot":""}